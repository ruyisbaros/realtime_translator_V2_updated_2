{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "# Assuming you have adapted XTTS-v2's training code into a modular function\n",
    "from your_xtts_training_module import train_one_epoch, evaluate  # Replace with your actual training functions\n",
    "from your_xtts_data_loading import create_data_loaders # Replace with your data loading\n",
    "\n",
    "def objective(trial):\n",
    "    # 1. Suggest Hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-4, log=True)\n",
    "    gradient_clip = trial.suggest_float(\"gradient_clip\", 0.5, 5.0)\n",
    "    # Example: Only fine-tune last N layers\n",
    "    num_layers_to_finetune = trial.suggest_int(\"num_layers_to_finetune\", 4, 12)\n",
    "\n",
    "    # 2. Load Data (10% Subset) - Replace with YOUR data loading code\n",
    "    train_loader, eval_loader = create_data_loaders(\n",
    "        dataset_config,  # Assuming you have a dataset_config\n",
    "        batch_size=batch_size,\n",
    "        train_split=0.9,  # Adjust split as needed\n",
    "        max_samples=0.1  # Limit to 10% of dataset\n",
    "    )\n",
    "\n",
    "    # 3. Initialize Model and Optimizer (Adapt to XTTS-v2)\n",
    "    model = YourXTTSModel().to(\"cuda\")  # Replace with your actual XTTS-v2 model loading\n",
    "    # Freeze earlier layers (example)\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"some_early_layer\" in name: # Replace with your layer naming logic\n",
    "            param.requires_grad = False\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), # Only train trainable parameters\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    # 4. Training Loop (Limited Epochs)\n",
    "    num_epochs = 30  # Or something reasonable for the 10% dataset\n",
    "    for epoch in range(num_epochs):\n",
    "        # Train one epoch (Assuming you have a function for this)\n",
    "        train_one_epoch(model, train_loader, optimizer, criterion, gradient_clip) # Adapt to XTTS\n",
    "\n",
    "        # 5. Evaluation (Adapt to XTTS and Consider Speaker Verification)\n",
    "        eval_loss, speaker_verification_accuracy = evaluate(model, eval_loader, criterion, speaker_verification_model) # Adapt to XTTS\n",
    "\n",
    "        # Report the evaluation loss and speaker verification accuracy to Optuna\n",
    "        trial.report(speaker_verification_accuracy, epoch) # Or combine eval_loss and speaker_verification_accuracy\n",
    "\n",
    "        # Handle pruning based on intermediate values.\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "    return speaker_verification_accuracy # Or your combined metric\n",
    "\n",
    "# Create Optuna Study\n",
    "study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner()) # Maximize speaker verification accuracy\n",
    "study.optimize(objective, n_trials=50) # Adjust n_trials\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(\"  Value: {}\".format(trial.value))\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
