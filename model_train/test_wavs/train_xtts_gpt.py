import os
import torch
from trainer import Trainer, TrainerArgs
from TTS.tts.datasets import load_tts_samples
from TTS.tts.configs.shared_configs import BaseDatasetConfig
from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig
from TTS.utils.manage import ModelManager
# âœ… Required for PyTorch 2.6 fix
from TTS.tts.configs.xtts_config import XttsConfig

# âœ… Fix for PyTorch 2.6 Security Restrictions
# torch.serialization.add_safe_globals([XttsConfig, XttsAudioConfig])

# âœ… Define Paths
OUTPUT_PATH = os.path.abspath("fine_tuning_output")  # ğŸ”¥ Ensure absolute path
DATASET_PATH = os.path.abspath("audio_dataset")  # ğŸ”¥ Ensure absolute path
METADATA_FILE = os.path.join(DATASET_PATH, "metadata.csv")

# âœ… Set XTTS Checkpoint Paths
CHECKPOINTS_OUT_PATH = os.path.abspath(
    "xtts_checkpoints")  # ğŸ”¥ Store downloaded files
os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)

DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, "dvae.pth")
MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, "mel_stats.pth")
TOKENIZER_FILE = os.path.join(CHECKPOINTS_OUT_PATH, "vocab.json")
XTTS_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, "model.pth")

# âœ… Download Required Files if Missing
if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):
    print("ğŸš€ Downloading DVAE & MEL Stats files...")
    ModelManager._download_model_files([
        "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/dvae.pth",
        "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/mel_stats.pth"
    ], CHECKPOINTS_OUT_PATH, progress_bar=True)

if not os.path.isfile(TOKENIZER_FILE) or not os.path.isfile(XTTS_CHECKPOINT):
    print("ğŸš€ Downloading XTTS Model files...")
    ModelManager._download_model_files([
        "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/vocab.json",
        "https://coqui.gateway.scarf.sh/hf-coqui/XTTS-v2/main/model.pth"
    ], CHECKPOINTS_OUT_PATH, progress_bar=True)

# âœ… Dataset Config (Ensure formatter="ljspeech" for correct format)
dataset_config = BaseDatasetConfig(
    formatter="ljspeech",  # ğŸ”¥ Ensure dataset follows LJSpeech format
    meta_file_train=METADATA_FILE,
    path=DATASET_PATH,
    language="de",  # ğŸ”¥ Adjust language as needed
)

# âœ… Load Training Samples
train_samples, eval_samples = load_tts_samples(
    dataset_config, eval_split=True, eval_split_size=0.02)

print(f"âœ… Training Samples: {len(train_samples)}")
print(f"âœ… Evaluation Samples: {len(eval_samples)}")

# âœ… Model Arguments for XTTS (GPT-Based)
model_args = GPTArgs(
    # ğŸ”¥ Ensure audio for conditioning latents is within this range
    max_conditioning_length=143677,
    min_conditioning_length=66150,  # ğŸ”¥ Ensure minimum length
    debug_loading_failures=True,  # ğŸ”¥ Print dataset errors
    max_wav_length=223997,
    max_text_length=200,
    mel_norm_file=MEL_NORM_FILE,
    dvae_checkpoint=DVAE_CHECKPOINT,
    xtts_checkpoint=XTTS_CHECKPOINT,
    tokenizer_file=TOKENIZER_FILE,
    gpt_num_audio_tokens=1026,
    gpt_start_audio_token=1024,
    gpt_stop_audio_token=1025,
    gpt_use_masking_gt_prompt_approach=True,
    gpt_use_perceiver_resampler=True,
)

# âœ… Audio Config (Ensure sampling rates match XTTS)
audio_config = XttsAudioConfig(
    sample_rate=22050, dvae_sample_rate=22050, output_sample_rate=24000
)

# âœ… Speaker Reference (Optional for Few-Shot Cloning)
SPEAKER_REFERENCE = os.path.join(DATASET_PATH, "wavs", "segment_001.wav")

# âœ… Training Config (Optimized for H100 GPU)
config = GPTTrainerConfig(
    dashboard_logger="tensorboard",
    run_eval=True,  # ğŸ”¥ Ensures evaluation runs
    epochs=1000,  # ğŸ”¥ High number, stop manually if needed
    output_path=OUTPUT_PATH,
    model_args=model_args,
    run_name="XTTS_v2_FT_CustomData",
    project_name="XTTS_Finetuning",
    logger_uri=None,
    audio=audio_config,
    batch_size=16,  # ğŸ”¥ Increased for H100 GPU
    batch_group_size=48,
    eval_batch_size=16,  # ğŸ”¥ Match batch size for stability
    num_loader_workers=8,  # ğŸ”¥ Maximize efficiency
    print_step=5,
    plot_step=100,
    log_model_step=400,
    save_step=100,  # ğŸ”¥ Save checkpoints frequently
    save_n_checkpoints=3,
    save_checkpoints=True,
    optimizer="AdamW",
    optimizer_params={"betas": [0.9, 0.96], "eps": 1e-8, "weight_decay": 1e-2},
    lr=5e-6,  # ğŸ”¥ Fine-tuning requires a lower LR
    lr_scheduler="MultiStepLR",
    lr_scheduler_params={"milestones": [50000, 150000, 300000], "gamma": 0.5},
    test_sentences=[
        {"text": "This is a test sentence for fine-tuned XTTS-v2.",
            "speaker_wav": SPEAKER_REFERENCE, "language": "en"},
        {"text": "Artificial intelligence is changing the world.",
            "speaker_wav": SPEAKER_REFERENCE, "language": "de"},
    ],
)


# âœ… Initialize Model
print(f"ğŸš€ Initializing GPT-Based XTTS Model...")
model = GPTTrainer.init_from_config(config)

# âœ… Load Checkpoint
try:
    model.load_checkpoint(config, XTTS_CHECKPOINT,
                          strict=False)
    print(f"âœ… Loaded checkpoint from {XTTS_CHECKPOINT}")
except Exception as e:
    raise ValueError(f"âš ï¸ XTTS-V2 Checkpoint loading failed: {e}")

# âœ… Trainer Arguments
trainer_args = TrainerArgs(
    restore_path=None,
    skip_train_epoch=False,
    start_with_eval=True,
    grad_accum_steps=252,  # ğŸ”¥ Matches Kaggleâ€™s fine-tuning method
)

# âœ… Initialize Trainer
trainer = Trainer(
    trainer_args, config, OUTPUT_PATH, model=model, train_samples=train_samples, eval_samples=eval_samples
)

# âœ… Start Fine-Tuning ğŸš€
print(f"ğŸš€ Starting fine-tuning process...")
trainer.fit()

print(f"ğŸ‰ Fine-tuning completed! Output saved to: {OUTPUT_PATH}")
