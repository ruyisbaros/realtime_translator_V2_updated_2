{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchaudio librosa matplotlib numpy pandas\n",
    "#!pip install wget\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/jaywalnut310/vits.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
      "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n",
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from datasets import load_dataset\n",
    "\n",
    "# âœ… Load dataset from local cache (NO re-downloading)\n",
    "dataset = load_dataset(\"erenfazlioglu/turkishvoicedataset\", cache_dir=\"/home/ahmet/.cache/huggingface/datasets\")\n",
    "\n",
    "# âœ… Define the base directory where audio files are stored\n",
    "dataset_cache_dir = \"/home/ahmet/.cache/huggingface/datasets/erenfazlioglu___turkishvoicedataset/default/0.0.0/13238c462f32f6c2fd8293f732f2eac2a03ce48c\"\n",
    "\n",
    "# âœ… Create a directory for extracted audio files\n",
    "output_dir = \"turkish_audio\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# âœ… Copy audio files to a new folder\n",
    "for i, entry in enumerate(dataset[\"train\"]):\n",
    "    relative_audio_path = entry[\"audio\"][\"path\"]  # e.g., '0000001.mp3'\n",
    "    full_audio_path = os.path.join(dataset_cache_dir, relative_audio_path)  # Convert to full path\n",
    "\n",
    "    if not os.path.exists(full_audio_path):\n",
    "        print(f\"âŒ Missing file: {full_audio_path}\")  # Check if file is missing\n",
    "        continue\n",
    "\n",
    "    # âœ… Save with a new name\n",
    "    new_audio_file = os.path.join(output_dir, f\"audio_{i}.mp3\")\n",
    "    shutil.copy(full_audio_path, new_audio_file)\n",
    "\n",
    "    print(f\"âœ… Copied: {new_audio_file}\")\n",
    "\n",
    "print(f\"\\nğŸ‰ Extraction complete! Check the 'turkish_audio/' folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['I want to meet with you yesterday evening.']\n",
      " > Processing time: 0.7659907341003418\n",
      " > Real-time factor: 0.23727376498809474\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ahmet_output_1.wav'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tts.tts_to_file(text=\"I want to meet with you yesterday evening.\",\n",
    "                file_path=\"ahmet_output_1.wav\",\n",
    "                speaker_wav=\"ahmt.wav\",\n",
    "                language=\"en\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello teammate. Everything is ok. Only I could not find additional English dataset yet. as you know we have 55h English, ~230h DE and with last added ~80h turkish dataset:\n",
    "so 82k DE, 69k en 45k Tr audio files\n",
    "What can we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def parse_metadata_with_wavs(metadata_path, wavs_folder):\n",
    "    \"\"\"\n",
    "    Reads a metadata.txt file and aligns transcriptions with actual wav files.\n",
    "    \n",
    "    Now includes **speaker names** in the final dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - A dictionary {wav_filename: {\"text\": transcription, \"speaker\": speaker_name}}\n",
    "    \"\"\"\n",
    "    speaker_name = os.path.basename(os.path.dirname(metadata_path))  # âœ… Extract speaker from folder name\n",
    "\n",
    "    # âœ… Step 1: Extract Transcriptions (Order Only)\n",
    "    transcriptions = []\n",
    "    with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        current_text = []\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            if \"|\" in line:\n",
    "                # âœ… New entry â†’ Save the last one\n",
    "                if current_text:\n",
    "                    transcriptions.append(\" \".join(current_text).strip())\n",
    "\n",
    "                # âœ… Extract transcription only (ignore filename)\n",
    "                parts = line.split(\"|\", 1)\n",
    "                current_text = [parts[1].strip()] if len(parts) > 1 else []\n",
    "            else:\n",
    "                # âœ… Continuation of the previous line\n",
    "                current_text.append(line.strip())\n",
    "\n",
    "        # âœ… Save the last transcription\n",
    "        if current_text:\n",
    "            transcriptions.append(\" \".join(current_text).strip())\n",
    "\n",
    "    # âœ… Step 2: Extract `.wav` filenames in order\n",
    "    wav_files = sorted([f for f in os.listdir(wavs_folder) if f.endswith(\".wav\")])\n",
    "\n",
    "    # âœ… Step 3: Zip & Align\n",
    "    if len(wav_files) != len(transcriptions):\n",
    "        print(f\"âš  Warning: Mismatch in number of WAV files ({len(wav_files)}) vs transcriptions ({len(transcriptions)}) in {metadata_path}\")\n",
    "\n",
    "    aligned_data = {wav: {\"text\": transcript, \"speaker\": speaker_name} for wav, transcript in zip(wav_files, transcriptions)}\n",
    "\n",
    "    return aligned_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# âœ… Root directory for processed dataset\n",
    "ROOT_DIR = \"m_ailabs_temp/de\"\n",
    "OUTPUT_FILE = \"m_ailabs_transcriptions_with_speakers.json\"\n",
    "\n",
    "# âœ… Store all speakers' data\n",
    "all_speakers_data = {}\n",
    "\n",
    "# âœ… Loop over each speaker's folder\n",
    "for speaker_folder in os.listdir(ROOT_DIR):\n",
    "    speaker_path = os.path.join(ROOT_DIR, speaker_folder)\n",
    "\n",
    "    # âœ… Ensure it's a folder\n",
    "    if not os.path.isdir(speaker_path):\n",
    "        continue\n",
    "\n",
    "    # âœ… Find all metadata.txt and corresponding wavs folders\n",
    "    metadata_files = {}\n",
    "    wav_folders = {}\n",
    "\n",
    "    for item in os.listdir(speaker_path):\n",
    "        item_path = os.path.join(speaker_path, item)\n",
    "\n",
    "        if item.endswith(\"_metadata.txt\"):  # âœ… Identify metadata files\n",
    "            situation_name = item.replace(\"_metadata.txt\", \"\")  # Extract situation name\n",
    "            metadata_files[situation_name] = item_path\n",
    "\n",
    "        elif item.endswith(\"_wavs\"):  # âœ… Identify wav folders\n",
    "            situation_name = item.replace(\"_wavs\", \"\")  # Extract situation name\n",
    "            wav_folders[situation_name] = item_path\n",
    "\n",
    "    # âœ… Now we process each situation (aligning metadata with wavs)\n",
    "    for situation, metadata_file in metadata_files.items():\n",
    "        if situation in wav_folders:  # âœ… Ensure matching wavs exist\n",
    "            wavs_folder = wav_folders[situation]\n",
    "            speaker_data = parse_metadata_with_wavs(metadata_file, wavs_folder)\n",
    "            all_speakers_data.update(speaker_data)\n",
    "\n",
    "print(f\"âœ… Processed {len(all_speakers_data)} audio files across all speakers.\")\n",
    "\n",
    "# âœ… Save full dataset to JSON\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_speakers_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… All speakers' transcriptions saved to: {OUTPUT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-) SET EACH LANGUAGE TO PROPER JSON ALONG WITH AUDIO INFO, TRANSCRIPTION INFO AND SPEAKERS INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ Processing Language: de\n",
      "   ğŸ¤ Speaker: rebeca\n",
      "   ğŸ¤ Speaker: karlson\n",
      "   ğŸ¤ Speaker: ramona\n",
      "   ğŸ¤ Speaker: eva\n",
      "ğŸ”¹ Processing Language: tr\n",
      "   ğŸ¤ Speaker: hakan\n",
      "   ğŸ¤ Speaker: emre\n",
      "   ğŸ¤ Speaker: nezus\n",
      "   ğŸ¤ Speaker: hulya\n",
      "   ğŸ¤ Speaker: ruya\n",
      "   ğŸ¤ Speaker: tolga\n",
      "   ğŸ¤ Speaker: melek\n",
      "   ğŸ¤ Speaker: baran\n",
      "   ğŸ¤ Speaker: ahmet\n",
      "ğŸ”¹ Processing Language: en\n",
      "   ğŸ¤ Speaker: test_noise\n",
      "   ğŸ¤ Speaker: elliot\n",
      "   ğŸ¤ Speaker: mary\n",
      "âš  Warning: Mismatch in m_ailabs_temp/en/mary/mary_northandsouth_metadata.csv â†’ WAVs: 9242, Transcripts: 9243\n",
      "   ğŸ¤ Speaker: elizabeth\n",
      "   ğŸ¤ Speaker: judy\n",
      "âœ… Processed 195657 audio files across all languages.\n",
      "âœ… Dataset saved: m_ailabs_dataset.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# âœ… Root directory containing all languages\n",
    "ROOT_DIR = \"m_ailabs_temp\"\n",
    "OUTPUT_FILE = \"lookup.json\"\n",
    "\n",
    "# âœ… Store all data (keyed by language)\n",
    "all_languages_data = {}\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "def parse_metadata_with_wavs(metadata_path, wavs_folder, file_type=\"txt\"):\n",
    "    \"\"\"\n",
    "    Reads a metadata file (TXT or CSV) and aligns transcriptions with WAV files.\n",
    "    \n",
    "    - \"emre\" uses filename-based mapping (because transcript order doesn't match WAV order).\n",
    "    - All other speakers use the original order-based approach.\n",
    "    \"\"\"\n",
    "    speaker_name = os.path.basename(os.path.dirname(metadata_path))  # âœ… Extract speaker from folder name\n",
    "    transcriptions = []\n",
    "    filename_mapping = {}\n",
    "\n",
    "    if file_type == \"txt\":\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if \"|\" in line:\n",
    "                    parts = line.split(\"|\", 1)\n",
    "                    if len(parts) == 2:\n",
    "                        filename = parts[0].strip()\n",
    "                        text = parts[1].strip()\n",
    "                        transcriptions.append(text)\n",
    "                        filename_mapping[filename] = text\n",
    "\n",
    "    elif file_type == \"csv\":\n",
    "        with open(metadata_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"|\")\n",
    "            for row in reader:\n",
    "                if len(row) < 2:\n",
    "                    continue\n",
    "                filename = row[0].strip()\n",
    "                text = row[1].strip()\n",
    "                transcriptions.append(text)\n",
    "                filename_mapping[filename] = text\n",
    "\n",
    "    # âœ… Get WAV filenames\n",
    "    wav_files = sorted([f for f in os.listdir(wavs_folder) if f.endswith(\".wav\")])\n",
    "\n",
    "    # âœ… Special case for \"emre\": Match filenames instead of order\n",
    "    if speaker_name == \"emre\":\n",
    "        aligned_data = {}\n",
    "        for wav in wav_files:\n",
    "            if wav in filename_mapping:\n",
    "                aligned_data[wav] = {\n",
    "                    \"text\": filename_mapping[wav],\n",
    "                    \"speaker\": speaker_name\n",
    "                }\n",
    "            else:\n",
    "                print(f\"âš  Warning: No transcript found for {wav} in {metadata_path}\")\n",
    "        return aligned_data\n",
    "\n",
    "    # âœ… For all other speakers: Use the order-based approach\n",
    "    if len(wav_files) != len(transcriptions):\n",
    "        print(f\"âš  Warning: Mismatch in {metadata_path} â†’ WAVs: {len(wav_files)}, Transcripts: {len(transcriptions)}\")\n",
    "\n",
    "    return {\n",
    "        wav: {\n",
    "            \"text\": transcript,\n",
    "            \"speaker\": speaker_name\n",
    "        }\n",
    "        for wav, transcript in zip(wav_files, transcriptions)\n",
    "    }\n",
    "\n",
    "\n",
    "# âœ… Process Each Language\n",
    "for language_folder in os.listdir(ROOT_DIR):\n",
    "    lang_path = os.path.join(ROOT_DIR, language_folder)\n",
    "\n",
    "    if not os.path.isdir(lang_path):\n",
    "        continue  # âœ… Skip non-folder items\n",
    "\n",
    "    print(f\"ğŸ”¹ Processing Language: {language_folder}\")\n",
    "\n",
    "    all_languages_data[language_folder] = {}\n",
    "\n",
    "    # âœ… Process Each Speaker in the Language\n",
    "    for speaker_folder in os.listdir(lang_path):\n",
    "        speaker_path = os.path.join(lang_path, speaker_folder)\n",
    "\n",
    "        if not os.path.isdir(speaker_path):\n",
    "            continue\n",
    "\n",
    "        print(f\"   ğŸ¤ Speaker: {speaker_folder}\")\n",
    "\n",
    "        metadata_files = {}\n",
    "        wav_folders = {}\n",
    "\n",
    "        # âœ… Find Metadata & Wavs\n",
    "        for item in os.listdir(speaker_path):\n",
    "            item_path = os.path.join(speaker_path, item)\n",
    "\n",
    "            if item.endswith(\"_metadata.txt\"):\n",
    "                situation_name = item.replace(\"_metadata.txt\", \"\")\n",
    "                metadata_files[situation_name] = (item_path, \"txt\")\n",
    "\n",
    "            elif item.endswith(\"_metadata.csv\"):\n",
    "                situation_name = item.replace(\"_metadata.csv\", \"\")\n",
    "                metadata_files[situation_name] = (item_path, \"csv\")\n",
    "\n",
    "            elif item.endswith(\"_wavs\"):\n",
    "                situation_name = item.replace(\"_wavs\", \"\")\n",
    "                wav_folders[situation_name] = item_path\n",
    "\n",
    "        # âœ… Process Each Speakerâ€™s Situations\n",
    "        for situation, (metadata_file, file_type) in metadata_files.items():\n",
    "            if situation in wav_folders:\n",
    "                wavs_folder = wav_folders[situation]\n",
    "\n",
    "                # âœ… Get Transcription Data\n",
    "                speaker_data = parse_metadata_with_wavs(metadata_file, wavs_folder, file_type)\n",
    "\n",
    "                # âœ… Store Under Language â†’ Speaker\n",
    "                all_languages_data[language_folder].update(speaker_data)\n",
    "                \n",
    "\n",
    "print(f\"âœ… Processed {sum(len(data) for data in all_languages_data.values())} audio files across all languages.\")\n",
    "\n",
    "# âœ… Save to JSON\n",
    "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_languages_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"âœ… Dataset saved: {OUTPUT_FILE}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-) ALIGNMENT FOR AUDIO AND TRANSCRIPT TEXT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A- Load whisperX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisperx\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import argparse\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "compute_type = \"float16\"\n",
    "\n",
    "\n",
    "# Load the model and aligner\n",
    "def load_model_with_location(model_name, device, compute_type):\n",
    "    \"\"\"Loads the model and prints its location.\"\"\"\n",
    "    print(f\"Loading model '{model_name}' with compute type '{compute_type}'...\")\n",
    "    cache_dir = os.path.expanduser(\"~/.cache/whisperx-models\")\n",
    "    model = whisperx.load_model(model_name, device, compute_type=compute_type, download_root=cache_dir)\n",
    "    print(f\"Model loaded from: {cache_dir}\")\n",
    "    return model\n",
    "\n",
    "# Load the model, and print the model location\n",
    "model = load_model_with_location(\"medium\", device, compute_type=compute_type)\n",
    "\n",
    "\n",
    "\"\"\" audio = whisperx.load_audio(audio_path)\n",
    "\n",
    "# Transcribe and align the audio.\n",
    "result = model.transcribe(audio)\n",
    "\n",
    "model_a, metadata = whisperx.load_align_model(language_code=result[\"language\"], device=device)\n",
    "\n",
    "result = whisperx.align(result[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n",
    "\n",
    "print(result) \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Control transcript Quality with Cosine Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"m_ailabs_temp\" \n",
    "def find_wav_file(file_name, dataset, language):\n",
    "    \"\"\"Finds the correct speaker folder and corresponding wav file path.\"\"\"\n",
    "    if language not in dataset:\n",
    "        print(f\"âš  Warning: Language '{language}' not found in dataset JSON!\")\n",
    "        return None\n",
    "\n",
    "    if file_name not in dataset[language]:\n",
    "        print(f\"âš  Warning: File {file_name} not found in dataset[{language}]!\")\n",
    "        return None  # Skip this file\n",
    "\n",
    "    speaker_name = dataset[language][file_name][\"speaker\"]\n",
    "    speaker_path = os.path.join(DATASET_PATH, language, speaker_name)\n",
    "\n",
    "    # âœ… Check all folders inside the speaker directory\n",
    "    for folder in os.listdir(speaker_path):\n",
    "        folder_path = os.path.join(speaker_path, folder)\n",
    "\n",
    "        # âœ… Find the correct wavs folder\n",
    "        if os.path.isdir(folder_path) and folder.endswith(\"_wavs\"):\n",
    "            wav_file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # âœ… If the file exists, return it\n",
    "            if os.path.exists(wav_file_path):\n",
    "                return wav_file_path\n",
    "\n",
    "    print(f\"âš ï¸ Warning: File {file_name} not found in speaker {speaker_name} folder.\")\n",
    "    return None  # File not found\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import Levenshtein \n",
    "\n",
    "# âœ… Load a strong word embedding model (Multilingual)\n",
    "model2 = SentenceTransformer(\"distiluse-base-multilingual-cased-v1\")\n",
    "def is_semantic_difference(text1, text2, threshold=0.85):\n",
    "    \"\"\"\n",
    "    Uses word embeddings to check if two sentences are semantically different.\n",
    "    If similarity is below `threshold`, flag it.\n",
    "    \"\"\"\n",
    "    emb1 = model2.encode(text1, convert_to_tensor=True)\n",
    "    emb2 = model2.encode(text2, convert_to_tensor=True)\n",
    "    similarity = util.pytorch_cos_sim(emb1, emb2).item()  # Cosine similarity\n",
    "    \n",
    "    return similarity < threshold  # Flag if too different\n",
    "def levenshtein_similarity(text1, text2):\n",
    "    \"\"\"Calculates Levenshtein similarity between two strings.\"\"\"\n",
    "    return Levenshtein.ratio(text1, text2)  # Returns a float between 0 and 1\n",
    "\n",
    "#def transcribe_groq(audio_path):\n",
    "    \"\"\"Sends an audio file to Groq's Whisper Large v3 API and gets the transcript.\"\"\"\n",
    "    #with open(audio_path, \"rb\") as audio_file:\n",
    "        #response = client.audio.transcriptions.create(\n",
    "            #model=\"whisper-large-v3\",\n",
    "            #file=audio_file,\n",
    "            #language=LANGUAGE,\n",
    "        #)\n",
    "    #return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import whisperx\n",
    "\n",
    "# âœ… WhisperX Model (Use the same model you used before)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "cache_dir = os.path.expanduser(\"~/.cache/whisperx-models\")\n",
    "model = whisperx.load_model(\"large-v2\", device, compute_type=\"float16\", download_root=cache_dir)\n",
    "\n",
    "# âœ… Define the dataset path\n",
    "DATASET_PATH = \"m_ailabs_temp/en\"\n",
    "LANGUAGE = \"en\"\n",
    "TARGET_SPEAKER = \"judy\"  # âœ… Only process \"judy\"\n",
    "\n",
    "# âœ… Load dataset JSON\n",
    "with open(\"m_ailabs_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# âœ… Function to find correct WAV file for Judy only\n",
    "def find_wav_file(file_name, dataset, language):\n",
    "    \"\"\"Finds the correct speaker folder and corresponding wav file path.\"\"\"\n",
    "    if file_name not in dataset[language]:\n",
    "        print(f\"âš  Warning: File {file_name} not found in dataset[{language}]!\")\n",
    "        return None  # Skip this file\n",
    "\n",
    "    speaker_name = dataset[language][file_name][\"speaker\"]\n",
    "    \n",
    "    # âœ… Only process Judy's folder, skip others\n",
    "    if speaker_name.lower() != TARGET_SPEAKER:\n",
    "        return None  # Skip non-Judy files\n",
    "    \n",
    "    speaker_path = os.path.join(DATASET_PATH, speaker_name)\n",
    "\n",
    "    # âœ… Search for `_wavs` folders dynamically\n",
    "    for folder in os.listdir(speaker_path):\n",
    "        folder_path = os.path.join(speaker_path, folder)\n",
    "        if os.path.isdir(folder_path) and folder.endswith(\"_wavs\"):\n",
    "            wav_file_path = os.path.join(folder_path, file_name)\n",
    "            if os.path.exists(wav_file_path):\n",
    "                return wav_file_path\n",
    "\n",
    "    print(f\"âš ï¸ Warning: File {file_name} not found for speaker {speaker_name}.\")\n",
    "    return None  # File not found\n",
    "\n",
    "# âœ… Process only Judy's files\n",
    "flagged_files = []\n",
    "\n",
    "for i, file_name in enumerate(dataset[LANGUAGE]):\n",
    "    file_path = find_wav_file(file_name, dataset, LANGUAGE)\n",
    "\n",
    "    if file_path is None:\n",
    "        continue  # Skip non-Judy files\n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"âš  ERROR: File {file_path} not found, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # âœ… Run WhisperX Transcription\n",
    "    audio = whisperx.load_audio(file_path)\n",
    "    result = model.transcribe(audio, language=LANGUAGE)\n",
    "\n",
    "    whisper_transcript = result[\"segments\"][0][\"text\"] if result[\"segments\"] else \"\"\n",
    "    original_transcript = dataset[LANGUAGE][file_name][\"text\"]\n",
    "\n",
    "    # âœ… Compare & flag mismatches\n",
    "    if whisper_transcript.lower() != original_transcript.lower():\n",
    "        flagged_files.append({\n",
    "            \"file\": file_name,\n",
    "            \"whisper_transcript\": whisper_transcript,\n",
    "            \"original_transcript\": original_transcript\n",
    "        })\n",
    "\n",
    "    print(f\"ğŸ” Processed {i+1} files for Judy...\", end=\"\\r\")\n",
    "\n",
    "# âœ… Save flagged files\n",
    "with open(\"flagged_transcriptions_judy.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(flagged_files, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"\\nâœ… WhisperX test completed for Judy.\")\n",
    "print(f\"ğŸš© {len(flagged_files)} files flagged for review.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-) Remove Noise from wav Records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize random wavs for Noise detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "# âœ… Step 1: Select a random sample from the dataset\n",
    "DATASET_PATH = \"m_ailabs_temp/de\"  # Change to \"en\" when testing English\n",
    "speakers = os.listdir(DATASET_PATH)\n",
    "selected_speaker = random.choice(speakers)  # Pick a random speaker\n",
    "speaker_path = os.path.join(DATASET_PATH, selected_speaker)\n",
    "\n",
    "# âœ… Find a valid `_wavs` folder for the selected speaker\n",
    "wavs_folders = [f for f in os.listdir(speaker_path) if f.endswith(\"_wavs\")]\n",
    "selected_wav_folder = os.path.join(speaker_path, random.choice(wavs_folders))\n",
    "\n",
    "# âœ… Pick a random .wav file\n",
    "wav_files = [f for f in os.listdir(selected_wav_folder) if f.endswith(\".wav\")]\n",
    "selected_wav = os.path.join(selected_wav_folder, random.choice(wav_files))\n",
    "\n",
    "# âœ… Step 2: Load the audio\n",
    "y, sr = librosa.load(selected_wav, sr=None)\n",
    "\n",
    "# âœ… Step 3: Plot Waveform\n",
    "plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title(f\"Waveform of {os.path.basename(selected_wav)}\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "librosa.display.specshow(D, sr=sr, x_axis=\"time\", y_axis=\"log\", cmap=\"magma\")\n",
    "plt.colorbar(format=\"%+2.0f dB\")\n",
    "plt.title(f\"Spectrogram of {os.path.basename(selected_wav)}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ğŸ”¹ Path to your processed dataset (Adjust for different speakers)\n",
    "AUDIO_DIR = \"m_ailabs_temp/en/elliot/elliot_pink_fairy_wavs\"\n",
    "\n",
    "# ğŸ”¹ List first 5 files in the directory\n",
    "audio_files = [f for f in os.listdir(AUDIO_DIR) if f.endswith(\".wav\")][:5]\n",
    "\n",
    "# ğŸ”¹ Increase the figure size for better visualization\n",
    "fig, axes = plt.subplots(len(audio_files), 1, figsize=(12, 4 * len(audio_files)))\n",
    "\n",
    "for i, file in enumerate(audio_files):\n",
    "    file_path = os.path.join(AUDIO_DIR, file)\n",
    "    y, sr = librosa.load(file_path, sr=None)  # Load audio\n",
    "\n",
    "    # **Normalize the waveform amplitude separately for each file**\n",
    "    y = y / np.max(np.abs(y))  # Scale amplitude between -1 and 1\n",
    "    y = y * 0.9  # Make waves fill more space (but prevent clipping)\n",
    "\n",
    "    axes[i].plot(y, color=\"blue\", linewidth=1.8)\n",
    "    axes[i].set_title(f\"ğŸ”‰ {file}\", fontsize=14, fontweight='bold')\n",
    "    axes[i].set_xlim([0, len(y)])\n",
    "    axes[i].set_ylim([-1, 1])  # **Each wave now fills the full Y space**\n",
    "    axes[i].set_ylabel(\"Amplitude\", fontsize=12)\n",
    "\n",
    "# ğŸ”¹ Reduce gaps & improve layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove 5 wavs noise and compare them with original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Paths and file selection\n",
    "SOURCE_DIR = \"m_ailabs_temp/en/elliot/elliot_pink_fairy_wavs\"\n",
    "TARGET_DIR = \"m_ailabs_temp/en/test_noise\"\n",
    "os.makedirs(TARGET_DIR, exist_ok=True)\n",
    "\n",
    "selected_files = [\n",
    "    \"pink_fairy_book_32_f000111.wav\",\n",
    "    \"pink_fairy_book_26_f000011.wav\",\n",
    "    \"pink_fairy_book_09_f000022.wav\",\n",
    "    \"pink_fairy_book_19_f000009.wav\",\n",
    "    \"pink_fairy_book_19_f000007.wav\"\n",
    "]\n",
    "\n",
    "def noise_reduction(y, sr, noise_reduction_factor=0.05):\n",
    "    \"\"\"\n",
    "    Apply noise reduction using spectral gating.\n",
    "    \"\"\"\n",
    "    # Generate the spectral gating threshold\n",
    "    stft = librosa.stft(y)\n",
    "    magnitude, phase = librosa.magphase(stft)\n",
    "    spectral_threshold = noise_reduction_factor * np.median(np.abs(magnitude), axis=1, keepdims=True)\n",
    "    \n",
    "    # Reduce noise by gating the spectrogram\n",
    "    denoised_stft = np.where(magnitude > spectral_threshold, stft, 0)\n",
    "    y_denoised = librosa.istft(denoised_stft)\n",
    "    \n",
    "    return y_denoised\n",
    "\n",
    "def process_and_save(file_name):\n",
    "    # Full paths\n",
    "    source_path = os.path.join(SOURCE_DIR, file_name)\n",
    "    target_path = os.path.join(TARGET_DIR, file_name)\n",
    "    \n",
    "    # Load audio\n",
    "    y, sr = librosa.load(source_path, sr=None)\n",
    "    \n",
    "    # Apply noise reduction\n",
    "    y_denoised = noise_reduction(y, sr)\n",
    "    \n",
    "    # Save denoised audio\n",
    "    sf.write(target_path, y_denoised, sr)\n",
    "    print(f\"âœ… Saved denoised file: {target_path}\")\n",
    "    \n",
    "    return y, y_denoised, sr\n",
    "\n",
    "# Visualization function\n",
    "def visualize_waveforms(original, denoised, sr, file_name):\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Original waveform\n",
    "    plt.subplot(2, 1, 1)\n",
    "    librosa.display.waveshow(original, sr=sr)\n",
    "    plt.title(f\"Original: {file_name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    # Denoised waveform\n",
    "    plt.subplot(2, 1, 2)\n",
    "    librosa.display.waveshow(denoised, sr=sr, color=\"orange\")\n",
    "    plt.title(f\"Denoised: {file_name}\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Process files and visualize\n",
    "for file_name in selected_files:\n",
    "    try:\n",
    "        original, denoised, sr = process_and_save(file_name)\n",
    "        visualize_waveforms(original, denoised, sr, file_name)\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing {file_name}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4-) Load Turkish Audio Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset_name = \"erenfazlioglu/turkishvoicedataset\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# 2. Inspect the dataset structure (optional, but good to verify)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"].features)\n",
    "\n",
    "# 3. Define a function to extract and save data\n",
    "def extract_audio_and_transcription(example, index, output_dir=\"extracted_data\"):\n",
    "    \"\"\"Extracts audio and transcription from a dataset example.\"\"\"\n",
    "    audio = example[\"audio\"]\n",
    "    transcription = example[\"transcription\"]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create filenames using the index as a unique ID\n",
    "    audio_filename = os.path.join(output_dir, f\"audio_{index:06d}.wav\")  # Format index to 6 digits with leading zeros\n",
    "    transcription_filename = os.path.join(output_dir, f\"transcription_{index:06d}.txt\")\n",
    "\n",
    "    # Save the audio\n",
    "    sf.write(audio_filename, audio[\"array\"], audio[\"sampling_rate\"])\n",
    "\n",
    "    # Save the transcription\n",
    "    with open(transcription_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcription)\n",
    "\n",
    "    return audio_filename, transcription_filename\n",
    "\n",
    "\n",
    "# 4. Process the dataset (or a subset for testing)\n",
    "output_directory = \"extracted_turkish_audio\"\n",
    "# Process the first 100 examples for testing\n",
    "extracted_files = []\n",
    "\n",
    "# We need to use enumerate() correctly to pass the index\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "   \n",
    "    try:\n",
    "        audio_file, transcription_file = extract_audio_and_transcription(example, i, output_directory) # Pass the index!\n",
    "        extracted_files.append((audio_file, transcription_file))\n",
    "        print(f\"Extracted example {i+1}: Audio - {audio_file}, Transcription - {transcription_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtracted {len(extracted_files)} audio and transcription files to '{output_directory}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "# 1. Load the dataset\n",
    "dataset_name = \"rakshya34/filtered_english_female_voice_v1\"\n",
    "dataset = load_dataset(dataset_name)\n",
    "\n",
    "# 2. Inspect the dataset structure (optional, but good to verify)\n",
    "print(dataset)\n",
    "print(dataset[\"train\"].features)\n",
    "\n",
    "# 3. Define a function to extract and save data\n",
    "def extract_audio_and_transcription(example, index, output_dir=\"extracted_data\"):\n",
    "    \"\"\"Extracts audio and transcription from a dataset example.\"\"\"\n",
    "    audio = example[\"audio\"]\n",
    "    # Changed this line: Use \"text\" instead of \"transcription\"\n",
    "    transcription = example[\"text\"]\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Create filenames using the index as a unique ID\n",
    "    audio_filename = os.path.join(output_dir, f\"audio_{index:06d}.wav\")  # Format index to 6 digits with leading zeros\n",
    "    transcription_filename = os.path.join(output_dir, f\"transcription_{index:06d}.txt\")\n",
    "\n",
    "    # Save the audio\n",
    "    sf.write(audio_filename, audio[\"array\"], audio[\"sampling_rate\"])\n",
    "\n",
    "    # Save the transcription\n",
    "    with open(transcription_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(transcription)\n",
    "\n",
    "    return audio_filename, transcription_filename\n",
    "\n",
    "\n",
    "# 4. Process the dataset (or a subset for testing)\n",
    "output_directory = \"extracted_turkish_audio\"\n",
    "# Process the first 100 examples for testing\n",
    "extracted_files = []\n",
    "\n",
    "# We need to use enumerate() correctly to pass the index\n",
    "for i, example in enumerate(dataset[\"train\"]):\n",
    "   \n",
    "    try:\n",
    "        audio_file, transcription_file = extract_audio_and_transcription(example, i, output_directory) # Pass the index!\n",
    "        extracted_files.append((audio_file, transcription_file))\n",
    "        print(f\"Extracted example {i+1}: Audio - {audio_file}, Transcription - {transcription_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing example {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nExtracted {len(extracted_files)} audio and transcription files to '{output_directory}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5-) Extract Turkish audio from mozilla Corpora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# âœ… Define paths\n",
    "DATASET_DIR = \"tr\"  # Make sure this is the correct path\n",
    "AUDIO_DIR = os.path.join(DATASET_DIR, \"clips\")\n",
    "TSV_FILES = [\"train.tsv\", \"test.tsv\", \"other.tsv\"]  # These should be in `tr/`\n",
    "\n",
    "# âœ… Check if TSV files exist\n",
    "for tsv_file in TSV_FILES:\n",
    "    tsv_path = os.path.join(DATASET_DIR, tsv_file)\n",
    "    if not os.path.exists(tsv_path):\n",
    "        print(f\"âŒ ERROR: {tsv_file} not found in {DATASET_DIR}!\")\n",
    "        exit(1)\n",
    "\n",
    "# âœ… Function to filter valid audio samples\n",
    "def filter_valid_samples(tsv_path):\n",
    "    \"\"\"Filters samples with high up_votes and low down_votes.\"\"\"\n",
    "    df = pd.read_csv(tsv_path, sep=\"\\t\")\n",
    "\n",
    "    # âœ… Keep only samples with at least 2 upvotes and 0 downvotes\n",
    "    filtered_df = df[(df[\"up_votes\"] >= 2) & (df[\"down_votes\"] == 0)]\n",
    "\n",
    "    print(f\"âœ… {len(filtered_df)} valid samples found in {os.path.basename(tsv_path)}\")\n",
    "    return filtered_df\n",
    "\n",
    "# âœ… Process all TSV files\n",
    "for tsv_file in TSV_FILES:\n",
    "    tsv_path = os.path.join(DATASET_DIR, tsv_file)\n",
    "    filtered_df = filter_valid_samples(tsv_path)\n",
    "\n",
    "    # âœ… Save the cleaned dataset\n",
    "    output_path = os.path.join(DATASET_DIR, f\"filtered_{tsv_file}\")\n",
    "    filtered_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    print(f\"âœ… Filtered data saved to: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… Define paths\n",
    "DATASET_PATH = \"tr\"\n",
    "CLIPS_PATH = os.path.join(DATASET_PATH, \"clips\")  # MP3 files location\n",
    "OUTPUT_PATH = os.path.join(DATASET_PATH, \"processed_speakers\")\n",
    "\n",
    "# âœ… Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# âœ… Load the filtered TSV files\n",
    "train_tsv = pd.read_csv(os.path.join(DATASET_PATH, \"filtered_train.tsv\"), sep=\"\\t\")\n",
    "test_tsv = pd.read_csv(os.path.join(DATASET_PATH, \"filtered_test.tsv\"), sep=\"\\t\")\n",
    "\n",
    "# âœ… Combine train & test sets\n",
    "filtered_data = pd.concat([train_tsv, test_tsv])\n",
    "\n",
    "# âœ… Dictionary to track speaker folders\n",
    "speaker_folders = {}\n",
    "\n",
    "# âœ… Process each row\n",
    "for _, row in tqdm(filtered_data.iterrows(), total=len(filtered_data), desc=\"Processing Speakers\"):\n",
    "    client_id, mp3_filename, _, transcription = row[:4]\n",
    "\n",
    "    # âœ… Define speaker folder\n",
    "    speaker_folder = os.path.join(OUTPUT_PATH, f\"speaker_{client_id}\")\n",
    "    wavs_folder = os.path.join(speaker_folder, f\"speaker_{client_id}_wavs\")\n",
    "\n",
    "    # âœ… Ensure directories exist\n",
    "    os.makedirs(wavs_folder, exist_ok=True)\n",
    "\n",
    "    # âœ… Convert MP3 to WAV (Store in speaker_wavs folder)\n",
    "    mp3_path = os.path.join(CLIPS_PATH, mp3_filename)\n",
    "    wav_filename = os.path.splitext(mp3_filename)[0] + \".wav\"\n",
    "    wav_path = os.path.join(wavs_folder, wav_filename)\n",
    "\n",
    "    try:\n",
    "        audio, sr = librosa.load(mp3_path, sr=24000)  # CommonVoice uses 24kHz\n",
    "        sf.write(wav_path, audio, sr)\n",
    "\n",
    "        # âœ… Store transcription\n",
    "        metadata_path = os.path.join(speaker_folder, f\"speaker_{client_id}_metadata.txt\")\n",
    "        with open(metadata_path, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(f\"{wav_filename}|{transcription}\\n\")\n",
    "\n",
    "        # âœ… Track processed speakers\n",
    "        speaker_folders[speaker_folder] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âš  Error processing {mp3_filename}: {e}\")\n",
    "\n",
    "# âœ… Show results\n",
    "num_speakers = len(speaker_folders)\n",
    "print(f\"âœ… Processing complete! {num_speakers} unique speakers organized.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### remove The speakers who have less than 50 audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "\n",
    "# âœ… Path to processed Mozilla dataset (where speakers are stored)\n",
    "DATASET_PATH = \"tr/processed_speakers\"  # Update if needed\n",
    "\n",
    "# âœ… Count files per speaker\n",
    "speaker_counts = {spk: len(os.listdir(os.path.join(DATASET_PATH, spk, f\"{spk}_wavs\")))\n",
    "                  for spk in os.listdir(DATASET_PATH)\n",
    "                  if os.path.isdir(os.path.join(DATASET_PATH, spk))}\n",
    "\n",
    "# âœ… Sort by most recordings\n",
    "sorted_speakers = sorted(speaker_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# âœ… Filtering: Only keep speakers with at least X samples\n",
    "MIN_AUDIO_FILES = 50  # âœ… Set minimum threshold (change as needed)\n",
    "valid_speakers = {spk: count for spk, count in sorted_speakers if count >= MIN_AUDIO_FILES}\n",
    "\n",
    "# âœ… Print summary\n",
    "print(\"\\nğŸ™ï¸ **Speaker Data Distribution (Mozilla TR)** ğŸ™ï¸\")\n",
    "print(f\"ğŸ“Š Total Speakers: {len(speaker_counts)}\")\n",
    "print(f\"âœ… Speakers with >= {MIN_AUDIO_FILES} samples: {len(valid_speakers)}\")\n",
    "print(\"ğŸ”¹ Top 10 Speakers by Data:\")\n",
    "\n",
    "for i, (spk, count) in enumerate(list(valid_speakers.items())[:10]):\n",
    "    print(f\"  {i+1}. {spk}: {count} samples\")\n",
    "\n",
    "# âœ… Decide next steps:\n",
    "remaining_speakers = len(speaker_counts) - len(valid_speakers)\n",
    "print(f\"\\nâš ï¸ {remaining_speakers} speakers have too few samples and will be excluded.\")\n",
    "\n",
    "# âœ… Evaluate HF dataset addition\n",
    "HF_TARGET_COUNT = 20000  # âœ… Decide how much we need from HF\n",
    "current_total = sum(valid_speakers.values())\n",
    "\n",
    "print(f\"\\nğŸ” Current total audio samples: {current_total}\")\n",
    "if current_total < HF_TARGET_COUNT:\n",
    "    needed = HF_TARGET_COUNT - current_total\n",
    "    print(f\"âš¡ Need to add {needed} samples from HF dataset.\")\n",
    "\n",
    "print(\"\\nğŸš€ Ready for next steps!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = \"tr/processed_speakers/\"\n",
    "\n",
    "# Define the top 10 speakers to keep\n",
    "top_speakers = {\n",
    "    \"speaker_ba25f19f22267b1f863cef586f17252a9067bc0ecb3d63dd7626fbce7014a16412dd64d5404d34505c0c90635adbc3cb2d830d77291a4db194e5abad91160957\",\n",
    "    \"speaker_ca179eb54e4e584d30587ba3c6c1d4de7d9082d649113d481d308f3e7d858e0a1df21d9591f9c613928b1e122aa7f54535d0f0e0c1380d87d8017844e6e73ea2\",\n",
    "    \"speaker_6cf4dea5df1dfe5dc5d7bacdb6da105b9e48e01fd0f069dbaf0ad906a7c5d78032194a3a9384e8589b93c58a04a12f676669f4c4501e52cee652f331b72b9205\",\n",
    "    \"speaker_b0ec03dc36c7ef4ef29409671a91c488e78765d3a17d32b778c7e4425c32026f9341a6e73f11cda3c0cd7ed97f4e5dc3ec6b8b099098c9ddfb21345a0c39eb07\",\n",
    "    \"speaker_60cee2235d7ec4cdeb89d601b8c373955b303c712ec729ed0affdabda8819908f51cefa990163d2bc4ac04c93e6dac2909cca67829211df4a2a17af2507dd50a\",\n",
    "    \"speaker_c3c204ffaebfc46c0265773376f9288a372694aa79f97afe224828033c28d6d2a90919aaf769bf14105cb4033650e10275760afe250490782eae15c1d1518799\",\n",
    "    \"speaker_509e62ded4e7e780266e54e2bb68c57f9311ded1ab57def015ff9a60de6d2efbc23adcd71e845d4c501938614bd5290d248a6202c3e7c5f7543af861b57fdcf9\",\n",
    "    \"speaker_5d0347c54aae3292ec3a39a8bef0c8cc5e02f4c86ddfdf9dacf3a51cd8b68ca9e0f645b88ac280a1e4b25200de0771ddac89962debaf9d666b6330ac58075e40\",\n",
    "    \"speaker_618f55a108c81342ad0f4e011b4346cc4c330158c46ef0105cc2d37a986b9025426aae64a2dc944896c5847c761f0d254b950559620f0605a5a813cf241d2c82\",\n",
    "    \"speaker_137102afdef7dc128fc101a2c19623d189087e7e3cf388c08fa5298349682fc8e9b7130a4ae2f4d8fdf8eaa584afd95f9b1df35dff651ad53c479703f3d83ecf\"\n",
    "}\n",
    "\n",
    "# List all speaker folders\n",
    "all_speakers = os.listdir(DATASET_PATH)\n",
    "\n",
    "# Remove unwanted speakers\n",
    "removed_speakers = []\n",
    "for speaker in all_speakers:\n",
    "    speaker_path = os.path.join(DATASET_PATH, speaker)\n",
    "    if os.path.isdir(speaker_path) and speaker not in top_speakers:\n",
    "        shutil.rmtree(speaker_path)  # Remove entire folder\n",
    "        removed_speakers.append(speaker)\n",
    "\n",
    "# Output summary\n",
    "len_removed = len(removed_speakers)\n",
    "len_remaining = len(top_speakers)\n",
    "{\n",
    "    \"removed_speakers\": len_removed,\n",
    "    \"remaining_speakers\": len_remaining,\n",
    "    \"total_speakers_now\": len(os.listdir(DATASET_PATH))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check some audio files Herz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "\n",
    "# âœ… Define some sample WAV files from each dataset\n",
    "samples = {\n",
    "    \"DE-1\": \"m_ailabs_temp/de/rebeca/rebeca_das_letzte_marchen_wavs/das_letzte_marchen_001_f000001.wav\",\n",
    "    \"DE-2\": \"m_ailabs_temp/de/karlson/karlson_altehous_wavs/altehaus_001_f000054.wav\",\n",
    "    \"DE-3\": \"m_ailabs_temp/de/ramona/ramona_alter_afrikaner_wavs/alter_afrikaner_01_f000055.wav\",\n",
    "    \"DE-4\": \"m_ailabs_temp/de/eva/eva_grune_haus_wavs/grune_haus_02_f000076.wav\",\n",
    "    \"EN-1\": \"m_ailabs_temp/en/mary/mary_midnight_wavs/midnight_passenger_01_f000001.wav\",\n",
    "    \"EN-2\": \"m_ailabs_temp/en/elliot/elliot_hunter_space_wavs/hunters_space_03_f000041.wav\",\n",
    "    \"EN-3\": \"m_ailabs_temp/en/judy/judy_dorothy_wizard_wavs/dorothy_and_wizard_oz_01_f000052.wav\",\n",
    "    \"EN-4\": \"m_ailabs_temp/en/elizabeth/elizabeth_jane_eyre_wavs/jane_eyre_01_f000068.wav\",\n",
    "    \"TR\": \"m_ailabs_temp/tr/ahmet/ahmet_wavs/common_voice_tr_30332345.wav\",\n",
    "    \"TR-2\": \"m_ailabs_temp/tr/emre/emre_wavs/audio_045851.wav\",\n",
    "    \"TR-3\": \"m_ailabs_temp/tr/emre/emre_wavs/audio_050000.wav\",\n",
    "    \"TR-4\": \"m_ailabs_temp/tr/emre/emre_wavs/audio_054310.wav\",\n",
    "    \"TR-5\": \"m_ailabs_temp/tr/emre/emre_wavs/audio_032427.wav\",\n",
    "    \"TR-6\": \"m_ailabs_temp/tr/emre/emre_wavs/audio_033125.wav\",\n",
    "}\n",
    "\n",
    "# âœ… Check the sample rates\n",
    "for lang, file_path in samples.items():\n",
    "    audio, sr = librosa.load(file_path, sr=None)  # âš ï¸ Load with original SR\n",
    "    print(f\"ğŸ™ï¸ {lang} Sampling Rate: {sr} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” XTTS Expected Sampling Rate: 22050 Hz\n"
     ]
    }
   ],
   "source": [
    "from TTS.tts.models.xtts import Xtts\n",
    "from TTS.tts.configs.xtts_config import XttsConfig\n",
    "\n",
    "# âœ… Load XTTS config (No need to load full model)\n",
    "config = XttsConfig()\n",
    "print(f\"ğŸ” XTTS Expected Sampling Rate: {config.audio.sample_rate} Hz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "TARGET_SR = 22050  # XTTS expected sampling rate\n",
    "DATASET_PATH = \"m_ailabs_temp\"\n",
    "\n",
    "def resample_audio(dataset_path, lang):\n",
    "    \"\"\" Convert all audio files in dataset to 22050Hz. \"\"\"\n",
    "    print(f\"ğŸš€ Processing {lang} dataset...\")\n",
    "\n",
    "    # Go through all speaker folders\n",
    "    for speaker in os.listdir(os.path.join(dataset_path, lang)):\n",
    "        if speaker==\"emre\":\n",
    "            speaker_path = os.path.join(dataset_path, lang, speaker)\n",
    "            \n",
    "            # Find the wavs folder\n",
    "            for folder in os.listdir(speaker_path):\n",
    "                folder_path = os.path.join(speaker_path, folder)\n",
    "                \n",
    "                if os.path.isdir(folder_path) and folder.endswith(\"_wavs\"):\n",
    "                    for file in os.listdir(folder_path):\n",
    "                        if file.endswith(\".wav\"):\n",
    "                            file_path = os.path.join(folder_path, file)\n",
    "                            \n",
    "                            # Load and check sampling rate\n",
    "                            audio, sr = librosa.load(file_path, sr=None)  # Keep original SR\n",
    "                            if sr != TARGET_SR:\n",
    "                                print(f\"ğŸ”„ Resampling {file} ({sr} Hz â†’ {TARGET_SR} Hz)\")\n",
    "                                audio = librosa.resample(audio, orig_sr=sr, target_sr=TARGET_SR)\n",
    "                                sf.write(file_path, audio, TARGET_SR)\n",
    "\n",
    "    print(f\"âœ… {lang} dataset converted to {TARGET_SR}Hz!\\n\")\n",
    "\n",
    "# ğŸ”¥ Run for all languages\n",
    "for lang in [\"tr\"]:\n",
    "    resample_audio(DATASET_PATH, lang)\n",
    "\n",
    "print(\"ğŸ¯ All datasets successfully standardized to 22050Hz!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run the function due to execution state reset\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# âœ… Define source & destination paths\n",
    "SRC_AUDIO_DIR = \"extracted_turkish_audio/\"\n",
    "DST_SPEAKER_DIR = \"m_ailabs_temp/tr/emre/\"\n",
    "DST_WAVS_DIR = os.path.join(DST_SPEAKER_DIR, \"emre_wavs\")\n",
    "DST_METADATA_FILE = os.path.join(DST_SPEAKER_DIR, \"emre_metadata.txt\")\n",
    "\n",
    "# âœ… Ensure destination folders exist\n",
    "os.makedirs(DST_WAVS_DIR, exist_ok=True)\n",
    "\n",
    "# âœ… Filtering criteria\n",
    "MIN_WORD_COUNT = 15  # At least 15 words\n",
    "\n",
    "# âœ… Process files\n",
    "selected_samples = []\n",
    "for file in os.listdir(SRC_AUDIO_DIR):\n",
    "    if file.endswith(\".txt\"):  # Process transcription files only\n",
    "        txt_path = os.path.join(SRC_AUDIO_DIR, file)\n",
    "        wav_file = file.replace(\"transcription\", \"audio\").replace(\".txt\", \".wav\")\n",
    "        wav_path = os.path.join(SRC_AUDIO_DIR, wav_file)\n",
    "\n",
    "        if not os.path.exists(wav_path):  # Ensure corresponding WAV file exists\n",
    "            continue\n",
    "\n",
    "        with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            transcription = f.read().strip()\n",
    "\n",
    "        # âœ… Check word count\n",
    "        if len(transcription.split()) >= MIN_WORD_COUNT:\n",
    "            # âœ… Move WAV file to destination\n",
    "            shutil.move(wav_path, os.path.join(DST_WAVS_DIR, wav_file))\n",
    "            selected_samples.append(f\"{wav_file}|{transcription}\")\n",
    "\n",
    "# âœ… Write metadata file\n",
    "with open(DST_METADATA_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(selected_samples))\n",
    "\n",
    "# âœ… Output summary\n",
    "num_files_moved = len(selected_samples)\n",
    "DST_WAVS_DIR, DST_METADATA_FILE, num_files_moved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
