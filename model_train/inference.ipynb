{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
      "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts_2 = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def resample_wav(input_wav, output_wav, target_sr=44100):\n",
    "    \"\"\"\n",
    "    Resample a WAV file to the target sample rate.\n",
    "\n",
    "    :param input_wav: Path to the input WAV file (e.g., \"output_22k.wav\")\n",
    "    :param output_wav: Path to the resampled output WAV file (e.g., \"output_44k.wav\")\n",
    "    :param target_sr: Target sample rate (default: 44100Hz)\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(input_wav, sr=None)  # Load with original SR\n",
    "\n",
    "    # Resample audio\n",
    "    resampled_audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Save the resampled audio\n",
    "    sf.write(output_wav, resampled_audio, target_sr)\n",
    "\n",
    "    print(f\"✅ Resampled {input_wav} from {sr}Hz to {target_sr}Hz → Saved as {output_wav}\")\n",
    "\n",
    "# Example usage after cloning:\n",
    "# Assuming cloned output is saved as \"output_22k.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Manchmal ist es besser, nur die für den Sprecher relevanten Ebenen zu optimieren']\n",
      " > Processing time: 1.0737414360046387\n",
      " > Real-time factor: 0.20058965927801176\n",
      "✅ Resampled de_org.wav from 24000Hz to 44100Hz → Saved as de_org_44.wav\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_AUDIO = \"audio_dataset/wavs/segment_001.wav\"\n",
    "sentence = \"Manchmal ist es besser, nur die für den Sprecher relevanten Ebenen zu optimieren\"\n",
    "\n",
    "tts_2.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"de\", file_path=\"de_org.wav\")\n",
    "resample_wav(\"de_org.wav\", \"de_org_44.wav\", target_sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['Manchmal ist es besser, nur die für den Sprecher relevanten Ebenen zu optimieren']\n",
      " > Processing time: 0.9570026397705078\n",
      " > Real-time factor: 0.18946548813873454\n",
      "✅ Resampled de_FT.wav from 22050Hz to 44100Hz → Saved as de_FT_44.wav\n",
      "🎉 Fine-tuned voice generated! Check output.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "torch.cuda.empty_cache()  # ✅ Clears GPU memory cache\n",
    "# ✅ Define paths\n",
    "MODEL_PATH = \"fine_tuning_output/XTTS_v2_FT\"\n",
    "CONFIG_PATH = \"fine_tuning_output/XTTS_v2_FT/config.json\"\n",
    "REFERENCE_AUDIO= \"audio_dataset/wavs/segment_001.wav\"\n",
    "# ✅ Load fine-tuned XTTS\n",
    "tts = TTS(model_path=MODEL_PATH, config_path=CONFIG_PATH).to(\"cuda\")\n",
    "\n",
    "# ✅ Define test sentence\n",
    "sentence = \"Manchmal ist es besser, nur die für den Sprecher relevanten Ebenen zu optimieren\"\n",
    "\n",
    "# ✅ Generate speech\n",
    "tts.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"de\", file_path=\"de_FT.wav\")\n",
    "resample_wav(\"de_FT.wav\", \"de_FT_44.wav\", target_sr=44100)\n",
    "print(\"🎉 Fine-tuned voice generated! Check output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def get_wav_sample_rate(wav_path):\n",
    "    \"\"\"Returns the sample rate (Hz) of a given WAV file.\"\"\"\n",
    "    try:\n",
    "        sample_rate = librosa.get_samplerate(wav_path)\n",
    "        return sample_rate\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage (update with your actual file path)\n",
    "wav_file_path = \"output_44_2.wav\"  # Replace with your actual file\n",
    "sample_rate = get_wav_sample_rate(wav_file_path)\n",
    "sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasɪlsɪn \n"
     ]
    }
   ],
   "source": [
    "from phonemizer import phonemize\n",
    "\n",
    "word = \"nasilsin\"\n",
    "ipa = phonemize(word, language=\"tr\", backend=\"espeak\")\n",
    "print(ipa)  # Output: /teʃeˈkʰyɾ/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Turkish          IPA\n",
      "      seni       /seni/\n",
      "gördügümde /ɡøɾdyɡymde/\n",
      " gercekten /ɡeɾdʒekten/\n",
      "     mutlu      /mutlu/\n",
      " oluyorum.  /olujoɾum./\n",
      "  inmazsan   /inmazsan/\n",
      "       sor        /soɾ/\n"
     ]
    }
   ],
   "source": [
    "# ✅ Define Turkish → IPA Mapping\n",
    "IPA_DICT = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\", \"e\": \"e\", \"f\": \"f\",\n",
    "    \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\", \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\",\n",
    "    \"l\": \"l\", \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\", \"r\": \"ɾ\",\n",
    "    \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\", \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\",\n",
    "    # Special cases for better pronunciation\n",
    "    \"ev\": \"ev\", \"an\": \"ɑn\", \"en\": \"en\", \"at\": \"ɑt\", \"et\": \"et\", \n",
    "    \"il\": \"il\", \"ol\": \"ol\", \"ul\": \"ul\", \"el\": \"el\",\n",
    "    \"ay\": \"aj\", \"ey\": \"ej\", \"oy\": \"oj\", \"uy\": \"uj\"\n",
    "}\n",
    "\n",
    "# ✅ Function to Convert Turkish Text → IPA\n",
    "def turkish_to_ipa(word):\n",
    "    \"\"\"\n",
    "    Converts a Turkish word into IPA representation.\n",
    "    \"\"\"\n",
    "    word = word.lower()  # Ensure lowercase\n",
    "    ipa_word = \"\"\n",
    "    \n",
    "    for char in word:\n",
    "        ipa_word += IPA_DICT.get(char, char)  # Replace using dictionary\n",
    "    \n",
    "    return f\"/{ipa_word}/\"  # Return IPA transcription\n",
    "\n",
    "# ✅ Test the function\n",
    "words = [\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"Türkçe\", \"kedi\", \"şişe\", \"uçak\"]\n",
    "words = \"seni gördügümde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "ipa_transcriptions = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# ✅ Display results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ipa_transcriptions.items(), columns=[\"Turkish\", \"IPA\"])\n",
    "\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba → /mɛɾha.baˈ/\n",
      "nasılsın → /naˈ.sɯɫsɯn/\n",
      "teşekkür → /tɛʃɛkkyɾ/\n",
      "güzel → /ɡyzɛɫ/\n",
      "doktor → /do.ktoˈ.ɾ/\n",
      "telefon → /tɛɫɛfoˈ.n/\n",
      "kaplan → /ka.pɫaˈ.n/\n",
      "yapıyor → /ja.pɯjoˈ.ɾ/\n",
      "yapma → /ja.pmaˈ/\n",
      "evde → /ɛvdɛ/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ✅ Turkish letter → IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\",\n",
    "    \"e\": \"ɛ\", \"f\": \"f\", \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\",\n",
    "    \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\", \"l\": \"ɫ\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\",\n",
    "    \"r\": \"ɾ\", \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# ✅ Function to divide words into syllables\n",
    "def syllabify(word):\n",
    "    vowels = \"aıoueiöü\"\n",
    "    syllables = []\n",
    "    current_syllable = \"\"\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        current_syllable += char\n",
    "        if char in vowels:\n",
    "            if i < len(word) - 1 and word[i + 1] not in vowels:\n",
    "                syllables.append(current_syllable)\n",
    "                current_syllable = \"\"\n",
    "    if current_syllable:\n",
    "        syllables.append(current_syllable)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to place primary stress correctly\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words don’t need stress\n",
    "    \n",
    "    # Default: Stress the **last vowel-ending syllable**\n",
    "    for i in range(len(syllables) - 1, -1, -1):\n",
    "        if syllables[i][-1] in \"aıoueiöü\":\n",
    "            syllables[i] = syllables[i] + \"ˈ\"\n",
    "            break\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to convert Turkish text to IPA\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Convert to IPA\n",
    "    ipa_word = \"\".join([turkish_to_ipa_dict.get(char, char) for char in word])\n",
    "\n",
    "    # Step 2: Syllabify\n",
    "    syllables = syllabify(ipa_word)\n",
    "\n",
    "    # Step 3: Add correct stress placement\n",
    "    stressed_syllables = add_primary_stress(syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# ✅ Example words\n",
    "words = [\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"doktor\", \"telefon\", \"kaplan\", \"yapıyor\", \"yapma\", \"evde\"]\n",
    "\n",
    "# ✅ Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# ✅ Display Results\n",
    "for word, ipa in ipa_results.items():\n",
    "    print(f\"{word} → {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ra', 'ba']\n",
      "['bi', 'çi', 'mi', 'ne']\n",
      "['in', 'sa', 'nın']\n",
      "['ka', 'ra', 'ca']\n",
      "['al', 'dı']\n",
      "['bir', 'lik']\n",
      "['sev', 'mek']\n",
      "['alt', 'lık']\n",
      "['türk', 'çe']\n",
      "['kork', 'mak']\n",
      "['al', 'ti', 'ni', ' oy', 'mak']\n"
     ]
    }
   ],
   "source": [
    "def get_syllables(word):\n",
    "    syllables = []\n",
    "\n",
    "    \"\"\"\n",
    "    Aşağıdaki satır gelen kelimenin ünlü harfler 1, ünsüzler 0 olacak\n",
    "    şekilde desenini çıkarır.\n",
    "    Örneğin: arabacı -> 1010101, türkiye -> 010010\n",
    "    \"\"\"\n",
    "\n",
    "    bits = ''.join(['1' if l in 'aeıioöuü' else '0' for l in word])\n",
    "\n",
    "    \"\"\"\n",
    "    Aşağıdaki seperators listesi, yakalanacak desenleri ve desen yakalandığında\n",
    "    kelimenin hangi pozisyondan kesileceğini tanımlıyor.\n",
    "    Türkçede kelime içinde iki ünlü arasındaki ünsüz, kendinden sonraki\n",
    "    ünlüyle hece kurar., yani 101 desenini yakaladığımızda kelimeyi\n",
    "    bulunduğumuz yerden 1 ileri pozisyondan kesmeliyiz. ('101', 1)\n",
    "    Kelime içinde yan yana gelen iki ünsüzden ilki kendinden önceki ünlüyle,\n",
    "    ikincisi kendinden sonraki ünlüyle hece kurar. Bu da demek oluyor ki\n",
    "    1001 desenini yakaladığımızda kelimeyi bulunduğumuz noktadan 2 ileriden\n",
    "    kesmeliyiz. ('1001', 2),\n",
    "    Kelime içinde yan yana gelen üç ünsüz harften ilk ikisi kendinden önceki\n",
    "    ünlüyle, üçüncüsü kendinden sonraki ünlüyle hece kurar. Yani 10001 desenini\n",
    "    gördüğümüzde kelimeyi bulunduğumuz yerden 3 ileri pozisyondan kesmemiz\n",
    "    gerek. ('10001', 3)\n",
    "    \"\"\"\n",
    "\n",
    "    seperators = (\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    )\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "\n",
    "    # index değerini elimizdeki bitler üzerinde yürütmeye başlıyoruz.\n",
    "    while index < len(bits):\n",
    "\n",
    "        \"\"\"\n",
    "        Elimizdeki her ayırıcıyı (seperator), bits'in index'inci karakterinden\n",
    "        itibarent tek tek deneyerek yakalamaya çalışıyoruz.\n",
    "        \"\"\"\n",
    "\n",
    "        for seperator_pattern, seperator_cut_pos in seperators:\n",
    "            if bits[index:].startswith(seperator_pattern):\n",
    "\n",
    "                \"\"\"\n",
    "                Yakaladığımızda, en son cut_start posizyonundan, bulunduğumuz\n",
    "                pozisyonun serpator_cut_pos kadar ilerisine kadar bölümü alıp\n",
    "                syllables sepetine atıyoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                syllables.append(word[cut_start_pos:index + seperator_cut_pos])\n",
    "\n",
    "                \"\"\"\n",
    "                Index'imiz seperator_cut_pos kadar ilerliyor, ve\n",
    "                cut_start_pos'u index'le aynı yapıyoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                index += seperator_cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "\n",
    "        \"\"\"\n",
    "        Index ilerliyor, cut_start_pos'da değişiklik yok.\n",
    "        \"\"\"\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    # Son kalan heceyi elle sepete atıyoruz.\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "print(get_syllables(u'araba'))\n",
    "print(get_syllables(u'biçimine'))\n",
    "print(get_syllables(u'insanın'))\n",
    "print(get_syllables(u'karaca'))\n",
    "\n",
    "print(get_syllables(u'aldı'))\n",
    "print(get_syllables(u'birlik'))\n",
    "print(get_syllables(u'sevmek'))\n",
    "\n",
    "print(get_syllables(u'altlık'))\n",
    "print(get_syllables(u'türkçe'))\n",
    "print(get_syllables(u'korkmak'))\n",
    "print(get_syllables(u'altini oymak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seni → /sɛ.ˈni/\n",
      "gördügümde → /ɡøɾ.dy.ɡym.ˈdɛ/\n",
      "gercekten → /ɡɛɾ.dʒɛk.ˈtɛn/\n",
      "mutlu → /mut.ˈɫu/\n",
      "oluyorum. → /o.ɫu.jo.ˈɾum./\n",
      "inmazsan → /in.maz.ˈsan/\n",
      "sor → /soɾ/\n",
      "sɛ.ˈni ɡøɾ.dy.ɡym.ˈdɛ ɡɛɾ.dʒɛk.ˈtɛn mut.ˈɫu o.ɫu.jo.ˈɾum. in.maz.ˈsan soɾ \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ✅ Turkish letter → IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\",\n",
    "    \"e\": \"ɛ\", \"f\": \"f\", \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\",\n",
    "    \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\", \"l\": \"ɫ\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\",\n",
    "    \"r\": \"ɾ\", \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# ✅ Your syllabification function\n",
    "def get_syllables(word):\n",
    "    vowels = \"aeıioöuü\"\n",
    "    bits = ''.join(['1' if l in vowels else '0' for l in word])\n",
    "\n",
    "    seperators = [\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    ]\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "    syllables = []\n",
    "\n",
    "    while index < len(bits):\n",
    "        for pattern, cut_pos in seperators:\n",
    "            if bits[index:].startswith(pattern):\n",
    "                syllables.append(word[cut_start_pos:index + cut_pos])\n",
    "                index += cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "        index += 1\n",
    "\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to add primary stress\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words don’t need stress\n",
    "    syllables[-1] = \"ˈ\" + syllables[-1]  # Default: stress last syllable\n",
    "    return syllables\n",
    "\n",
    "# ✅ Convert Turkish → IPA with syllables\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Get syllables\n",
    "    syllables = get_syllables(word)\n",
    "\n",
    "    # Step 2: Convert each syllable to IPA\n",
    "    ipa_syllables = []\n",
    "    for syl in syllables:\n",
    "        ipa_syll = \"\".join([turkish_to_ipa_dict.get(char, char) for char in syl])\n",
    "        ipa_syllables.append(ipa_syll)\n",
    "\n",
    "    # Step 3: Add stress\n",
    "    stressed_syllables = add_primary_stress(ipa_syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# ✅ Example words\n",
    "words = [\"değil\",\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"doktor\", \"telefon\", \"kaplan\", \"yapıyor\", \"yapma\", \"evde\"]\n",
    "words = \"seni gördügümde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "\n",
    "# ✅ Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "loanwords = {\n",
    "    \"telefon\", \"televizyon\", \"müzik\", \"kültür\", \"problem\",\n",
    "    \"futbol\", \"doktor\", \"radyo\", \"otel\", \"tren\", \"klasik\", \"teknoloji\"\n",
    "}\n",
    "\n",
    "# ✅ Process IPA Results with Fixes\n",
    "corrected_ipa_results = {}\n",
    "for word, ipa in ipa_results.items():\n",
    "    if word in loanwords:\n",
    "        # Ensure first occurrence of 'ʏ' → 'y' and 'ɛ' → 'e' are replaced\n",
    "        if \"ʏ\" in ipa:\n",
    "            ipa = ipa.replace(\"ʏ\",\"y\")\n",
    "        if \"ɛ\" in ipa:\n",
    "            ipa = ipa.replace(\"ɛ\",\"e\")\n",
    "    corrected_ipa_results[word] = ipa\n",
    "\n",
    "# ✅ Display Results\n",
    "new_sen=\"\"\n",
    "for word, ipa in corrected_ipa_results.items():\n",
    "    new_sen += ipa + \" \"\n",
    "    \n",
    "    print(f\"{word} → {ipa}\")\n",
    "print(new_sen.replace(\"/\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
