{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
      "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ğŸ‘‰v4.50ğŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts_2 = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def resample_wav(input_wav, output_wav, target_sr=44100):\n",
    "    \"\"\"\n",
    "    Resample a WAV file to the target sample rate.\n",
    "\n",
    "    :param input_wav: Path to the input WAV file (e.g., \"output_22k.wav\")\n",
    "    :param output_wav: Path to the resampled output WAV file (e.g., \"output_44k.wav\")\n",
    "    :param target_sr: Target sample rate (default: 44100Hz)\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(input_wav, sr=None)  # Load with original SR\n",
    "\n",
    "    # Resample audio\n",
    "    resampled_audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Save the resampled audio\n",
    "    sf.write(output_wav, resampled_audio, target_sr)\n",
    "\n",
    "    print(f\"âœ… Resampled {input_wav} from {sr}Hz to {target_sr}Hz â†’ Saved as {output_wav}\")\n",
    "\n",
    "# Example usage after cloning:\n",
    "# Assuming cloned output is saved as \"output_22k.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Manchmal ist es besser, nur die fÃ¼r den Sprecher relevanten Ebenen zu optimieren']\n",
      " > Processing time: 1.0737414360046387\n",
      " > Real-time factor: 0.20058965927801176\n",
      "âœ… Resampled de_org.wav from 24000Hz to 44100Hz â†’ Saved as de_org_44.wav\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_AUDIO = \"audio_dataset/wavs/segment_001.wav\"\n",
    "sentence = \"Manchmal ist es besser, nur die fÃ¼r den Sprecher relevanten Ebenen zu optimieren\"\n",
    "\n",
    "tts_2.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"de\", file_path=\"de_org.wav\")\n",
    "resample_wav(\"de_org.wav\", \"de_org_44.wav\", target_sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['Manchmal ist es besser, nur die fÃ¼r den Sprecher relevanten Ebenen zu optimieren']\n",
      " > Processing time: 0.9570026397705078\n",
      " > Real-time factor: 0.18946548813873454\n",
      "âœ… Resampled de_FT.wav from 22050Hz to 44100Hz â†’ Saved as de_FT_44.wav\n",
      "ğŸ‰ Fine-tuned voice generated! Check output.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "torch.cuda.empty_cache()  # âœ… Clears GPU memory cache\n",
    "# âœ… Define paths\n",
    "MODEL_PATH = \"fine_tuning_output/XTTS_v2_FT\"\n",
    "CONFIG_PATH = \"fine_tuning_output/XTTS_v2_FT/config.json\"\n",
    "REFERENCE_AUDIO= \"audio_dataset/wavs/segment_001.wav\"\n",
    "# âœ… Load fine-tuned XTTS\n",
    "tts = TTS(model_path=MODEL_PATH, config_path=CONFIG_PATH).to(\"cuda\")\n",
    "\n",
    "# âœ… Define test sentence\n",
    "sentence = \"Manchmal ist es besser, nur die fÃ¼r den Sprecher relevanten Ebenen zu optimieren\"\n",
    "\n",
    "# âœ… Generate speech\n",
    "tts.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"de\", file_path=\"de_FT.wav\")\n",
    "resample_wav(\"de_FT.wav\", \"de_FT_44.wav\", target_sr=44100)\n",
    "print(\"ğŸ‰ Fine-tuned voice generated! Check output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def get_wav_sample_rate(wav_path):\n",
    "    \"\"\"Returns the sample rate (Hz) of a given WAV file.\"\"\"\n",
    "    try:\n",
    "        sample_rate = librosa.get_samplerate(wav_path)\n",
    "        return sample_rate\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage (update with your actual file path)\n",
    "wav_file_path = \"output_44_2.wav\"  # Replace with your actual file\n",
    "sample_rate = get_wav_sample_rate(wav_file_path)\n",
    "sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasÉªlsÉªn \n"
     ]
    }
   ],
   "source": [
    "from phonemizer import phonemize\n",
    "\n",
    "word = \"nasilsin\"\n",
    "ipa = phonemize(word, language=\"tr\", backend=\"espeak\")\n",
    "print(ipa)  # Output: /teÊƒeËˆkÊ°yÉ¾/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Turkish          IPA\n",
      "      seni       /seni/\n",
      "gÃ¶rdÃ¼gÃ¼mde /É¡Ã¸É¾dyÉ¡ymde/\n",
      " gercekten /É¡eÉ¾dÊ’ekten/\n",
      "     mutlu      /mutlu/\n",
      " oluyorum.  /olujoÉ¾um./\n",
      "  inmazsan   /inmazsan/\n",
      "       sor        /soÉ¾/\n"
     ]
    }
   ],
   "source": [
    "# âœ… Define Turkish â†’ IPA Mapping\n",
    "IPA_DICT = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dÊ’\", \"Ã§\": \"tÊƒ\", \"d\": \"d\", \"e\": \"e\", \"f\": \"f\",\n",
    "    \"g\": \"É¡\", \"ÄŸ\": \"É£\", \"h\": \"h\", \"Ä±\": \"É¯\", \"i\": \"i\", \"j\": \"Ê’\", \"k\": \"k\",\n",
    "    \"l\": \"l\", \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"Ã¶\": \"Ã¸\", \"p\": \"p\", \"r\": \"É¾\",\n",
    "    \"s\": \"s\", \"ÅŸ\": \"Êƒ\", \"t\": \"t\", \"u\": \"u\", \"Ã¼\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\",\n",
    "    # Special cases for better pronunciation\n",
    "    \"ev\": \"ev\", \"an\": \"É‘n\", \"en\": \"en\", \"at\": \"É‘t\", \"et\": \"et\", \n",
    "    \"il\": \"il\", \"ol\": \"ol\", \"ul\": \"ul\", \"el\": \"el\",\n",
    "    \"ay\": \"aj\", \"ey\": \"ej\", \"oy\": \"oj\", \"uy\": \"uj\"\n",
    "}\n",
    "\n",
    "# âœ… Function to Convert Turkish Text â†’ IPA\n",
    "def turkish_to_ipa(word):\n",
    "    \"\"\"\n",
    "    Converts a Turkish word into IPA representation.\n",
    "    \"\"\"\n",
    "    word = word.lower()  # Ensure lowercase\n",
    "    ipa_word = \"\"\n",
    "    \n",
    "    for char in word:\n",
    "        ipa_word += IPA_DICT.get(char, char)  # Replace using dictionary\n",
    "    \n",
    "    return f\"/{ipa_word}/\"  # Return IPA transcription\n",
    "\n",
    "# âœ… Test the function\n",
    "words = [\"merhaba\", \"nasÄ±lsÄ±n\", \"teÅŸekkÃ¼r\", \"gÃ¼zel\", \"TÃ¼rkÃ§e\", \"kedi\", \"ÅŸiÅŸe\", \"uÃ§ak\"]\n",
    "words = \"seni gÃ¶rdÃ¼gÃ¼mde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "ipa_transcriptions = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# âœ… Display results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ipa_transcriptions.items(), columns=[\"Turkish\", \"IPA\"])\n",
    "\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba â†’ /mÉ›É¾ha.baËˆ/\n",
      "nasÄ±lsÄ±n â†’ /naËˆ.sÉ¯É«sÉ¯n/\n",
      "teÅŸekkÃ¼r â†’ /tÉ›ÊƒÉ›kkyÉ¾/\n",
      "gÃ¼zel â†’ /É¡yzÉ›É«/\n",
      "doktor â†’ /do.ktoËˆ.É¾/\n",
      "telefon â†’ /tÉ›É«É›foËˆ.n/\n",
      "kaplan â†’ /ka.pÉ«aËˆ.n/\n",
      "yapÄ±yor â†’ /ja.pÉ¯joËˆ.É¾/\n",
      "yapma â†’ /ja.pmaËˆ/\n",
      "evde â†’ /É›vdÉ›/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# âœ… Turkish letter â†’ IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dÊ’\", \"Ã§\": \"tÊƒ\", \"d\": \"d\",\n",
    "    \"e\": \"É›\", \"f\": \"f\", \"g\": \"É¡\", \"ÄŸ\": \"É£\", \"h\": \"h\",\n",
    "    \"Ä±\": \"É¯\", \"i\": \"i\", \"j\": \"Ê’\", \"k\": \"k\", \"l\": \"É«\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"Ã¶\": \"Ã¸\", \"p\": \"p\",\n",
    "    \"r\": \"É¾\", \"s\": \"s\", \"ÅŸ\": \"Êƒ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"Ã¼\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# âœ… Function to divide words into syllables\n",
    "def syllabify(word):\n",
    "    vowels = \"aÄ±oueiÃ¶Ã¼\"\n",
    "    syllables = []\n",
    "    current_syllable = \"\"\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        current_syllable += char\n",
    "        if char in vowels:\n",
    "            if i < len(word) - 1 and word[i + 1] not in vowels:\n",
    "                syllables.append(current_syllable)\n",
    "                current_syllable = \"\"\n",
    "    if current_syllable:\n",
    "        syllables.append(current_syllable)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# âœ… Function to place primary stress correctly\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words donâ€™t need stress\n",
    "    \n",
    "    # Default: Stress the **last vowel-ending syllable**\n",
    "    for i in range(len(syllables) - 1, -1, -1):\n",
    "        if syllables[i][-1] in \"aÄ±oueiÃ¶Ã¼\":\n",
    "            syllables[i] = syllables[i] + \"Ëˆ\"\n",
    "            break\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# âœ… Function to convert Turkish text to IPA\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Convert to IPA\n",
    "    ipa_word = \"\".join([turkish_to_ipa_dict.get(char, char) for char in word])\n",
    "\n",
    "    # Step 2: Syllabify\n",
    "    syllables = syllabify(ipa_word)\n",
    "\n",
    "    # Step 3: Add correct stress placement\n",
    "    stressed_syllables = add_primary_stress(syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# âœ… Example words\n",
    "words = [\"merhaba\", \"nasÄ±lsÄ±n\", \"teÅŸekkÃ¼r\", \"gÃ¼zel\", \"doktor\", \"telefon\", \"kaplan\", \"yapÄ±yor\", \"yapma\", \"evde\"]\n",
    "\n",
    "# âœ… Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# âœ… Display Results\n",
    "for word, ipa in ipa_results.items():\n",
    "    print(f\"{word} â†’ {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ra', 'ba']\n",
      "['bi', 'Ã§i', 'mi', 'ne']\n",
      "['in', 'sa', 'nÄ±n']\n",
      "['ka', 'ra', 'ca']\n",
      "['al', 'dÄ±']\n",
      "['bir', 'lik']\n",
      "['sev', 'mek']\n",
      "['alt', 'lÄ±k']\n",
      "['tÃ¼rk', 'Ã§e']\n",
      "['kork', 'mak']\n",
      "['al', 'ti', 'ni', ' oy', 'mak']\n"
     ]
    }
   ],
   "source": [
    "def get_syllables(word):\n",
    "    syllables = []\n",
    "\n",
    "    \"\"\"\n",
    "    AÅŸaÄŸÄ±daki satÄ±r gelen kelimenin Ã¼nlÃ¼ harfler 1, Ã¼nsÃ¼zler 0 olacak\n",
    "    ÅŸekilde desenini Ã§Ä±karÄ±r.\n",
    "    Ã–rneÄŸin: arabacÄ± -> 1010101, tÃ¼rkiye -> 010010\n",
    "    \"\"\"\n",
    "\n",
    "    bits = ''.join(['1' if l in 'aeÄ±ioÃ¶uÃ¼' else '0' for l in word])\n",
    "\n",
    "    \"\"\"\n",
    "    AÅŸaÄŸÄ±daki seperators listesi, yakalanacak desenleri ve desen yakalandÄ±ÄŸÄ±nda\n",
    "    kelimenin hangi pozisyondan kesileceÄŸini tanÄ±mlÄ±yor.\n",
    "    TÃ¼rkÃ§ede kelime iÃ§inde iki Ã¼nlÃ¼ arasÄ±ndaki Ã¼nsÃ¼z, kendinden sonraki\n",
    "    Ã¼nlÃ¼yle hece kurar., yani 101 desenini yakaladÄ±ÄŸÄ±mÄ±zda kelimeyi\n",
    "    bulunduÄŸumuz yerden 1 ileri pozisyondan kesmeliyiz. ('101', 1)\n",
    "    Kelime iÃ§inde yan yana gelen iki Ã¼nsÃ¼zden ilki kendinden Ã¶nceki Ã¼nlÃ¼yle,\n",
    "    ikincisi kendinden sonraki Ã¼nlÃ¼yle hece kurar. Bu da demek oluyor ki\n",
    "    1001 desenini yakaladÄ±ÄŸÄ±mÄ±zda kelimeyi bulunduÄŸumuz noktadan 2 ileriden\n",
    "    kesmeliyiz. ('1001', 2),\n",
    "    Kelime iÃ§inde yan yana gelen Ã¼Ã§ Ã¼nsÃ¼z harften ilk ikisi kendinden Ã¶nceki\n",
    "    Ã¼nlÃ¼yle, Ã¼Ã§Ã¼ncÃ¼sÃ¼ kendinden sonraki Ã¼nlÃ¼yle hece kurar. Yani 10001 desenini\n",
    "    gÃ¶rdÃ¼ÄŸÃ¼mÃ¼zde kelimeyi bulunduÄŸumuz yerden 3 ileri pozisyondan kesmemiz\n",
    "    gerek. ('10001', 3)\n",
    "    \"\"\"\n",
    "\n",
    "    seperators = (\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    )\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "\n",
    "    # index deÄŸerini elimizdeki bitler Ã¼zerinde yÃ¼rÃ¼tmeye baÅŸlÄ±yoruz.\n",
    "    while index < len(bits):\n",
    "\n",
    "        \"\"\"\n",
    "        Elimizdeki her ayÄ±rÄ±cÄ±yÄ± (seperator), bits'in index'inci karakterinden\n",
    "        itibarent tek tek deneyerek yakalamaya Ã§alÄ±ÅŸÄ±yoruz.\n",
    "        \"\"\"\n",
    "\n",
    "        for seperator_pattern, seperator_cut_pos in seperators:\n",
    "            if bits[index:].startswith(seperator_pattern):\n",
    "\n",
    "                \"\"\"\n",
    "                YakaladÄ±ÄŸÄ±mÄ±zda, en son cut_start posizyonundan, bulunduÄŸumuz\n",
    "                pozisyonun serpator_cut_pos kadar ilerisine kadar bÃ¶lÃ¼mÃ¼ alÄ±p\n",
    "                syllables sepetine atÄ±yoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                syllables.append(word[cut_start_pos:index + seperator_cut_pos])\n",
    "\n",
    "                \"\"\"\n",
    "                Index'imiz seperator_cut_pos kadar ilerliyor, ve\n",
    "                cut_start_pos'u index'le aynÄ± yapÄ±yoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                index += seperator_cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "\n",
    "        \"\"\"\n",
    "        Index ilerliyor, cut_start_pos'da deÄŸiÅŸiklik yok.\n",
    "        \"\"\"\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    # Son kalan heceyi elle sepete atÄ±yoruz.\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "print(get_syllables(u'araba'))\n",
    "print(get_syllables(u'biÃ§imine'))\n",
    "print(get_syllables(u'insanÄ±n'))\n",
    "print(get_syllables(u'karaca'))\n",
    "\n",
    "print(get_syllables(u'aldÄ±'))\n",
    "print(get_syllables(u'birlik'))\n",
    "print(get_syllables(u'sevmek'))\n",
    "\n",
    "print(get_syllables(u'altlÄ±k'))\n",
    "print(get_syllables(u'tÃ¼rkÃ§e'))\n",
    "print(get_syllables(u'korkmak'))\n",
    "print(get_syllables(u'altini oymak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seni â†’ /sÉ›.Ëˆni/\n",
      "gÃ¶rdÃ¼gÃ¼mde â†’ /É¡Ã¸É¾.dy.É¡ym.ËˆdÉ›/\n",
      "gercekten â†’ /É¡É›É¾.dÊ’É›k.ËˆtÉ›n/\n",
      "mutlu â†’ /mut.ËˆÉ«u/\n",
      "oluyorum. â†’ /o.É«u.jo.ËˆÉ¾um./\n",
      "inmazsan â†’ /in.maz.Ëˆsan/\n",
      "sor â†’ /soÉ¾/\n",
      "sÉ›.Ëˆni É¡Ã¸É¾.dy.É¡ym.ËˆdÉ› É¡É›É¾.dÊ’É›k.ËˆtÉ›n mut.ËˆÉ«u o.É«u.jo.ËˆÉ¾um. in.maz.Ëˆsan soÉ¾ \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# âœ… Turkish letter â†’ IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dÊ’\", \"Ã§\": \"tÊƒ\", \"d\": \"d\",\n",
    "    \"e\": \"É›\", \"f\": \"f\", \"g\": \"É¡\", \"ÄŸ\": \"É£\", \"h\": \"h\",\n",
    "    \"Ä±\": \"É¯\", \"i\": \"i\", \"j\": \"Ê’\", \"k\": \"k\", \"l\": \"É«\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"Ã¶\": \"Ã¸\", \"p\": \"p\",\n",
    "    \"r\": \"É¾\", \"s\": \"s\", \"ÅŸ\": \"Êƒ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"Ã¼\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# âœ… Your syllabification function\n",
    "def get_syllables(word):\n",
    "    vowels = \"aeÄ±ioÃ¶uÃ¼\"\n",
    "    bits = ''.join(['1' if l in vowels else '0' for l in word])\n",
    "\n",
    "    seperators = [\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    ]\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "    syllables = []\n",
    "\n",
    "    while index < len(bits):\n",
    "        for pattern, cut_pos in seperators:\n",
    "            if bits[index:].startswith(pattern):\n",
    "                syllables.append(word[cut_start_pos:index + cut_pos])\n",
    "                index += cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "        index += 1\n",
    "\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "# âœ… Function to add primary stress\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words donâ€™t need stress\n",
    "    syllables[-1] = \"Ëˆ\" + syllables[-1]  # Default: stress last syllable\n",
    "    return syllables\n",
    "\n",
    "# âœ… Convert Turkish â†’ IPA with syllables\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Get syllables\n",
    "    syllables = get_syllables(word)\n",
    "\n",
    "    # Step 2: Convert each syllable to IPA\n",
    "    ipa_syllables = []\n",
    "    for syl in syllables:\n",
    "        ipa_syll = \"\".join([turkish_to_ipa_dict.get(char, char) for char in syl])\n",
    "        ipa_syllables.append(ipa_syll)\n",
    "\n",
    "    # Step 3: Add stress\n",
    "    stressed_syllables = add_primary_stress(ipa_syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# âœ… Example words\n",
    "words = [\"deÄŸil\",\"merhaba\", \"nasÄ±lsÄ±n\", \"teÅŸekkÃ¼r\", \"gÃ¼zel\", \"doktor\", \"telefon\", \"kaplan\", \"yapÄ±yor\", \"yapma\", \"evde\"]\n",
    "words = \"seni gÃ¶rdÃ¼gÃ¼mde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "\n",
    "# âœ… Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "loanwords = {\n",
    "    \"telefon\", \"televizyon\", \"mÃ¼zik\", \"kÃ¼ltÃ¼r\", \"problem\",\n",
    "    \"futbol\", \"doktor\", \"radyo\", \"otel\", \"tren\", \"klasik\", \"teknoloji\"\n",
    "}\n",
    "\n",
    "# âœ… Process IPA Results with Fixes\n",
    "corrected_ipa_results = {}\n",
    "for word, ipa in ipa_results.items():\n",
    "    if word in loanwords:\n",
    "        # Ensure first occurrence of 'Ê' â†’ 'y' and 'É›' â†’ 'e' are replaced\n",
    "        if \"Ê\" in ipa:\n",
    "            ipa = ipa.replace(\"Ê\",\"y\")\n",
    "        if \"É›\" in ipa:\n",
    "            ipa = ipa.replace(\"É›\",\"e\")\n",
    "    corrected_ipa_results[word] = ipa\n",
    "\n",
    "# âœ… Display Results\n",
    "new_sen=\"\"\n",
    "for word, ipa in corrected_ipa_results.items():\n",
    "    new_sen += ipa + \" \"\n",
    "    \n",
    "    print(f\"{word} â†’ {ipa}\")\n",
    "print(new_sen.replace(\"/\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
