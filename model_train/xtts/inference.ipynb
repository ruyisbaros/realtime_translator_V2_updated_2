{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source /workspace/tts_env/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/api.py:70: UserWarning: `gpu` will be deprecated. Please use `tts.to(device)` instead.\n",
      "  warnings.warn(\"`gpu` will be deprecated. Please use `tts.to(device)` instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > tts_models/multilingual/multi-dataset/xtts_v2 is already downloaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/layers/xtts/xtts_manager.py:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.speakers = torch.load(speaker_file_path)\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/utils/io.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=map_location, **kwargs)\n",
      "GPT2InferenceModel has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from TTS.api import TTS\n",
    "tts_2 = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def resample_wav(input_wav, output_wav, target_sr=44100):\n",
    "    \"\"\"\n",
    "    Resample a WAV file to the target sample rate.\n",
    "\n",
    "    :param input_wav: Path to the input WAV file (e.g., \"output_22k.wav\")\n",
    "    :param output_wav: Path to the resampled output WAV file (e.g., \"output_44k.wav\")\n",
    "    :param target_sr: Target sample rate (default: 44100Hz)\n",
    "    \"\"\"\n",
    "    # Load audio file\n",
    "    audio, sr = librosa.load(input_wav, sr=None)  # Load with original SR\n",
    "\n",
    "    # Resample audio\n",
    "    resampled_audio = librosa.resample(audio, orig_sr=sr, target_sr=target_sr)\n",
    "\n",
    "    # Save the resampled audio\n",
    "    \n",
    "    sf.write(output_wav, resampled_audio, target_sr)\n",
    "\n",
    "    print(f\"✅ Resampled {input_wav} from {sr}Hz to {target_sr}Hz → Saved as {output_wav}\")\n",
    "\n",
    "# Example usage after cloning:\n",
    "# Assuming cloned output is saved as \"output_22k.wav\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Text splitted to sentences.\n",
      "['Hello everyone.', 'Today I will teach you how to print from the computer.', 'Please turn on your computers']\n",
      " > Processing time: 1.5583691596984863\n",
      " > Real-time factor: 0.19447862883360287\n",
      "✅ Resampled org_en.wav from 24000Hz to 44100Hz → Saved as org_en_44_3.wav\n"
     ]
    }
   ],
   "source": [
    "REFERENCE_AUDIO= \"cropped_0_16.wav\"\n",
    "sentence = \"Hello everyone. Today I will teach you how to print from the computer. Please turn on your computers\"\n",
    "\n",
    "tts_2.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"en\", file_path=\"org_en.wav\")\n",
    "resample_wav(\"org_en.wav\", \"org_en_44_3.wav\", target_sr=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Using model: xtts\n",
      " > Text splitted to sentences.\n",
      "['Herkese merhaba bugün sizlere bilgisayardan nasıl dosya yazdırılacağını göstereceğim.', 'lütfen bilgisayarlarınızı açın.']\n",
      " > Processing time: 1.546891689300537\n",
      " > Real-time factor: 0.1903062050809947\n",
      "✅ Resampled FT_en_new.wav from 22050Hz to 44100Hz → Saved as FT_tr_ipa_44.wav\n",
      "🎉 Fine-tuned voice generated! Check output.wav\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from TTS.api import TTS\n",
    "import torch\n",
    "import os\n",
    "torch.cuda.empty_cache()  # ✅ Clears GPU memory cache\n",
    "\n",
    "# ✅ Define paths\n",
    "MODEL_PATH = os.path.join(os.getcwd(),\"fine_tuning_output_3/XTTS_v2_FT_ipa\")\n",
    "CONFIG_PATH = os.path.join(os.getcwd(),\"fine_tuning_output_3/XTTS_v2_FT_ipa/config.json\")\n",
    "REFERENCE_AUDIO= os.path.join(os.getcwd(),\"cropped_0_16.wav\")\n",
    "# ✅ Load fine-tuned XTTS\n",
    "tts = TTS(model_path=MODEL_PATH, config_path=CONFIG_PATH).to(\"cuda\")\n",
    "\n",
    "# ✅ Define test sentence\n",
    "sentence = \"Herkese merhaba bugün sizlere bilgisayardan nasıl dosya yazdırılacağını göstereceğim. lütfen bilgisayarlarınızı açın.\"\n",
    "\n",
    "# ✅ Generate speech\n",
    "tts.tts_to_file(text=sentence, speaker_wav=REFERENCE_AUDIO, language=\"tr\", file_path=\"FT_en_new.wav\")\n",
    "resample_wav(\"FT_en_new.wav\", \"FT_tr_ipa_44.wav\", target_sr=44100)\n",
    "print(\"🎉 Fine-tuned voice generated! Check output.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 Cropped 0-16s → cropped_0_16.wav\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "def crop_audio(input_path, output_path, start_sec=0, end_sec=16):\n",
    "    \"\"\"Crop audio between start and end seconds.\"\"\"\n",
    "    audio = AudioSegment.from_file(input_path)\n",
    "    cropped = audio[start_sec * 1000:end_sec * 1000]  # pydub works with milliseconds\n",
    "    cropped.export(output_path, format=\"wav\")\n",
    "    print(f\"🎯 Cropped {start_sec}-{end_sec}s → {output_path}\")\n",
    "\n",
    "# Usage\n",
    "crop_audio(\"org.mp4\", \"cropped_0_16.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔊 Stabilized: FT_en_new_44_stable.wav\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# Re-normalize output audio with stable amplitude\n",
    "def stabilize_volume(input_wav, output_wav):\n",
    "    y, sr = librosa.load(input_wav, sr=None)\n",
    "    y_normalized = librosa.util.normalize(y)  # Normalize amplitude\n",
    "    sf.write(output_wav, y_normalized, sr)\n",
    "    print(f\"🔊 Stabilized: {output_wav}\")\n",
    "\n",
    "stabilize_volume(\"FT_en_new_44.wav\", \"FT_en_new_44_stable.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ahmet/my_projects/realtime_translator_V2_updated/model_train'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r-- 1 ahmet ahmet 353K Feb 11 20:59 ./xtts_checkpoints/vocab.json\n"
     ]
    }
   ],
   "source": [
    "!ls -lh ./xtts_checkpoints/vocab.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44100"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "\n",
    "def get_wav_sample_rate(wav_path):\n",
    "    \"\"\"Returns the sample rate (Hz) of a given WAV file.\"\"\"\n",
    "    try:\n",
    "        sample_rate = librosa.get_samplerate(wav_path)\n",
    "        return sample_rate\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "# Example usage (update with your actual file path)\n",
    "wav_file_path = \"FT_en_new_44.wav\"  # Replace with your actual file\n",
    "sample_rate = get_wav_sample_rate(wav_file_path)\n",
    "sample_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nasɪlsɪn \n"
     ]
    }
   ],
   "source": [
    "from phonemizer import phonemize\n",
    "\n",
    "word = \"nasilsin\"\n",
    "ipa = phonemize(word, language=\"tr\", backend=\"espeak\")\n",
    "print(ipa)  # Output: /teʃeˈkʰyɾ/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Turkish          IPA\n",
      "      seni       /seni/\n",
      "gördügümde /ɡøɾdyɡymde/\n",
      " gercekten /ɡeɾdʒekten/\n",
      "     mutlu      /mutlu/\n",
      " oluyorum.  /olujoɾum./\n",
      "  inmazsan   /inmazsan/\n",
      "       sor        /soɾ/\n"
     ]
    }
   ],
   "source": [
    "# ✅ Define Turkish → IPA Mapping\n",
    "IPA_DICT = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\", \"e\": \"e\", \"f\": \"f\",\n",
    "    \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\", \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\",\n",
    "    \"l\": \"l\", \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\", \"r\": \"ɾ\",\n",
    "    \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\", \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\",\n",
    "    # Special cases for better pronunciation\n",
    "    \"ev\": \"ev\", \"an\": \"ɑn\", \"en\": \"en\", \"at\": \"ɑt\", \"et\": \"et\", \n",
    "    \"il\": \"il\", \"ol\": \"ol\", \"ul\": \"ul\", \"el\": \"el\",\n",
    "    \"ay\": \"aj\", \"ey\": \"ej\", \"oy\": \"oj\", \"uy\": \"uj\"\n",
    "}\n",
    "\n",
    "# ✅ Function to Convert Turkish Text → IPA\n",
    "def turkish_to_ipa(word):\n",
    "    \"\"\"\n",
    "    Converts a Turkish word into IPA representation.\n",
    "    \"\"\"\n",
    "    word = word.lower()  # Ensure lowercase\n",
    "    ipa_word = \"\"\n",
    "    \n",
    "    for char in word:\n",
    "        ipa_word += IPA_DICT.get(char, char)  # Replace using dictionary\n",
    "    \n",
    "    return f\"/{ipa_word}/\"  # Return IPA transcription\n",
    "\n",
    "# ✅ Test the function\n",
    "words = [\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"Türkçe\", \"kedi\", \"şişe\", \"uçak\"]\n",
    "words = \"seni gördügümde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "ipa_transcriptions = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# ✅ Display results\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(ipa_transcriptions.items(), columns=[\"Turkish\", \"IPA\"])\n",
    "\n",
    "print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "merhaba → /mɛɾha.baˈ/\n",
      "nasılsın → /naˈ.sɯɫsɯn/\n",
      "teşekkür → /tɛʃɛkkyɾ/\n",
      "güzel → /ɡyzɛɫ/\n",
      "doktor → /do.ktoˈ.ɾ/\n",
      "telefon → /tɛɫɛfoˈ.n/\n",
      "kaplan → /ka.pɫaˈ.n/\n",
      "yapıyor → /ja.pɯjoˈ.ɾ/\n",
      "yapma → /ja.pmaˈ/\n",
      "evde → /ɛvdɛ/\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ✅ Turkish letter → IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\",\n",
    "    \"e\": \"ɛ\", \"f\": \"f\", \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\",\n",
    "    \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\", \"l\": \"ɫ\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\",\n",
    "    \"r\": \"ɾ\", \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# ✅ Function to divide words into syllables\n",
    "def syllabify(word):\n",
    "    vowels = \"aıoueiöü\"\n",
    "    syllables = []\n",
    "    current_syllable = \"\"\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        current_syllable += char\n",
    "        if char in vowels:\n",
    "            if i < len(word) - 1 and word[i + 1] not in vowels:\n",
    "                syllables.append(current_syllable)\n",
    "                current_syllable = \"\"\n",
    "    if current_syllable:\n",
    "        syllables.append(current_syllable)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to place primary stress correctly\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words don’t need stress\n",
    "    \n",
    "    # Default: Stress the **last vowel-ending syllable**\n",
    "    for i in range(len(syllables) - 1, -1, -1):\n",
    "        if syllables[i][-1] in \"aıoueiöü\":\n",
    "            syllables[i] = syllables[i] + \"ˈ\"\n",
    "            break\n",
    "\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to convert Turkish text to IPA\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Convert to IPA\n",
    "    ipa_word = \"\".join([turkish_to_ipa_dict.get(char, char) for char in word])\n",
    "\n",
    "    # Step 2: Syllabify\n",
    "    syllables = syllabify(ipa_word)\n",
    "\n",
    "    # Step 3: Add correct stress placement\n",
    "    stressed_syllables = add_primary_stress(syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# ✅ Example words\n",
    "words = [\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"doktor\", \"telefon\", \"kaplan\", \"yapıyor\", \"yapma\", \"evde\"]\n",
    "\n",
    "# ✅ Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "\n",
    "# ✅ Display Results\n",
    "for word, ipa in ipa_results.items():\n",
    "    print(f\"{word} → {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'ra', 'ba']\n",
      "['bi', 'çi', 'mi', 'ne']\n",
      "['in', 'sa', 'nın']\n",
      "['ka', 'ra', 'ca']\n",
      "['al', 'dı']\n",
      "['bir', 'lik']\n",
      "['sev', 'mek']\n",
      "['alt', 'lık']\n",
      "['türk', 'çe']\n",
      "['kork', 'mak']\n",
      "['al', 'ti', 'ni', ' oy', 'mak']\n"
     ]
    }
   ],
   "source": [
    "def get_syllables(word):\n",
    "    syllables = []\n",
    "\n",
    "    \"\"\"\n",
    "    Aşağıdaki satır gelen kelimenin ünlü harfler 1, ünsüzler 0 olacak\n",
    "    şekilde desenini çıkarır.\n",
    "    Örneğin: arabacı -> 1010101, türkiye -> 010010\n",
    "    \"\"\"\n",
    "\n",
    "    bits = ''.join(['1' if l in 'aeıioöuü' else '0' for l in word])\n",
    "\n",
    "    \"\"\"\n",
    "    Aşağıdaki seperators listesi, yakalanacak desenleri ve desen yakalandığında\n",
    "    kelimenin hangi pozisyondan kesileceğini tanımlıyor.\n",
    "    Türkçede kelime içinde iki ünlü arasındaki ünsüz, kendinden sonraki\n",
    "    ünlüyle hece kurar., yani 101 desenini yakaladığımızda kelimeyi\n",
    "    bulunduğumuz yerden 1 ileri pozisyondan kesmeliyiz. ('101', 1)\n",
    "    Kelime içinde yan yana gelen iki ünsüzden ilki kendinden önceki ünlüyle,\n",
    "    ikincisi kendinden sonraki ünlüyle hece kurar. Bu da demek oluyor ki\n",
    "    1001 desenini yakaladığımızda kelimeyi bulunduğumuz noktadan 2 ileriden\n",
    "    kesmeliyiz. ('1001', 2),\n",
    "    Kelime içinde yan yana gelen üç ünsüz harften ilk ikisi kendinden önceki\n",
    "    ünlüyle, üçüncüsü kendinden sonraki ünlüyle hece kurar. Yani 10001 desenini\n",
    "    gördüğümüzde kelimeyi bulunduğumuz yerden 3 ileri pozisyondan kesmemiz\n",
    "    gerek. ('10001', 3)\n",
    "    \"\"\"\n",
    "\n",
    "    seperators = (\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    )\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "\n",
    "    # index değerini elimizdeki bitler üzerinde yürütmeye başlıyoruz.\n",
    "    while index < len(bits):\n",
    "\n",
    "        \"\"\"\n",
    "        Elimizdeki her ayırıcıyı (seperator), bits'in index'inci karakterinden\n",
    "        itibarent tek tek deneyerek yakalamaya çalışıyoruz.\n",
    "        \"\"\"\n",
    "\n",
    "        for seperator_pattern, seperator_cut_pos in seperators:\n",
    "            if bits[index:].startswith(seperator_pattern):\n",
    "\n",
    "                \"\"\"\n",
    "                Yakaladığımızda, en son cut_start posizyonundan, bulunduğumuz\n",
    "                pozisyonun serpator_cut_pos kadar ilerisine kadar bölümü alıp\n",
    "                syllables sepetine atıyoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                syllables.append(word[cut_start_pos:index + seperator_cut_pos])\n",
    "\n",
    "                \"\"\"\n",
    "                Index'imiz seperator_cut_pos kadar ilerliyor, ve\n",
    "                cut_start_pos'u index'le aynı yapıyoruz.\n",
    "                \"\"\"\n",
    "\n",
    "                index += seperator_cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "\n",
    "        \"\"\"\n",
    "        Index ilerliyor, cut_start_pos'da değişiklik yok.\n",
    "        \"\"\"\n",
    "\n",
    "        index += 1\n",
    "\n",
    "    # Son kalan heceyi elle sepete atıyoruz.\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "print(get_syllables(u'araba'))\n",
    "print(get_syllables(u'biçimine'))\n",
    "print(get_syllables(u'insanın'))\n",
    "print(get_syllables(u'karaca'))\n",
    "\n",
    "print(get_syllables(u'aldı'))\n",
    "print(get_syllables(u'birlik'))\n",
    "print(get_syllables(u'sevmek'))\n",
    "\n",
    "print(get_syllables(u'altlık'))\n",
    "print(get_syllables(u'türkçe'))\n",
    "print(get_syllables(u'korkmak'))\n",
    "print(get_syllables(u'altini oymak'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seni → /sɛ.ˈni/\n",
      "gördügümde → /ɡøɾ.dy.ɡym.ˈdɛ/\n",
      "gercekten → /ɡɛɾ.dʒɛk.ˈtɛn/\n",
      "mutlu → /mut.ˈɫu/\n",
      "oluyorum. → /o.ɫu.jo.ˈɾum./\n",
      "inmazsan → /in.maz.ˈsan/\n",
      "sor → /soɾ/\n",
      "sɛ.ˈni ɡøɾ.dy.ɡym.ˈdɛ ɡɛɾ.dʒɛk.ˈtɛn mut.ˈɫu o.ɫu.jo.ˈɾum. in.maz.ˈsan soɾ \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# ✅ Turkish letter → IPA phoneme mapping\n",
    "turkish_to_ipa_dict = {\n",
    "    \"a\": \"a\", \"b\": \"b\", \"c\": \"dʒ\", \"ç\": \"tʃ\", \"d\": \"d\",\n",
    "    \"e\": \"ɛ\", \"f\": \"f\", \"g\": \"ɡ\", \"ğ\": \"ɣ\", \"h\": \"h\",\n",
    "    \"ı\": \"ɯ\", \"i\": \"i\", \"j\": \"ʒ\", \"k\": \"k\", \"l\": \"ɫ\",\n",
    "    \"m\": \"m\", \"n\": \"n\", \"o\": \"o\", \"ö\": \"ø\", \"p\": \"p\",\n",
    "    \"r\": \"ɾ\", \"s\": \"s\", \"ş\": \"ʃ\", \"t\": \"t\", \"u\": \"u\",\n",
    "    \"ü\": \"y\", \"v\": \"v\", \"y\": \"j\", \"z\": \"z\"\n",
    "}\n",
    "\n",
    "# ✅ Your syllabification function\n",
    "def get_syllables(word):\n",
    "    vowels = \"aeıioöuü\"\n",
    "    bits = ''.join(['1' if l in vowels else '0' for l in word])\n",
    "\n",
    "    seperators = [\n",
    "        ('101', 1),\n",
    "        ('1001', 2),\n",
    "        ('10001', 3)\n",
    "    ]\n",
    "\n",
    "    index, cut_start_pos = 0, 0\n",
    "    syllables = []\n",
    "\n",
    "    while index < len(bits):\n",
    "        for pattern, cut_pos in seperators:\n",
    "            if bits[index:].startswith(pattern):\n",
    "                syllables.append(word[cut_start_pos:index + cut_pos])\n",
    "                index += cut_pos\n",
    "                cut_start_pos = index\n",
    "                break\n",
    "        index += 1\n",
    "\n",
    "    syllables.append(word[cut_start_pos:])\n",
    "    return syllables\n",
    "\n",
    "# ✅ Function to add primary stress\n",
    "def add_primary_stress(syllables):\n",
    "    if len(syllables) == 1:\n",
    "        return syllables  # One-syllable words don’t need stress\n",
    "    syllables[-1] = \"ˈ\" + syllables[-1]  # Default: stress last syllable\n",
    "    return syllables\n",
    "\n",
    "# ✅ Convert Turkish → IPA with syllables\n",
    "def turkish_to_ipa(word):\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Step 1: Get syllables\n",
    "    syllables = get_syllables(word)\n",
    "\n",
    "    # Step 2: Convert each syllable to IPA\n",
    "    ipa_syllables = []\n",
    "    for syl in syllables:\n",
    "        ipa_syll = \"\".join([turkish_to_ipa_dict.get(char, char) for char in syl])\n",
    "        ipa_syllables.append(ipa_syll)\n",
    "\n",
    "    # Step 3: Add stress\n",
    "    stressed_syllables = add_primary_stress(ipa_syllables)\n",
    "\n",
    "    return \"/{}/\".format(\".\".join(stressed_syllables))\n",
    "\n",
    "# ✅ Example words\n",
    "words = [\"değil\",\"merhaba\", \"nasılsın\", \"teşekkür\", \"güzel\", \"doktor\", \"telefon\", \"kaplan\", \"yapıyor\", \"yapma\", \"evde\"]\n",
    "words = \"seni gördügümde gercekten mutlu oluyorum. inmazsan sor\".split(\" \")\n",
    "\n",
    "# ✅ Convert words to IPA\n",
    "ipa_results = {word: turkish_to_ipa(word) for word in words}\n",
    "loanwords = {\n",
    "    \"telefon\", \"televizyon\", \"müzik\", \"kültür\", \"problem\",\n",
    "    \"futbol\", \"doktor\", \"radyo\", \"otel\", \"tren\", \"klasik\", \"teknoloji\"\n",
    "}\n",
    "\n",
    "# ✅ Process IPA Results with Fixes\n",
    "corrected_ipa_results = {}\n",
    "for word, ipa in ipa_results.items():\n",
    "    if word in loanwords:\n",
    "        # Ensure first occurrence of 'ʏ' → 'y' and 'ɛ' → 'e' are replaced\n",
    "        if \"ʏ\" in ipa:\n",
    "            ipa = ipa.replace(\"ʏ\",\"y\")\n",
    "        if \"ɛ\" in ipa:\n",
    "            ipa = ipa.replace(\"ɛ\",\"e\")\n",
    "    corrected_ipa_results[word] = ipa\n",
    "\n",
    "# ✅ Display Results\n",
    "new_sen=\"\"\n",
    "for word, ipa in corrected_ipa_results.items():\n",
    "    new_sen += ipa + \" \"\n",
    "    \n",
    "    print(f\"{word} → {ipa}\")\n",
    "print(new_sen.replace(\"/\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wollen wir das Ganze einfach mal ausführen → fɔ.llɛ.n fɪ.r da.s ga.nt͡sɛ. ɛɪ.nfa.x ma.l a.ʊsfʏ.hrɛ.n\n",
      "Das ist ein Test → da.s ɪ.ʃt ɛɪ.n tɛ.ʃt\n",
      "Können wir den Text ins IPA umwandeln → kœ.nnɛ.n fɪ.r dɛ.n tɛ.xt ɪ.ns ɪ.pa. ʊmfa.ndɛ.ln\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def text_to_german_ipa(text):\n",
    "    # German vowel and consonant mappings\n",
    "    vowel_map = {\n",
    "        'a': 'a', 'e': 'ɛ', 'i': 'ɪ', 'o': 'ɔ', 'u': 'ʊ',\n",
    "        'ä': 'ɛ', 'ö': 'œ', 'ü': 'ʏ', 'ei': 'aɪ̯', 'ie': 'iː',\n",
    "        'au': 'aʊ̯', 'eu': 'ɔɪ̯', 'äu': 'ɔɪ̯'\n",
    "    }\n",
    "\n",
    "    consonant_map = {\n",
    "        'w': 'v', 'j': 'j', 'v': 'f', 'z': 't͡s',\n",
    "        'sch': 'ʃ', 'ch': 'x', 'tsch': 't͡ʃ', 'pf': 'pf',\n",
    "        's$': 's', '^s': 'z', 'sp': 'ʃp', 'st': 'ʃt'\n",
    "    }\n",
    "\n",
    "    # Apply vowel transformations\n",
    "    for seq, ipa in vowel_map.items():\n",
    "        text = re.sub(seq, ipa, text)\n",
    "\n",
    "    # Apply consonant transformations\n",
    "    for seq, ipa in consonant_map.items():\n",
    "        text = re.sub(seq, ipa, text)\n",
    "\n",
    "    # Handle final consonants with devoicing\n",
    "    text = re.sub(r'b$', 'p', text)\n",
    "    text = re.sub(r'd$', 't', text)\n",
    "    text = re.sub(r'g$', 'k', text)\n",
    "\n",
    "    # Insert syllable boundaries (simple heuristic)\n",
    "    text = re.sub(r'([aeiouäöüɪɛʏœɔɐ])([^aeiouäöüɪɛʏœɔɐ])', r'\\1.\\2', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Test the function\n",
    "sentences = [\n",
    "    \"Wollen wir das Ganze einfach mal ausführen\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Können wir den Text ins IPA umwandeln\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    ipa = text_to_german_ipa(sentence.lower())\n",
    "    print(f\"{sentence} → {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wollen → ˈWɔ.llɛ.n\n",
      "wir → ˈvɪ.ʁ\n",
      "das → ˈda.z\n",
      "Ganze → ˈGa.nt͡sɛ\n",
      "einfach → ˈɛɪ.nfa.kh\n",
      "mal → ˈma.l\n",
      "ausführen → ˈaʊ.zfʏ.hʁɛ.n\n",
      "Das → ˈDa.z\n",
      "ist → ˈɪ.zt\n",
      "ein → ˈɛɪ.n\n",
      "Test → ˈTɛ.zt\n",
      "Können → ˈKœ.nnɛ.n\n",
      "wir → ˈvɪ.ʁ\n",
      "den → ˈdɛ.n\n",
      "Text → ˈTɛ.kst\n",
      "ins → ˈɪ.nz\n",
      "IPA → ˈI.PA\n",
      "umwandeln → ˈʊ.mva.ndɛ.ln\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_german_syllables(word):\n",
    "    vowels = \"aeiouäöüyAEIOUÄÖÜ\"\n",
    "    syllables = []\n",
    "    current_syllable = \"\"\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        current_syllable += char\n",
    "        if char in vowels:\n",
    "            if i < len(word) - 1 and word[i+1] not in vowels:\n",
    "                syllables.append(current_syllable)\n",
    "                current_syllable = \"\"\n",
    "    if current_syllable:\n",
    "        syllables.append(current_syllable)\n",
    "\n",
    "    return syllables\n",
    "\n",
    "def german_to_ipa(word):\n",
    "    ipa_mapping = {\n",
    "        \"a\": \"a\", \"b\": \"b\", \"c\": \"k\", \"d\": \"d\", \"e\": \"ɛ\", \"f\": \"f\", \"g\": \"g\", \"h\": \"h\", \"i\": \"ɪ\", \"j\": \"j\", \"k\": \"k\", \"l\": \"l\", \"m\": \"m\", \"n\": \"n\", \"o\": \"ɔ\", \"p\": \"p\", \"q\": \"k\", \"r\": \"ʁ\", \"s\": \"z\", \"ß\": \"s\", \"t\": \"t\", \"u\": \"ʊ\", \"v\": \"f\", \"w\": \"v\", \"x\": \"ks\", \"y\": \"ʏ\", \"z\": \"t͡s\",\n",
    "        \"ä\": \"ɛ\", \"ö\": \"œ\", \"ü\": \"ʏ\"\n",
    "    }\n",
    "    syllables = get_german_syllables(word)\n",
    "    ipa_syllables = []\n",
    "\n",
    "    for syllable in syllables:\n",
    "        ipa_syllable = \"\"\n",
    "        for char in syllable:\n",
    "            ipa_syllable += ipa_mapping.get(char, char)\n",
    "        ipa_syllables.append(ipa_syllable)\n",
    "\n",
    "    # Add primary stress to the first syllable for simplicity\n",
    "    if ipa_syllables:\n",
    "        ipa_syllables[0] = \"ˈ\" + ipa_syllables[0]\n",
    "\n",
    "    return \".\".join(ipa_syllables)\n",
    "\n",
    "# Test the function\n",
    "german_sentences = [\n",
    "    \"Wollen\", \"wir\", \"das\", \"Ganze\", \"einfach\", \"mal\", \"ausführen\",\n",
    "    \"Das\", \"ist\", \"ein\", \"Test\",\n",
    "    \"Können\", \"wir\", \"den\", \"Text\", \"ins\", \"IPA\", \"umwandeln\"\n",
    "]\n",
    "\n",
    "for word in german_sentences:\n",
    "    ipa = german_to_ipa(word)\n",
    "    print(f\"{word} → {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "German: Wollen wir das Ganze einfach mal ausführen\n",
      "IPA: 'v ɔ l ə nv iː' ɾd a.'sɡ a.'n ts ə aɪ.'n f a.'xm ɑː l aʊ.'s f yː r ə n\n"
     ]
    }
   ],
   "source": [
    "from phonemizer import phonemize\n",
    "from phonemizer.separator import Separator\n",
    "import re\n",
    "\n",
    "\n",
    "def german_to_ipa_with_syllables(text):\n",
    "    \"\"\"\n",
    "    Converts a German sentence to IPA with syllable boundaries and stress markers.\n",
    "\n",
    "    Args:\n",
    "        text (str): The German sentence to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: The IPA transcription with syllable boundaries and stress markers.\n",
    "    \"\"\"\n",
    "    # Step 1: Get initial IPA transcription\n",
    "    separator = Separator(phone=' ', word=None)\n",
    "    ipa_text = phonemize(\n",
    "        text,\n",
    "        language='de',\n",
    "        backend='espeak',\n",
    "        separator=separator,\n",
    "        strip=True,\n",
    "        njobs=1\n",
    "    )\n",
    "\n",
    "    # Step 2: Add syllable boundaries (simple heuristic)\n",
    "    ipa_text = re.sub(r'([aeiouyäöüɪɛæʊɐ]) ', r'\\1.', ipa_text)  # Add '.' after vowels\n",
    "\n",
    "    # Step 3: Add primary stress to first vowel in each word (simplistic rule)\n",
    "    ipa_text = re.sub(r'\\b([^.]*?[aeiouyäöüɪɛæʊɐ])', r\"'\\1\", ipa_text)\n",
    "\n",
    "    # Step 4: Handle common diphthongs\n",
    "    ipa_text = ipa_text.replace('ai', 'aɪ̯').replace('ei', 'aɪ̯').replace('au', 'aʊ̯').replace('eu', 'ɔʏ̯').replace('äu', 'ɔʏ̯')\n",
    "\n",
    "    return ipa_text\n",
    "\n",
    "\n",
    "# Example Usage\n",
    "german_sentence = \"Wollen wir das Ganze einfach mal ausführen\"\n",
    "ipa_transcription = german_to_ipa_with_syllables(german_sentence)\n",
    "print(f\"German: {german_sentence}\")\n",
    "print(f\"IPA: {ipa_transcription}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wollen wir das Ganze einfach mal ausführen → ˈwo.lle.n ˈwi.ʁ ˈda.s ˈga.ntse ˈa.ɪ.̯nfa.x ˈma.l ˈa.ʊ̯sfʏ.hʁe.n\n",
      "Das ist ein Test → ˈda.s ˈi.ʃt ˈa.ɪ.̯n ˈte.ʃt\n",
      "Können wir den Text ins IPA umwandeln → ˈkœ.nne.n ˈwi.ʁ ˈde.n ˈte.xt ˈi.ns ˈi.pa ˈu.mwa.nde.ln\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def german_to_ipa(text):\n",
    "    ipa_mapping = {\n",
    "        \"sch\": \"ʃ\", \"ch\": \"x\", \"z\": \"ts\", \"j\": \"j\", \"r\": \"ʁ\", \"ng\": \"ŋ\",\n",
    "        \"au\": \"aʊ̯\", \"ei\": \"aɪ̯\", \"eu\": \"ɔʏ̯\", \"äu\": \"ɔʏ̯\", \"sp\": \"ʃp\", \"st\": \"ʃt\",\n",
    "        \"ä\": \"ɛ\", \"ö\": \"œ\", \"ü\": \"ʏ\", \"ß\": \"s\", \"ph\": \"f\", \"qu\": \"kv\"\n",
    "    }\n",
    "\n",
    "    words = text.lower().split()\n",
    "    ipa_words = []\n",
    "\n",
    "    for word in words:\n",
    "        ipa_word = word\n",
    "        for key, val in ipa_mapping.items():\n",
    "            ipa_word = ipa_word.replace(key, val)\n",
    "\n",
    "        # Insert syllable boundaries and primary stress for the first syllable\n",
    "        ipa_word = re.sub(r\"([aeiouäöüɪɛœʏ])\", r\"\\1.\", ipa_word)  # syllables after vowels\n",
    "        ipa_word = re.sub(r\"\\.$\", \"\", ipa_word)  # remove trailing syllable\n",
    "        ipa_word = \"ˈ\" + ipa_word  # add primary stress\n",
    "\n",
    "        ipa_words.append(ipa_word)\n",
    "\n",
    "    return \" \".join(ipa_words)\n",
    "\n",
    "# Test the function\n",
    "sentences = [\n",
    "    \"Wollen wir das Ganze einfach mal ausführen\",\n",
    "    \"Das ist ein Test\",\n",
    "    \"Können wir den Text ins IPA umwandeln\"\n",
    "]\n",
    "\n",
    "for sentence in sentences:\n",
    "    ipa = german_to_ipa(sentence.lower())\n",
    "    print(f\"{sentence} → {ipa}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ IPA conversion completed. Saved to audio_dataset/metadata_ipa.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def convert_metadata_to_ipa(input_file='audio_dataset/metadata.csv', output_file='audio_dataset/metadata_ipa.csv'):\n",
    "    \"\"\"Converts the second column of metadata.csv to IPA and saves it\"\"\"\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8', newline='') as outfile:\n",
    "        reader = csv.reader(infile, delimiter='|')\n",
    "        writer = csv.writer(outfile, delimiter='|')\n",
    "\n",
    "        for row in reader:\n",
    "            if len(row) < 3:\n",
    "                continue\n",
    "\n",
    "            original_text = row[1]\n",
    "            ipa_text = german_to_ipa(original_text)\n",
    "\n",
    "            new_row = [row[0], f\"/{ipa_text}/\", row[2]]\n",
    "            writer.writerow(new_row)\n",
    "\n",
    "    print(f\"✅ IPA conversion completed. Saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Run the conversion\n",
    "convert_metadata_to_ipa()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tts_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
