{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Hardcoded paths (update these based on actual locations)\n",
    "wav_file_path = os.path.join(os.getcwd(), \"temp_video/cf5f9419ae994200bf4af7472bc51abd.wav\")\n",
    "json_transcript_path = os.path.join(os.getcwd(), \"temp_video/subtitles/subtitles-original.json\")\n",
    "output_folder = os.path.join(os.getcwd(), \"segmented_wavs\")  # Make sure it exists\n",
    "dataset = os.path.join(os.getcwd(), \"dataset\")  # Make sure it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# ✅ Paths\n",
    "wav_file_path = os.path.join(os.getcwd(), \"temp_video/cf5f9419ae994200bf4af7472bc51abd.wav\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ✅ Load full audio\n",
    "audio, sr = librosa.load(wav_file_path, sr=22050)  # 🔥 Ensure XTTS-compatible sample rate\n",
    "\n",
    "# ✅ Split parameters\n",
    "chunk_duration = 9.6  # Split length in seconds\n",
    "samples_per_chunk = int(chunk_duration * sr)  # Convert seconds to samples\n",
    "\n",
    "# ✅ Process and Save Chunks\n",
    "num_chunks = len(audio) // samples_per_chunk\n",
    "for i in range(num_chunks + 1):  # +1 to ensure we get the last segment if any\n",
    "    start = i * samples_per_chunk\n",
    "    end = min((i + 1) * samples_per_chunk, len(audio))\n",
    "    chunk_audio = audio[start:end]\n",
    "\n",
    "    if len(chunk_audio) > 1000:  # 🔥 Skip empty or tiny segments\n",
    "        chunk_filename = f\"segment_{i:03d}.wav\"\n",
    "        chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "        sf.write(chunk_path, chunk_audio, sr)\n",
    "        print(f\"✅ Saved {chunk_path}\")\n",
    "\n",
    "print(f\"🎉 Splitting complete! Segments saved in: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Transcribed: segment_000.wav → Hallo und herzlich willkommen zur zweiten Folge von Einführung in React mit dem Thema React Setup. Noch einmal kurz zu mir, mein Name ist David Losert.\n",
      "✅ Transcribed: segment_001.wav → Ich bin Software Engineer und seit über zehn Jahren im Web unterwegs und arbeite nun auch bereits seit vier Jahren mit React. Neben React mag ich die Arbeit mit Chubbys.\n",
      "✅ Transcribed: segment_002.wav → TypeScript, Node.js, Linux-Servern, Docker und AWS. Die heutige Folge dreht sich also nun komplett darum, eine\n",
      "✅ Transcribed: segment_003.wav → Entwicklungsumgebung aufzusetzen und dort eine erste React Hello World Applikation zu implementieren. Wenn wir uns kurz erinnern, in der letzten Folge habe ich die Geschichte\n",
      "✅ Transcribed: segment_004.wav → und Prinzipien von React kurz vorgestellt und einen ersten theoretischen Einblick in den Virtual Dom und in JSX gegeben. Das habe ich an dieser Stelle auch einbezogen.\n",
      "✅ Transcribed: segment_005.wav → einmal kurz visualisiert. Wir erinnern uns, der Virtual DOM ist eine Abstraktion, die React verwendet, um den DOM zu synchronisieren.\n",
      "✅ Transcribed: segment_006.wav → Der Virtual Dong erlaubt uns zum einen das deklarative Programmieren und zum anderen gibt es uns einige Performance Vorteile.\n",
      "✅ Transcribed: segment_007.wav → In dieser Folge wollen wir nun eben also eine Entwicklungsumgebung aufsetzen. Ich nutze dazu Visual Studio Code. Wir werden uns ein erstes Video zeigen.\n",
      "✅ Transcribed: segment_008.wav → Toolset anschauen mit npm, nbx und Babel, was uns bei der Entwicklung von React-Applikationen hilft. Und wir werden natürlich eine erste React-Applikation implementieren und nutzen\n",
      "✅ Transcribed: segment_009.wav → dazu das React Element, ein atomarer Bildung Block von React und JSX. In dieser Stelle werdet ihr vielleicht kurz aufmerksam machen.\n",
      "✅ Transcribed: segment_010.wav → Ich habe das letzte Mal viel von React Components gesprochen. React Components sind nicht zu verwechseln mit React Element. Ich stelle nun aber in dieser Folge zu...\n",
      "✅ Transcribed: segment_011.wav → erst React Element vor, weil es sozusagen die Grundlage ist, oder der atomare Baustein, der tatsächlich atomare Baustein von React.\n",
      "✅ Transcribed: segment_012.wav → nimmt uns aus GSX auch so ein wenig die Magie. Denn wenn man GSX das erste Ball sieht, kann man sich schnell fragen, wie funktioniert das eigentlich hinter den Kulissen?\n",
      "✅ Transcribed: segment_013.wav → Und Viect Element ist letztendlich das, was hinter den Kulissen steckt. Das werden wir am Ende der Folge dann auch einfach sehen. Bevor wir loslegen...\n",
      "✅ Transcribed: segment_014.wav → möchte ich euch ermutigen, alle Code-Beispiele und praktischen Hands-on-Teile, die wir in dieser Folge machen, nachzuprogrammieren. Der praktische Einsatz ist\n",
      "✅ Transcribed: segment_015.wav → einfach der beste, um eine neue Technologie zu lernen. Das könnt ihr entweder machen, indem ihr nebenher programmiert und die Folge immer wieder pausiert oder aber\n",
      "✅ Transcribed: segment_016.wav → Ihr schaut euch die Folge einmal komplett an und programmiert das Beispiel im Nachhinein alleine. Wir werden den gesamten Code auch auf GitHub verwendet.\n",
      "✅ Transcribed: segment_017.wav → zur Verfügung stellen. Das kann dann ein wenig als Orientierung dienen. Dabei geht einfach auf GitHub und sucht dort nach tech-lounge-sylject und ihr solltet das entsprechende Repost machen.\n",
      "✅ Transcribed: segment_018.wav → Tutorial finden. Das ist aktuell hier noch leer, weil ich den Code natürlich erst nach dieser Folge hochladen werde.\n",
      "✅ Transcribed: segment_019.wav → Und damit würde ich sagen, legen wir auch einfach schon mal los. Um unsere Umgebung vorzubereiten, müssen ein paar Schritte unternommen werden.\n",
      "✅ Transcribed: segment_020.wav → mit React erstmal noch nichts zu tun. Zum einen müsst ihr euch Visual Studio Code installieren, oder müsst ihr nicht. Wenn ihr einen anderen Editor bevorzugt, ist das auch\n",
      "✅ Transcribed: segment_021.wav → vollkommen okay. Ich arbeite nur hier mit Visual Studio Code, weil ich diese Idee doch recht gerne habe. Sie bittet mir einige Unwahrheiten.\n",
      "✅ Transcribed: segment_022.wav → z.B. Codevervollständigung, was wir nachher auch sehen werden. Neben einer Entwicklungsumgebung Visual Studio Code brauchen wir Node.js und NPM.\n",
      "✅ Transcribed: segment_023.wav → Da erkläre ich auch gleich ein paar Worte dazu. Und wir müssen natürlich einen Projektordner anlegen und unsere Umgebung vorbereiten mit ein paar wenigen Commands in der Kommandozeile.\n",
      "✅ Transcribed: segment_024.wav → Ein paar Worte zu NPM, falls ihr das noch nicht gehört habt. NPM ist ein Paketmanager.\n",
      "✅ Transcribed: segment_025.wav → für JavaScript Tools, Bibliotheken und Frameworks und erlaubt uns über eine einfache Command Line Interface, das installieren und verwalten eben dieser Tools Bibliothek.\n",
      "✅ Transcribed: segment_026.wav → und Frameworks. Das ist ein einfacher Befehl wie npm install babel, den wir heute auch noch ausführen werden, der uns Abhängigkeiten macht.\n",
      "✅ Transcribed: segment_027.wav → unser Projekt installiert. Und bei npm dreht sich eigentlich alles um die Package.json. Package.json ist eine Datei, die wir in unserem Projektordner im Bootfolder meistens\n",
      "✅ Transcribed: segment_028.wav → erstellen werden und in dieser Package JSON List sind Abhängigkeiten beschrieben, Skripte und auch Projektmetadaten.\n",
      "✅ Transcribed: segment_029.wav → Tool, das npm mitbringt, ist npx oder np execute. Und das ermöglicht uns die Ausführung all dieser JavaScript Tools biblik.\n",
      "✅ Transcribed: segment_030.wav → und frameworks ohne eine Installation. Und das können wir zum Beispiel nutzen, um einen lokalen Websever zu starten, der unser Testprojekt ausliefert.\n",
      "✅ Transcribed: segment_031.wav → werden wir auch tun und wir werden dazu den Web-Server Servo verwenden. Das ist kein Schreibfehler. Dieses Paket heißt wirklich so.\n",
      "✅ Transcribed: segment_032.wav → startet mit diesem Befehl einen lokalen Webserver im aktuellen Verzeichnis. Wir haben hier noch das Argument \"-reload\".\n",
      "✅ Transcribed: segment_033.wav → Das erlaubt uns oder das erlaubt dem Server alle Dateien, die in unserem Projekt sich tummeln, zu beobachten und bei einer Änderung unseres\n",
      "✅ Transcribed: segment_034.wav → Browser automatisch neu zu laden. Das ist während der Entwicklung sehr bequem, weil es uns das Neuladen der Seite händisch erspart, indem wir entweder F5 drücken oder hier über den\n",
      "✅ Transcribed: segment_035.wav → Reload-Button die Seite neu laden. Wenn wir diesen Befehl per Default ausführen, liefert er eine Index-HTML, welche wir auch gleich erstellen werden.\n",
      "✅ Transcribed: segment_036.wav → im aktuellen Ordner unter der Adresse htdp localhost 8080 aus. Und nun um euch auch mal zu zeigen,\n",
      "✅ Transcribed: segment_037.wav → NPM Paket auf der Registry ausschaut. Hier unter npm.js.com habt ihr eine Suche, in der ihr alle Pakete, die es so gibt, suchen könnt.\n",
      "✅ Transcribed: segment_038.wav → Für jedes Paket gibt es dann auch eine Seite mit einer Beschreibung und Installationsanweisungen. Alles, was man zu diesem Paket wissen muss.\n",
      "✅ Transcribed: segment_039.wav → Wollen wir das Ganze einfach mal ausführen? Dazu gehen wir also in unserer Command Line.\n",
      "✅ Transcribed: segment_040.wav → und legen uns erstmal einen Ordner an. Den nenne ich hier einfach mal Einführung Reakt.\n",
      "✅ Transcribed: segment_041.wav → Jetzt sieht man, dass ich den zuvor schon angelegt habe, deswegen bringt er mir hier einen Error. Bei euch wird das dann funktionieren. Wir können einfach in diesem Ordner reinnamigieren und wir werden den Befehl ändern.\n",
      "✅ Transcribed: segment_042.wav → NPM Init ausführen. NPM Init erzeugt uns eben eine Package JSON, eine Initialen. Und das spart uns so ein wenig, das von Hand zu tun, indem es uns\n",
      "✅ Transcribed: segment_043.wav → über die Kommandozeile ein paar Fragen stellt. Als allererstes will es den Package-Namen von uns wissen, den es per Default aus dem aktuellen Ordner einfach herauszieht. Einfach umgelegt ist in diesem Fall ok.\n",
      "✅ Transcribed: segment_044.wav → Die Version ist für uns jetzt auch erstmal ok. Wir erinnern uns kurz an die letzte Folge. Semper, Symantec Version, kommt bei NPM.\n",
      "✅ Transcribed: segment_045.wav → stark zum Einsatz. Eine Description, da können wir uns einfach irgendeinen Freitext überlegen.\n",
      "✅ Transcribed: segment_046.wav → Das ist eine einfache Reaktion zum Beispiel. Der Entry Point werden wir nachher sehen. Das ist bei uns source.app.js. Test Command. Wir haben.\n",
      "✅ Transcribed: segment_047.wav → keine automatisierten Tests, deswegen lassen wir das leer. Wir haben auch noch kein Git Repository eingerichtet. Wir wollen auch erstmal keine Keywords vergeben. Den Author können wir uns selber reinschreiben.\n",
      "✅ Transcribed: segment_048.wav → Und eine Lizenz ist bei privaten Testprojekten auch eher nicht ganz so wichtig. Ich nehme hier in der Regel MIT. Könnt ihr aber im Prinzip nicht.\n",
      "✅ Transcribed: segment_049.wav → auch auf ISC lassen. NPM fragt uns das nochmal, ob alle unsere Eingaben korrekt waren und wird uns eben diese JSON-Datei als Package JSON im aktuellen Moment zeigen.\n",
      "✅ Transcribed: segment_050.wav → ein Zeichniss anlegen. In unserem Fall ist das jetzt okay. Deswegen, yes. Als nächstes öffnen wir nun diesen Ordner in unserer Entwicklungsumgebung.\n",
      "✅ Transcribed: segment_051.wav → In meinem Fall ist das eben Bisholz Studio Code. Das können wir ganz einfach machen, indem wir hier auf File Open gehen, zum entsprechenden Ordner navigieren und dann auf Öffnen.\n",
      "✅ Transcribed: segment_052.wav → drücken. Und jetzt sehen wir, dass uns eben eine Package JSON generiert wurde, in der all die Feller, die wir vorher verwendet haben, verwendet haben.\n",
      "✅ Transcribed: segment_053.wav → Frage beantwortet haben, entsprechend eingetragen sind. Als nächsten Schritt wollen wir uns nun also noch eine Index-HTML anlegen.\n",
      "✅ Transcribed: segment_054.wav → initiale HTML-Seite, die ausgeliefert werden soll. Das machen wir über new file index.html.\n",
      "✅ Transcribed: segment_055.wav → einfach eine standardmäßige HTML-Datei, angefangen mit dem DocType. Und hier haben wir jetzt schon ein tolles Feature von Visual Studio Code gesehen, die Autovervollständigung.\n",
      "✅ Transcribed: segment_056.wav → Ich werde das nochmal, indem ich also nur die ersten paar Zeichen des Codes eingebe, den ich hier erzeugen will, bietet mir Visual Studio Code schon eine\n",
      "✅ Transcribed: segment_057.wav → Vorauswahl an. Wenn ich diese bestätige, entweder über Enter oder indem ich draufklicke, füllt es mir das entsprechend aus.\n",
      "✅ Transcribed: segment_058.wav → HTML. Wir brauchen einen Het oder einen Het-Tag, in welchem wir dann einen Titel vergeben können. Das nennen wir mal Einführung in React.\n",
      "✅ Transcribed: segment_059.wav → Und wir brauchen einen Buddy. Und da wollen wir jetzt einfach, wie das so üblich ist, bei einem Code Beispiel mit HelloWorld anfangen.\n",
      "✅ Transcribed: segment_060.wav → Wenn ich nur noch Speichern drücke, nicht wundern, das ist auch ein Feature von Visual Studio Code, das ist mir automatisch mein Dokument nach gewissen\n",
      "✅ Transcribed: segment_061.wav → Kriterien formatiert. Dementsprechend kann es sein, dass hier manchmal ein Zeilenumbruch hinzugeführt wird. Das ist einfach sehr bequem, dass es immer alles einheitlich formatiert wird.\n",
      "✅ Transcribed: segment_062.wav → So, nun haben wir eine Index-HTML erstellt. Diese wollen wir jetzt natürlich noch über einen Webserver ausliefern, um unsere Entwicklungsumgebung zu vervollständigen.\n",
      "✅ Transcribed: segment_063.wav → Und wie vorher besprochen, nehmen wir dazu npx, geben ein npx, servo, minus minus reload, enter.\n",
      "✅ Transcribed: segment_064.wav → Das dauert kurz und gibt uns dann hier auch entsprechend zurück, dass nun unter adp localhost 8080 die aktuelle Web-Partei\n",
      "✅ Transcribed: segment_065.wav → Website oder der aktuelle Ordner ausgeliefert wird und standardmäßig eben diese Index HTML ausgeliefert wird. Und das sehen wir auch, indem wir zum Browser navigieren.\n",
      "✅ Transcribed: segment_066.wav → und entsprechend diese URL eingeben. Und siehe da, unsere Hello World Index HTML wird ausgeliefert. Eine kleine Unschönheit.\n",
      "✅ Transcribed: segment_067.wav → sehen wir hier noch Einführung in React. Das Ü hat er irgendwie noch nicht erkannt. Dazu müssen wir in dem HTML im Het-Teil noch\n",
      "✅ Transcribed: segment_068.wav → das Charset auf UTF-8 setzen. Geht auch ganz einfach in die Variante UTF-8\n",
      "✅ Transcribed: segment_069.wav → wir drücken speichern und wenn wir nun zurück auf die website navigieren sehen wir dass das Reload Flag von Server schon seinen Job getan hat.\n",
      "✅ Transcribed: segment_070.wav → Die Seite wurde automatisch neu geladen und das Ü wird hier nun korrekt dargestellt. Und damit haben wir im Wesentlichen schon eine laufende Entwicklung.\n",
      "✅ Transcribed: segment_071.wav → Umgebung, in der wir nun React entwickeln können. Dazu nun wieder ein bisschen Theorie. Es gibt mehrere Methoden.\n",
      "✅ Transcribed: segment_072.wav → wir React nun in unser Projekt installieren können oder wie wir auch React aufsetzen können. Die einfachste Methode sind Online Playgrounds.\n",
      "✅ Transcribed: segment_073.wav → im Prinzip Entwicklungsumgebung direkt im Browser. React bietet selber auf seiner Seite Dokumentationen, einen Online-Project-Content an, zum Beispiel CodePen.\n",
      "✅ Transcribed: segment_074.wav → Das können wir uns auch mal ganz kurz anschauen. Wenn wir hier auf Get Started drücken, Try React, sehen wir hier die Online-Breakdowns. Wenn wir da zum Beispiel auf CodePen navigieren,\n",
      "✅ Transcribed: segment_075.wav → werden wir gleich weitergeleitet und haben nun hier ebenfalls eine Hello World Applikation von React.\n",
      "✅ Transcribed: segment_076.wav → Die wir entsprechend auch bearbeiten können, geben wir hier noch ein U ein, über Command Enter wird die Seite nun aktualisiert.\n",
      "✅ Transcribed: segment_077.wav → wie gesagt super, um React auszuprobieren, um vielleicht auch mal, wenn irgendwas bei euch lokal nicht funktioniert, nachzustellen. Aber wenn wir langfristig\n",
      "✅ Transcribed: segment_078.wav → Projekte entwickeln, wollen wir natürlich irgendwie den Code, den wir produzieren, auch richtig abspeichern und das ist in diesen Online-Playgrounds eher schwierig möglich.\n",
      "✅ Transcribed: segment_079.wav → Dementsprechend wollen wir den Code irgendwie lokal bei uns zur Verfügung haben. Eine zweite Minute wäre natürlich, den Quellcode von React runterzuladen.\n",
      "✅ Transcribed: segment_080.wav → könnten wir machen, ist heutzutage aber eher altmutig. Ist auch sehr aufwendig, wenn wir zum Beispiel eine neue Version von Vect installieren wollen.\n",
      "✅ Transcribed: segment_081.wav → müssten wir wieder auf die React-Webseite den Code herunterladen. Also das ist ein Vorgehen, welches eher heutzutage nicht mehr zu empfehlen ist. Da gibt es höhere Methoden.\n",
      "✅ Transcribed: segment_082.wav → Eine ist die Nutzung einer sogenannten CDN URL. CDN steht für Content Delivery Network. Das ist letztendlich einfach eine Website, auf der React uns\n",
      "✅ Transcribed: segment_083.wav → den Quellcode schon einmal hochgeladen hat und den wir über die URL ganz einfach in unserer index.html einbinden können. Und das ist ideal für schnelle Tests oder auch für Lernprozesse.\n",
      "✅ Transcribed: segment_084.wav → so wie dieses Jahr hier ein Lernprojekt ist. Und dementsprechend werden wir auch heute auf diese Variante der Installation von Reaktor greifen.\n",
      "✅ Transcribed: segment_085.wav → Die vierte Variante, die es auch noch gibt, ist natürlich die Installation mit NPM. Denn React gibt es auch auf NPM.\n",
      "✅ Transcribed: segment_086.wav → Stelle. Ügen nicht. Project wurde auch auf NPM hochgeladen.\n",
      "✅ Transcribed: segment_087.wav → dass die Installation über NPM bzw. danach die Einbindung in eure Website nicht ganz so leicht von da angeht. Da braucht es dann doch\n",
      "✅ Transcribed: segment_088.wav → das ein oder andere Tool, was die Sache initial sehr komplex macht. Das rentiert sich bei größeren\n",
      "✅ Transcribed: segment_089.wav → komplexer Aufwand wird dann amortisiert über viel Arbeit, die uns diese Variante später erspart. Die Installation von React mit NPM\n",
      "✅ Transcribed: segment_090.wav → werden wir in einer späteren Folge dann auch noch vornehmen, spätestens im zweiten Teil dieser Videoserie, wo sich ja alles um das Tooling drehen wird. Für heute wie gesagt\n",
      "✅ Transcribed: segment_091.wav → nutzen wir aber den CDL URL Link. Das Ganze sieht dann so aus, dass wir in unserer index.html diese zwei Script Tags einstellen.\n",
      "✅ Transcribed: segment_092.wav → führen und damit ist React dann auf unserer Seite schon verfügbar. Diese zwei Script Tags findet ihr ebenfalls auf\n",
      "✅ Transcribed: segment_093.wav → reactjs.org auf der offiziellen Seite unter CDN links. Ihr werdet hier sehen, dass es zwei unterschiedliche Varianten gibt, die Projekt einzubinden.\n",
      "✅ Transcribed: segment_094.wav → einmal im Development Modus und einmal im Production Modus. Der Unterschied ist ganz einfach, dass der Development Modus einige bessere Fehler meldet.\n",
      "✅ Transcribed: segment_095.wav → zur Verfügung stellt und einige Tools uns zur Verfügung stellt, die das Entwickeln von React leichter machen, die aber gleichzeitig diese JavaScript-Dateien sehr aufplänen und sehr groß machen.\n",
      "✅ Transcribed: segment_096.wav → Im produktiven Einsatz von React wollen wir natürlich so wenig wie möglich Code auf der Seite, weil der ja auch heruntergeladen werden muss. Dementsprechend hier diese Unterscheidung.\n",
      "✅ Transcribed: segment_097.wav → jetzt aber noch weit weg sind von einem produktiven Einsatz, beschränken wir uns heute auf die Development-Sourcen, die ich hier einfach schon mal kopiere.\n",
      "✅ Transcribed: segment_098.wav → Wenn wir dann nachher die Reakt-Sourcen in unserem Projekt eingebunden haben, wollen wir Reakt natürlich auch verwenden.\n",
      "✅ Transcribed: segment_099.wav → Der ist den ersten Code oder das erste, den ersten Block mit React, den wir verwenden werden, ist eben, wie vorher angekündigt schon, React Element. React Element\n",
      "✅ Transcribed: segment_100.wav → oder ein Reakt-Element erstellen wir über diesen Funktionsauffruch ReaktCreateElement und der besteht aus drei Parametern. Der erste Parameter gibt das HTML.\n",
      "✅ Transcribed: segment_101.wav → html-Tag an, das wir generieren wollen. In diesem Fall zum Beispiel ein p-Tag. Der zweite Parameter gibt an, welche Attribute wir diesem html-Tag mitgeben wollen.\n",
      "✅ Transcribed: segment_102.wav → und der dritte Parameter gibt an, welchen Inhalt wir in dieses HTML-Tech reinschreiben wollen.\n",
      "✅ Transcribed: segment_103.wav → Dieser Funktionsaufruf macht letztendlich nichts anderes als ein simples JavaScript-Objekt zu erzeugen.\n",
      "✅ Transcribed: segment_104.wav → JavaScript-Objekt wird aber von React verstanden und kann dann von React wiederum in den Virtual DOM von React implementiert werden oder eingebunden werden.\n",
      "✅ Transcribed: segment_105.wav → Sobald es im Virtual Dom eingebunden ist, wir erinnern uns, React synchronisiert den Virtual Dom damit im richtigen Dom, wird eben dieses Element zu einem realen Dom.\n",
      "✅ Transcribed: segment_106.wav → Dom-Objekt und dementsprechend im Browser dargestellt. Man kann also sagen, dass ein Reakt-Element ein reales Dom-Objekt repräsentiert.\n",
      "✅ Transcribed: segment_107.wav → Wenn wir uns das Ganze auf der Visualisierung noch einmal anschauen, haben wir nun also neben dem Virtual Dumpf Reakt in unseren JavaScript-Dateien diesen Funktionsaufruf.\n",
      "✅ Transcribed: segment_108.wav → React-Create-Element, welcher dann eben hier im Virtual Dom als einzelnen Element eingebunden wird, dann mit dem Dom synchronisiert und nachher in unserem Browser dargestellt.\n",
      "✅ Transcribed: segment_109.wav → Dieser Visualisierung habe ich ebenfalls npm hinzugefügt, welches eben über die Package Chasing, die wir gerade schon gesehen haben, initiiert wird.\n",
      "✅ Transcribed: segment_110.wav → und installiert wird. Und wir haben NPX kennengelernt, was uns den Webseller startet. Nun wollen wir das Ganze noch einmal.\n",
      "✅ Transcribed: segment_111.wav → eben umsetzen. Wir kopieren nochmal kurz die Sources, die brauchen wir jetzt als allererstes. Gehen zurück in unser Visual Studio.\n",
      "✅ Transcribed: segment_112.wav → und fügen nun React ganz einfach am Ende des BodyTags ein. An sich war es das schon.\n",
      "✅ Transcribed: segment_113.wav → Jetzt haben wir Reakt auf unserer Seite. Wir tun aber natürlich noch nichts damit. Um noch etwas damit zu tun, brauchen wir noch etwas eigenes JavaScript, das wir einbinden.\n",
      "✅ Transcribed: segment_114.wav → Dementsprechend fügen wir nun erstmal hier noch ein zweites oder ein drittes Script Tag hinzu. Das bei uns auf.\n",
      "✅ Transcribed: segment_115.wav → sourceapp.js zeigen wird. Diese Totei legen wir dann noch an. Erstmal ein Ordner source.\n",
      "✅ Transcribed: segment_116.wav → Das ist einfach ein gängiges Vorgehen. SRC steht eben für Source, in dem alle Source-Dateien eines Projektes abgelegt werden. Dort legen wir dann die Datei an.\n",
      "✅ Transcribed: segment_117.wav → beobachten. Bevor wir nun JavaScript schreiben, bereiten wir in der Index.html noch eine weitere Sache vor. Wir erinnern uns\n",
      "✅ Transcribed: segment_118.wav → vielleicht das letzte Mal an die React DOM Render Funktion, der wir zum einen mitgeben, eine React Komponente, die wir gerendert haben wollen, zum anderen aber auch angeben müssen, wohin in unserer\n",
      "✅ Transcribed: segment_119.wav → HTML wir diese Komponente gelündert haben wollen. Und dieses wohin erstellen wir jetzt hier. Wir machen uns ein einfaches div und geben dem eine id.\n",
      "✅ Transcribed: segment_120.wav → anhand der wir es dann später identifizieren können und das nehmen wir dann einfach React.\n",
      "✅ Transcribed: segment_121.wav → Wenn wir nochmal kurz auf die Seite schauen, es hat sich nichts verändert. Alles beim alten.\n",
      "✅ Transcribed: segment_122.wav → noch kein Vierktelement generiert haben, das wir hier einbinden. Das tun wir jetzt. Zuerst generieren wir uns also ein Vierktelement.\n",
      "✅ Transcribed: segment_123.wav → Das speichern wir in eine Variable, in die wir jetzt einfach mal Element nennen, nutzen nun eben Reakt.Create.\n",
      "✅ Transcribed: segment_124.wav → Element. React ist eine globale Variable, die uns jetzt eben zur Verfügung steht, weil wir hier diese über dieses Script eingebunden haben.\n",
      "✅ Transcribed: segment_125.wav → macht letztendlich nichts anderes als die Reaktionshosen unter dieser Variablen zur Verfügung zu stellen und so auch die CreateElement Methode. Als ersten Parameter eben das\n",
      "✅ Transcribed: segment_126.wav → Tag, in dem Fall bleiben wir immer beim P tag. Als zweiter Parameter die Attribute. Das lassen wir erstmal leer.\n",
      "✅ Transcribed: segment_127.wav → Dritten Parameter den Content. Hier schreiben wir jetzt einfach rein, das ist mein erstes React.\n",
      "✅ Transcribed: segment_128.wav → Element. Richtiger. Wie nur angesprochen, React-Trade Element generiert erst einmal nur ein ...\n",
      "✅ Transcribed: segment_129.wav → Simples JavaScript-Objekt. Mit diesem Objekt müssen wir jetzt noch etwas tun. Wir müssen ein React zeigen, wohin es uns dieses JavaScript-Objekt oder dieses React-Element...\n",
      "✅ Transcribed: segment_130.wav → nachher im Browser rendern soll. Und dazu nutzen wir react-dom. Das ist ebenfalls eine globale Variable, die uns zur Verfügung steht.\n",
      "✅ Transcribed: segment_131.wav → wir hier dieses zweite Skript eingebunden haben für React Dom. Das liefert uns eben die Wenderfunktion.\n",
      "✅ Transcribed: segment_132.wav → Wir müssen erst unser Element reingeben. High Element. Und nun sagen müssen, wohin wir das geländert haben wollen. Und hierzu haben wir uns ja vorher das Diff angelegt. Und eine Edifier-Partei.\n",
      "✅ Transcribed: segment_133.wav → und das können wir hier nun mit einem Selektor herausfinden oder herausfiltern, indem wir sagen document getElementBy\n",
      "✅ Transcribed: segment_134.wav → Und hier die ID, die wir vorher vergeben haben, in dem Fall React App, kopieren und einfügen.\n",
      "✅ Transcribed: segment_135.wav → Wenn wir das ganze speichern und auf die Website gehen, dann sehen wir, juhu, wir haben unsere erste Reakt Komponente.\n",
      "✅ Transcribed: segment_136.wav → unser erstes React Element auf der Website gerendert. Wir haben zum allerersten Mal React eingesetzt, um unseren Dom zu manipulieren. Das ist natürlich jetzt noch etwas...\n",
      "✅ Transcribed: segment_137.wav → statisch, aber ist doch immerhin schon mal ein toller erster Schritt. Jetzt werdet ihr euch fragen, ich habe das letzte mal erzählt.\n",
      "✅ Transcribed: segment_138.wav → von Deklarativen programmieren und von JSX, dass es uns erlaubt, HTML in JavaScript-Dateien zu schreiben. Ihr würdet nun erwarten, dass wir...\n",
      "✅ Transcribed: segment_139.wav → eigentlich das ganze nicht hier mit dem Reactory Element Aufruf machen, sondern eher sowas schreiben können wie so also ein HTML Element\n",
      "✅ Transcribed: segment_140.wav → mit der ID direkt in der JavaScript-Datei.\n",
      "✅ Transcribed: segment_141.wav → Wenn wir das machen, werden wir einen Fehler bekommen. Wenn wir zurück navigieren in den Browser, sehen wir zum einen\n",
      "✅ Transcribed: segment_142.wav → dass unser Element nicht mehr gerendert wurde. Um herauszufinden, was hier gerade schiefgelaufen ist, können wir die Entwicklertools von Chrome oder ...\n",
      "✅ Transcribed: segment_143.wav → von jedem anderen Browser, jeder Browser bringt Entwicklertools mit, uns zur Hand nehmen. Das können wir entweder über die Taste F12 machen oder indem wir rechtsklick untersuchen machen.\n",
      "✅ Transcribed: segment_144.wav → In diesen Untersuchen sehen wir dann unseren aktuellen DOM oder unser HTML, das wir auch hier durch navigieren können. Unser Title, unser Meta-Characters.\n",
      "✅ Transcribed: segment_145.wav → eben der Buddy. Wenn ich es auch schaffe, da mal hinzukicken. So, perfekt. Und wir haben auch die Konsolen.\n",
      "✅ Transcribed: segment_146.wav → die uns Fehler ausspuckt. Und in diesem Fall sehen wir jetzt, wir haben einen Sündungsfehler, ein unerwartetes Zeichen, Unexpected Token, eine geöffnete Klammer.\n",
      "✅ Transcribed: segment_147.wav → Und das ist genau dieses Zeichen. Blick daran, dass ein Browser heutzutage zumindest noch nicht JSX versteht.\n",
      "✅ Transcribed: segment_148.wav → natives JavaScript und das hier ist nun mal kein natives JavaScript. Dementsprechend kann der Browser diesen Code-Schnippel einfach auch nicht verstehen.\n",
      "✅ Transcribed: segment_149.wav → Und wir müssen erst etwas tun, damit er das kann. Wir müssen nämlich diesen Code Snippet zurück umwandeln in ganz normales JavaScript. Und an dieser Stelle schon ein kleiner Hint.\n",
      "✅ Transcribed: segment_150.wav → React Create Element ist ganz normales JavaScript. Das haben wir gerade gesehen, dass das funktioniert. Um das nun zu ermöglichen und um diese Übersetzung vorzunehmen,\n",
      "✅ Transcribed: segment_151.wav → Das müssen wir natürlich nicht händisch machen, das wäre sehr aufwendig, sondern da gibt es Tools. Und eines dieser Tools, wir gehen zurück zur Theorie,\n",
      "✅ Transcribed: segment_152.wav → Babel. Babel ist eben so eine Art Helferin oder eine Übersetzerin, die uns Features und Funktionen sowie GSX zur Verfügung stellt.\n",
      "✅ Transcribed: segment_153.wav → und zurück übersetzt in natives JavaScript, dass der Browser versteht. Babel besteht im Wesentlichen aus drei Kernkomponenten, die wir auch heute einsetzen werden.\n",
      "✅ Transcribed: segment_154.wav → Das ist einmal der Core. Das ist die gesamte Übersetzungslogik. Das Command Line Interface, oder kurz CLI, das lässt uns mit Babel kommunizieren und sprechen.\n",
      "✅ Transcribed: segment_155.wav → und Babel mitteilen, was wir eigentlich übersetzt haben wollen. Und es gibt Presets. Presets könnt ihr euch vorstellen als Wörterbücher, die wir Babel mitgeben, um Babel zu ermitteln.\n",
      "✅ Transcribed: segment_156.wav → beispielsweise GSX, den normalen JavaScript zu übersetzen. Und wir setzen heute Preset React ein. Preset React ist eben genau dieses Wörterbuch, das den Einsatz\n",
      "✅ Transcribed: segment_157.wav → JSX in unseren Dateien erlaubt und Babel wird dieses JSX dann umwandeln in ganz normales JavaScript.\n",
      "✅ Transcribed: segment_158.wav → aktuell standen wir hier, wir haben mit JavaScript React und Create Element ausgeführt, das hat funktioniert, haben jetzt aber JSX mit reingebracht.\n",
      "✅ Transcribed: segment_159.wav → mehr funktioniert. JSX müssen wir also nun erst durch Babel schleifen, um das in normales JavaScript umzuwandeln, das dann wiederum von React bzw. von unseren Browsern aufwacht.\n",
      "✅ Transcribed: segment_160.wav → verstanden werden kann. Und an dieser Stelle wird es vielleicht schon langsam bewusst, Babel macht nichts anderes als aus diesen JSXs\n",
      "✅ Transcribed: segment_161.wav → oder aus den HTML-Teilen in JavaScript, die wir in JSX schreiben, react.createElement-Funktionsaufrufe zu machen.\n",
      "✅ Transcribed: segment_162.wav → Das ist die ganze Magie, die dahinter steckt und das zeige ich euch auch gleich hands-on. Aber zuerst, wie installieren wir Babel?\n",
      "✅ Transcribed: segment_163.wav → Dazu nutzen wir nun eben NPM und wir werden den Babel Core, die Babel CLI und das Preset React in unserem Projekt installieren.\n",
      "✅ Transcribed: segment_164.wav → machen wir jetzt. Zurück also in die Command Line. Wir beenden mal kurz unseren Web Server an dieser Stelle und führen nun das Command aus.\n",
      "✅ Transcribed: segment_165.wav → install add-barbel-core, add-barbel-cli und add-barbel-reset\n",
      "✅ Transcribed: segment_166.wav → Wir fügen hier noch einen kleinen weiteren Parameter hinzu, nämlich \"-save-def\". Warum wir das tun, erkläre ich dann gleich.\n",
      "✅ Transcribed: segment_167.wav → Das dauert ein wenig, weil er die ganzen Pakete natürlich erst runterladen muss.\n",
      "✅ Transcribed: segment_168.wav → Jetzt seht ihr hier ein Error, das passiert auf Mac. Nicht wundern, wenn ihr diesen Error seht und auch keine Sorge, das spielt für uns erstmal keine Rolle.\n",
      "✅ Transcribed: segment_169.wav → Die Features von Babel werden wegen dieses Errors nicht funktionieren, das sind aber keine Features, die wir jetzt gerade benötigen. Von dem her können wir diesen Error einfach getrost ignorieren.\n",
      "✅ Transcribed: segment_170.wav → ist abgeschlossen. Gehen wir zurück in Visual Studio Code. Schauen wir mal kurz, hier hat sich etwas verändert. Zum einen wurden in unserer Package Chasen\n",
      "✅ Transcribed: segment_171.wav → ein neues Feld hinzugefügt, nämlich diese Dev-Dependencies. Das ist das, was ich mit dem Befehl \"-save-dev veranlasst habe und das schreibt letztendlich\n",
      "✅ Transcribed: segment_172.wav → einfach alle Abhängigkeiten unseres Projekts oder alle Entwicklungsabhängigkeiten hier in diese Package chasen. Ich will jetzt nicht zu sehr ins Detail gehen, weil das\n",
      "✅ Transcribed: segment_173.wav → Das ist ja ein Kurs über React und nicht über NPM, aber einfach, dass ihr versteht, warum ich diesen Befehl angegeben habe. Es ist auch eine Package-Log-Json generiert worden. Das ist sozusagen ein Log-Json.\n",
      "✅ Transcribed: segment_174.wav → aller derer Pakete, die wir installiert haben mit genauen Funktionen. Und zu guter Letzt wurde hier Node Modules angelegt. Das ist der Ordner, in dem die Pakete, die wir installiert haben,\n",
      "✅ Transcribed: segment_175.wav → haben tatsächlich heruntergeladen wurden. Wenn wir das mal kurz aufmachen, sehen wir hier natürlich deutlich mehr als wir installiert haben. Das liegt einfach da.\n",
      "✅ Transcribed: segment_176.wav → dass Babel selbst ja auch Abhängigkeiten hat, die es wiederum über eine Package-JSON definiert und alle diese Abhängigkeiten installiert Babel oder NPM für uns dann gleich mit.\n",
      "✅ Transcribed: segment_177.wav → dass wir das nicht händisch machen müssen. Aber wir sehen auch, AdBabel CLI wurde installiert, AdBabel Core wurde installiert und weiter unten.\n",
      "✅ Transcribed: segment_178.wav → etwa mit Preset Reakt ist dann auch in unserem Projekt vorhanden. Nun müssen wir\n",
      "✅ Transcribed: segment_179.wav → Babel noch entsprechend ein wenig konfigurieren bzw. auch ausführen. Denn aktuell wüsste Babel ja noch nicht, was es tun soll und Babel weiß aktuell auch noch nicht.\n",
      "✅ Transcribed: segment_180.wav → welche Wörterbücher es einsetzen soll. Fangen wir mit dem Wörterbuch an. Wir können Babel ganz einfach konfigurieren, indem wir nochmal eine neue Datei anlegen, die sich Punkt Babel\n",
      "✅ Transcribed: segment_181.wav → RC nennt. In dieser Datei können wir valides Chasern schreiben. Also das ist eigentlich nur eine Chasen-Datei, auch wenn sie nicht auf Chasen-M sind.\n",
      "✅ Transcribed: segment_182.wav → Wir können dort ein Objekt anlegen und das Feld Presets befüllen und dort nun in einem Array alle die Presets reinschreiben, die ...\n",
      "✅ Transcribed: segment_183.wav → Babel für uns verwenden, also sprich alle diese Wörterbücher, die wir übersetzen wollen. In unserem Fall ist das nur eines, nämlich Babel preset react.\n",
      "✅ Transcribed: segment_184.wav → Das ist der erste Schritt. Im zweiten Schritt müssen wir nun aus dieser App.js eine App\n",
      "✅ Transcribed: segment_185.wav → JSX-Datei machen. Denn wir haben hier ja tatsächlich nicht mehr verlieh das JavaScript stehen, deswegen ist das auch keine JavaScript-Datei mehr, sondern es ist jetzt eine JSX-Datei.\n",
      "✅ Transcribed: segment_186.wav → wir hier JSX eingefügt haben. Und nun müssen wir Babel diese Datei für uns übersetzen lassen. Das können wir nun eben mit Babel CLI über die Command-Karte\n",
      "✅ Transcribed: segment_187.wav → Es gibt nun zwei Möglichkeiten.\n",
      "✅ Transcribed: segment_188.wav → einsetzen, um Babel direkt auszuführen. Wir haben aber die Babel CLI auch bei uns installiert. Das Ganze nehme ich in Node Modules.\n",
      "✅ Transcribed: segment_189.wav → bin oder binary folder gibt es den Befehl babel. Dieser Befehl erwartet drei Parameter. Zum einen\n",
      "✅ Transcribed: segment_190.wav → das Surs-Verzeichnis oder das Verzeichnis, das Babel für uns übersetzen soll. In unserem Fall ist das tatsächlich Surs. Es erwartet den Parameter minus minus out hier.\n",
      "✅ Transcribed: segment_191.wav → wo Babel die kombinierten oder die übersetzten Dateien hinschreiben soll. Und das machen wir bei uns jetzt einfach mal in ein Verzeichnis, das wir lib nennen.\n",
      "✅ Transcribed: segment_192.wav → Wenn wir mit diesen Befehlen nun ausführen, quittiert uns Babel das mit einer erfolgreichen Meldung, dass es uns eine Datei erfolgreich kompiliert hat.\n",
      "✅ Transcribed: segment_193.wav → oder übersetzt hat. Wenn wir zurück ins Projektverzeichnis springen, sehen wir auch, dass der Lipfolder angelegt wurde und hier entsprechend analog zu unserer App JSX.\n",
      "✅ Transcribed: segment_194.wav → eine App.js-Datei angelegt wurde. Und die können wir uns auch anschauen. Und siehe da. Aus unserem JSX ist nichts anderes.\n",
      "✅ Transcribed: segment_195.wav → geworden als ein React-Create-Element-Aufruf, der relativ oder nicht nur relativ, sondern ziemlich gleich aussieht, wie das, was wir davor händisch eingegeben haben.\n",
      "✅ Transcribed: segment_196.wav → Das ist tatsächlich die gesamte Magie hinter JSX und Babel. Das macht nichts anderes, als die Teile, die HTML und unserem JSX sind, in ein React.CreateElement zu übersetzen.\n",
      "✅ Transcribed: segment_197.wav → Nun müssen wir, damit das auch funktioniert, noch eine kleine Änderung in unserer Index-HTML vornehmen, weil wir jetzt\n",
      "✅ Transcribed: segment_198.wav → nicht mehr die App.js aus unseren Source ausliefern oder die App.js X, sondern wir müssen ja die übersetzte Variante unserer App ausliefern. Dementsprechend erinnern wir...\n",
      "✅ Transcribed: segment_199.wav → das hier einfach auf LIP. Und wenn wir nun nochmal einen LAP-Server starten.\n",
      "✅ Transcribed: segment_200.wav → und zurück auf unsere Website navigieren. Sehen wir, wir haben ein erstes, ups, nein, wir haben noch das Alchemy Element.\n",
      "✅ Transcribed: segment_201.wav → weiß auch warum. Weil wir hier noch das alte Element auch eingebunden haben. Wir wollen jetzt aber eigentlich hier das die erste ChessX Komponenta einbinden. Dementsprechend\n",
      "✅ Transcribed: segment_202.wav → müssen wir das hier ersetzen. Wir wollen nun My.js.x-Element rendern. Wir müssen das ganze jetzt natürlich von Babel nochmal neu überwinden.\n",
      "✅ Transcribed: segment_203.wav → setzen lassen. Wenn wir jetzt den Webserver starten sollten wir tatsächlich sehen, dass wir unsere erste CharsX Component\n",
      "✅ Transcribed: segment_204.wav → erfolgreich im Browser gewendet haben. Das ist super!\n",
      "✅ Transcribed: segment_205.wav → Zum Abschluss kommen möchte ich euch noch einen kleinen Tipp mitgeben, denn diesen Befehl, den wir hier gerade gesehen haben Not Modules Bin Babels Source\n",
      "✅ Transcribed: segment_206.wav → ist schwer zu merken und ist auch nicht sonderlich schön einzutippen. Da können wir uns mit npm und npm scripts ein wenig Abfehlung verschaffen.\n",
      "✅ Transcribed: segment_207.wav → Erlaubt uns es, nämlich Skripte unter einem sogenannten Alias oder einem anderen Namen vorzudefinieren. Wenn wir jetzt also hier sowas wie compile schreiben.\n",
      "✅ Transcribed: segment_208.wav → Dort den Befehl, WABEL, SUS, minus minus OUT, DIR, LIP, gefolgt von einem Komma reinschreiben.\n",
      "✅ Transcribed: segment_209.wav → können wir diesen sehr aufwendigen command line ausführen. ganz kurz wir können uns hier das noten module spin sparen, weil\n",
      "✅ Transcribed: segment_210.wav → die Scripts von npm per Default dieses Binary-Verzeichnis einbinden. Das heißt, hier können wir auf diesen ganzen vorherigen Pfad verzichten und einfach Babel direkt ausführen.\n",
      "✅ Transcribed: segment_211.wav → Und nun können wir einfach über den Befehl in unserem aktuellen Verzeichnis npm run jedes Script, das wir definiert haben, ausführen.\n",
      "✅ Transcribed: segment_212.wav → und wir sehen, wurde Babel wieder ausgeführt, um uns unsere JSX-Dateien zu übersetzen.\n",
      "✅ Transcribed: segment_213.wav → Letztendlich kann hier in scripts jedes valide CLI-Command eingefügt werden. Wir können so die für unser Projekt relevanten\n",
      "✅ Transcribed: segment_214.wav → die CLE-Commands sehr, sehr einfach und sehr, sehr übersichtlich in der Package-Chase zu pflegen. Das ist ein kleiner Trick, der in vielen Projekten auch sehr massiv eingesetzt wird.\n",
      "✅ Transcribed: segment_215.wav → Das können wir zum Beispiel als letztes noch erweitern, indem wir hier NPX Server eintragen.\n",
      "✅ Transcribed: segment_216.wav → ein tippen müssen, sondern nun um unseren Webserver zu starten und die Website auszuliefern, npm run start eingeben können. Und nun ist unser Webserver wieder.\n",
      "✅ Transcribed: segment_217.wav → verfügbar. Und das war es auch schon zur heutigen Folge.\n",
      "✅ Transcribed: segment_218.wav → Noch mal ein kurzes Review. Was haben wir gemacht? Wir haben das lokale Setup mit Visual Studio Code und NPM vorbereitet. Ich habe einen ersten Einblick in die Tools NPM, NPX und ...\n",
      "✅ Transcribed: segment_219.wav → und Babel gegeben und wir haben diese auch schon live eingesetzt. Wir haben React eingebunden mit der Variante CDN oder Content Delivery Network, URL.\n",
      "✅ Transcribed: segment_220.wav → wir haben mit React Element und JSX eine erste Hello World Applikation von React implementiert. Da haben wir schon einen großen Schritt gemacht. Das nächste Mal wird es noch spannender.\n",
      "✅ Transcribed: segment_221.wav → Da werden wir nämlich React Components vorstellen und einsetzen und mit den React Components die Renderfunktion, Props und dann noch ein paar Besonderheiten von JL.\n",
      "✅ Transcribed: segment_222.wav → an dieser Stelle bedanke ich mich recht herzlich, dass ihr dieses Video angesehen habt. Ich hoffe, ihr konntet etwas lernen.\n",
      "✅ Transcribed: segment_223.wav → Bei Fragen, Feedback oder sonstigen Wünschen und Anregungen meldet euch gerne bei uns über diese Kanäle. Hello at theNativeFab.io per E-Mail oder auch auf Twitter und Github.\n",
      "✅ Transcribed: segment_224.wav → und ich wünsche euch jetzt noch einen schönen Tag und bis zur nächsten Folge. Ciao!\n",
      "🎉 Transcription complete! Metadata saved: /home/ahmet/my_projects/realtime_translator_V2_updated/server/segmented_wavs/metadata.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "\n",
    "# ✅ Load Whisper Model (Use 'medium' for better accuracy)\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# ✅ Paths\n",
    "input_folder = os.path.join(os.getcwd(), \"segmented_wavs\")\n",
    "output_metadata = os.path.join(os.getcwd(), \"segmented_wavs/metadata.txt\")\n",
    "\n",
    "# ✅ Transcribe and Save Metadata\n",
    "with open(output_metadata, \"w\", encoding=\"utf-8\") as f:\n",
    "    for file in sorted(os.listdir(input_folder)):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            result = model.transcribe(file_path, language=\"de\")  # Set your language\n",
    "            transcript = result[\"text\"].strip()\n",
    "            \n",
    "            if transcript:\n",
    "                f.write(f\"{file.replace('.wav', '')}|{transcript}\\n\")\n",
    "                print(f\"✅ Transcribed: {file} → {transcript}\")\n",
    "\n",
    "print(f\"🎉 Transcription complete! Metadata saved: {output_metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All audio files are within the allowed duration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "# ✅ Set Paths\n",
    "audio_folder = os.path.join(os.getcwd(), \"segmented_wavs/wavs\")  # Update this path\n",
    "max_duration = 10.0  # Maximum allowed duration in seconds\n",
    "\n",
    "# ✅ Scan and Check Durations\n",
    "long_files = []\n",
    "for file in sorted(os.listdir(audio_folder)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(audio_folder, file)\n",
    "        audio, sr = librosa.load(file_path, sr=None)  # Load with original sample rate\n",
    "        duration = librosa.get_duration(y=audio, sr=sr)\n",
    "\n",
    "        if duration > max_duration:\n",
    "            long_files.append((file, round(duration, 2)))  # Store file name & duration\n",
    "\n",
    "# ✅ Print Results\n",
    "if long_files:\n",
    "    print(f\"🚨 {len(long_files)} audio files exceed {max_duration}s:\")\n",
    "    for file, duration in long_files:\n",
    "        print(f\"  ❌ {file}: {duration}s\")\n",
    "else:\n",
    "    print(\"✅ All audio files are within the allowed duration.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def extract_audio_segments(wav_file, json_file, output_folder):\n",
    "    \"\"\"\n",
    "    Extracts speech segments from the full WAV file using JSON timestamps.\n",
    "\n",
    "    Args:\n",
    "        wav_file (str): Path to the full WAV audio file.\n",
    "        json_file (str): Path to the JSON subtitles/transcripts file.\n",
    "        output_folder (str): Folder where extracted audio files and metadata.txt will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Load full audio file\n",
    "    audio, sr = librosa.load(wav_file, sr=22050)  # Resample to 22.05kHz for TTS\n",
    "\n",
    "    # Load JSON transcript\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript_data = json.load(f)\n",
    "\n",
    "    metadata_entries = []\n",
    "\n",
    "    # Process each segment in the JSON file\n",
    "    for idx, segment in enumerate(transcript_data):\n",
    "        start_time = segment[\"start_time\"] / 1000.0  # Convert ms to sec\n",
    "        end_time = segment[\"end_time\"] / 1000.0 +0.5  # Convert ms to sec\n",
    "        text = segment[\"text\"].strip()\n",
    "\n",
    "        # Extract audio segment\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "\n",
    "        # Save audio segment\n",
    "        segment_filename = f\"audio_{idx+1:03d}.wav\"\n",
    "        segment_path = os.path.join(output_folder, segment_filename)\n",
    "        sf.write(segment_path, audio_segment, sr)\n",
    "\n",
    "        # Add entry to metadata\n",
    "        metadata_entries.append(f\"{segment_filename}|{text}|{text.lower()}\")\n",
    "\n",
    "    # Save metadata.txt\n",
    "    metadata_path = os.path.join(output_folder, \"metadata.txt\")\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(metadata_entries))\n",
    "\n",
    "    print(f\"✅ Audio segments saved to: {output_folder}\")\n",
    "    print(f\"✅ Metadata file saved: {metadata_path}\")\n",
    "\n",
    "    return metadata_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Audio segments saved to: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones\n",
      "✅ Metadata file saved: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones/metadata.txt\n"
     ]
    }
   ],
   "source": [
    "metadata_file = extract_audio_segments(wav_file_path, json_transcript_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment, silence\n",
    "import numpy as np\n",
    "\n",
    "def segment_audio_smart_v2(wav_file, output_folder, target_chunk_duration=10, max_chunk_duration=12, min_chunk_duration=5, silence_threshold=-40, silence_padding=0.2):\n",
    "    \"\"\"\n",
    "    Segments an audio file into chunks of approximately target_chunk_duration seconds,\n",
    "    prioritizing cuts at silent periods to avoid mid-word cuts.\n",
    "\n",
    "    Args:\n",
    "        wav_file (str): Path to the input WAV file.\n",
    "        output_folder (str): Directory for segmented audio files.\n",
    "        target_chunk_duration (int): Target duration for each chunk in seconds.\n",
    "        max_chunk_duration (int): Maximum allowed duration for a chunk in seconds.\n",
    "        min_chunk_duration (int): Minimum allowed duration for a chunk in seconds.\n",
    "        silence_threshold (int): Silence threshold in dBFS for pydub silence detection.\n",
    "        silence_padding (float):  Seconds of silence to include after a detected silence point.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths to the generated audio segment files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    segment_paths = []\n",
    "    segment_count = 1\n",
    "\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "    audio_duration_sec = len(audio) / 1000  # Duration in seconds\n",
    "    start_time_ms = 0\n",
    "\n",
    "    while start_time_ms < len(audio):\n",
    "        target_end_time_ms = start_time_ms + target_chunk_duration * 1000\n",
    "        max_end_time_ms = min(start_time_ms + max_chunk_duration * 1000, len(audio))\n",
    "        min_end_time_ms = min(start_time_ms + min_chunk_duration * 1000, len(audio))\n",
    "\n",
    "        if max_end_time_ms >= len(audio): # Last segment\n",
    "            end_time_ms = len(audio)\n",
    "        else:\n",
    "            # Look for silence near the target end time\n",
    "            search_start_ms = max(start_time_ms + min_chunk_duration * 1000, target_end_time_ms - 2000) # Search silence after min duration and around target\n",
    "            search_end_ms = max_end_time_ms\n",
    "\n",
    "            silent_ranges = silence.detect_silence(\n",
    "                audio[search_start_ms:search_end_ms],\n",
    "                min_silence_len=200,  # Shorter silence detection for finer cuts\n",
    "                silence_thresh=silence_threshold\n",
    "            )\n",
    "\n",
    "            if silent_ranges:\n",
    "                # Take the first silence found\n",
    "                silence_end_relative_ms = silent_ranges[0][0] # Start of silence relative to search_start_ms\n",
    "                end_time_ms = search_start_ms + silence_end_relative_ms + int(silence_padding * 1000) # Cut at silence + padding\n",
    "                end_time_ms = min(end_time_ms, max_end_time_ms) # Ensure not exceeding max duration\n",
    "                end_time_ms = max(end_time_ms, min_end_time_ms) # Ensure not less than min duration\n",
    "            else:\n",
    "                end_time_ms = max_end_time_ms # If no silence found, cut at max duration\n",
    "\n",
    "\n",
    "        segment_audio = audio[start_time_ms:end_time_ms]\n",
    "        segment_filename = f\"segment_{segment_count:03d}.wav\"\n",
    "        segment_path = os.path.join(output_folder, segment_filename)\n",
    "        segment_audio.export(segment_path, format=\"wav\")\n",
    "        segment_paths.append(segment_path)\n",
    "\n",
    "        start_time_ms = end_time_ms\n",
    "        segment_count += 1\n",
    "\n",
    "    print(f\"✅ Smart segmentation v2 done with {len(segment_paths)} files.\")\n",
    "    return segment_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Smart segmentation v2 done with 256 files.\n",
      "Segmented files: ['/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_001.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_002.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_003.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_004.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_005.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_006.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_007.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_008.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_009.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_010.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_011.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_012.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_013.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_014.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_015.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_016.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_017.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_018.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_019.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_020.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_021.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_022.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_023.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_024.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_025.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_026.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_027.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_028.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_029.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_030.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_031.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_032.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_033.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_034.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_035.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_036.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_037.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_038.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_039.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_040.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_041.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_042.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_043.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_044.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_045.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_046.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_047.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_048.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_049.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_050.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_051.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_052.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_053.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_054.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_055.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_056.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_057.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_058.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_059.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_060.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_061.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_062.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_063.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_064.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_065.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_066.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_067.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_068.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_069.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_070.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_071.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_072.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_073.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_074.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_075.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_076.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_077.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_078.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_079.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_080.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_081.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_082.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_083.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_084.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_085.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_086.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_087.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_088.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_089.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_090.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_091.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_092.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_093.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_094.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_095.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_096.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_097.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_098.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_099.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_100.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_101.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_102.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_103.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_104.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_105.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_106.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_107.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_108.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_109.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_110.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_111.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_112.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_113.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_114.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_115.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_116.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_117.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_118.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_119.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_120.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_121.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_122.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_123.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_124.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_125.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_126.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_127.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_128.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_129.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_130.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_131.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_132.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_133.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_134.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_135.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_136.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_137.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_138.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_139.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_140.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_141.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_142.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_143.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_144.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_145.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_146.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_147.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_148.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_149.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_150.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_151.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_152.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_153.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_154.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_155.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_156.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_157.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_158.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_159.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_160.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_161.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_162.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_163.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_164.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_165.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_166.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_167.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_168.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_169.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_170.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_171.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_172.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_173.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_174.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_175.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_176.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_177.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_178.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_179.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_180.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_181.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_182.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_183.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_184.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_185.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_186.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_187.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_188.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_189.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_190.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_191.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_192.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_193.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_194.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_195.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_196.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_197.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_198.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_199.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_200.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_201.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_202.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_203.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_204.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_205.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_206.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_207.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_208.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_209.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_210.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_211.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_212.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_213.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_214.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_215.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_216.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_217.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_218.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_219.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_220.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_221.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_222.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_223.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_224.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_225.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_226.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_227.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_228.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_229.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_230.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_231.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_232.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_233.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_234.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_235.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_236.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_237.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_238.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_239.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_240.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_241.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_242.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_243.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_244.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_245.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_246.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_247.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_248.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_249.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_250.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_251.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_252.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_253.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_254.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_255.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_256.wav']\n"
     ]
    }
   ],
   "source": [
    "segmented_files = segment_audio_smart_v2(\n",
    "            wav_file_path,\n",
    "            output_folder_2,\n",
    "            target_chunk_duration=10,\n",
    "            max_chunk_duration=12,\n",
    "            min_chunk_duration=5,\n",
    "            silence_threshold=-40,\n",
    "            silence_padding=0.2 # Add a bit of silence after cut\n",
    "        )\n",
    "print(\"Segmented files:\", segmented_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated metadata saved as: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones/metadata_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clean_metadata(metadata_file):\n",
    "    \"\"\"\n",
    "    Removes '.wav' extension from filenames in metadata.txt.\n",
    "    \n",
    "    Args:\n",
    "        metadata_file (str): Path to metadata.txt\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to updated metadata file\n",
    "    \"\"\"\n",
    "    new_metadata_file = metadata_file.replace(\".txt\", \"_cleaned.txt\")\n",
    "    \n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove .wav extension from filenames\n",
    "    cleaned_lines = [line.replace(\".wav|\", \"|\", 1) for line in lines]\n",
    "\n",
    "    # Save new metadata file\n",
    "    with open(new_metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"✅ Updated metadata saved as: {new_metadata_file}\")\n",
    "    return new_metadata_file\n",
    "\n",
    "# Run the function\n",
    "metadata_path = output_folder+\"/metadata.txt\"  # Update with actual path\n",
    "cleaned_metadata_path = clean_metadata(metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Downloading DVAE files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.07k/1.07k [00:01<00:00, 708iB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'TTS.tts.datasets' has no attribute 'custom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m\n\u001b[1;32m     43\u001b[0m config_dataset \u001b[38;5;241m=\u001b[39m BaseDatasetConfig(\n\u001b[1;32m     44\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     45\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_xtts_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     language\u001b[38;5;241m=\u001b[39mLANGUAGE,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ✅ Load Training Samples\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_samples, eval_samples \u001b[38;5;241m=\u001b[39m \u001b[43mload_tts_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_split_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 5% for evaluation\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training samples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m eval samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ✅ Define Model Arguments (GPT-Based Fine-Tuning)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/datasets/__init__.py:118\u001b[0m, in \u001b[0;36mload_tts_samples\u001b[0;34m(datasets, eval_split, formatter, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# setup the right data processor\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m formatter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_get_formatter_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# load train set\u001b[39;00m\n\u001b[1;32m    120\u001b[0m meta_data_train \u001b[38;5;241m=\u001b[39m formatter(root_path, meta_file_train, ignored_speakers\u001b[38;5;241m=\u001b[39mignored_speakers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/datasets/__init__.py:166\u001b[0m, in \u001b[0;36m_get_formatter_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the respective preprocessing function.\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m thismodule \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mthismodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'TTS.tts.datasets' has no attribute 'custom'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# ✅ Define Paths\n",
    "dataset = os.path.join(os.getcwd(), \"dataset\")\n",
    "DATASET_PATH = os.path.join(os.getcwd(),\"audio_dataset\")  # Your dataset folder\n",
    "METADATA_FILE =os.path.join(os.getcwd(),\"audio_dataset/metadata.txt\")\n",
    "OUTPUT_PATH = \"/home/ahmet/tts_finetuned_models/xtts_v2_gpt\"  # Fine-tuned models output\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUTPUT_PATH, \"checkpoints\")  \n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "# ✅ Load XTTS-V2 Pretrained Model Checkpoint\n",
    "XTTS_CHECKPOINT = \"/home/ahmet/yourtts_model/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n",
    "\n",
    "# ✅ Define Speaker Reference (One high-quality voice sample)\n",
    "SPEAKER_REFERENCE = [\n",
    "    os.path.join(DATASET_PATH, \"wavs\", \"segment_001.wav\")\n",
    "]\n",
    "\n",
    "# ✅ Training Parameters\n",
    "BATCH_SIZE = 3  # Keep small to avoid memory issues\n",
    "GRAD_ACCUM_STEPS = 84  # Must be at least 252 when multiplied with batch size\n",
    "LEARNING_RATE = 5e-6  # Lower learning rate for stable fine-tuning\n",
    "LANGUAGE = \"de\"  # Adjust based on dataset\n",
    "\n",
    "# ✅ DVAE & XTTS Model Files (GPT-Based Fine-Tuning)\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, \"dvae.pth\")\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, \"mel_stats.pth\")\n",
    "\n",
    "# ✅ Download DVAE & XTTS Model Files If Not Available\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([\n",
    "        \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-V2/main/mel_stats.pth\",\n",
    "        \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-V2/main/dvae.pth\"\n",
    "    ], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "# ✅ Define Dataset Configuration\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"custom\",  \n",
    "    dataset_name=\"custom_xtts_dataset\",\n",
    "    path=DATASET_PATH,\n",
    "    meta_file_train=os.path.join(DATASET_PATH, METADATA_FILE),\n",
    "    language=LANGUAGE,\n",
    ")\n",
    "\n",
    "# ✅ Load Training Samples\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config_dataset,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.05,  # Use 5% for evaluation\n",
    ")\n",
    "\n",
    "print(f\"✅ Loaded {len(train_samples)} training samples and {len(eval_samples)} eval samples\")\n",
    "\n",
    "# ✅ Define Model Arguments (GPT-Based Fine-Tuning)\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=132300,  # 6 seconds\n",
    "    min_conditioning_length=66150,  # 3 seconds\n",
    "    debug_loading_failures=False,\n",
    "    max_wav_length=255995,  # ~11.6 seconds\n",
    "    max_text_length=200,\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  # XTTS-V2 checkpoint path\n",
    "    tokenizer_file=None,  # XTTS already has a tokenizer\n",
    "    gpt_num_audio_tokens=1026,\n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")\n",
    "\n",
    "# ✅ Define Audio Configuration\n",
    "audio_config = XttsAudioConfig(\n",
    "    sample_rate=22050,  \n",
    "    dvae_sample_rate=22050,  \n",
    "    output_sample_rate=24000\n",
    ")\n",
    "\n",
    "# ✅ Define Training Config (GPT-Based)\n",
    "config = GPTTrainerConfig(\n",
    "    output_path=OUTPUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=\"GPT_XTTS_v2.0_FT\",\n",
    "    project_name=\"XTTS_trainer\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,  \n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,  # Save model every 5000 steps\n",
    "    save_n_checkpoints=1,  \n",
    "    save_checkpoints=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=True,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=LEARNING_RATE,  # Learning rate\n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[\n",
    "        {\n",
    "            \"text\": \"This is an AI-powered voice cloning model.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Artificial intelligence is changing the world.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ✅ Initialize Model\n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# ✅ Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None,  # XTTS checkpoint is restored via `xtts_checkpoint`, so no need to restore here\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=True,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "\n",
    "# ✅ Start Training 🚀\n",
    "trainer.fit()\n",
    "\n",
    "print(f\"🎉 XTTS-V2 GPT-based fine-tuning completed! Model saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7489/1117580102.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model loaded successfully!\n",
      "🔥 Model Keys: dict_keys(['config', 'model', 'scaler', 'optimizer', 'step', 'date', 'model_loss'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "XTTS_CHECKPOINT = \"/home/ahmet/xtts_gpt_model/xtts_model.pth\"\n",
    "\n",
    "try:\n",
    "    model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
    "    print(\"✅ Model loaded successfully!\")\n",
    "    print(\"🔥 Model Keys:\", model_state.keys())  # 🔥 Print all keys in the checkpoint\n",
    "except Exception as e:\n",
    "    print(f\"❌ Model failed to load! Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195868/1349123997.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔥 Available Keys in XTTS Checkpoint: dict_keys(['config', 'model', 'scaler', 'optimizer', 'step', 'date', 'model_loss'])\n",
      "✅ Found `model` key instead of `net`. Trying to load...\n",
      "❌ XTTS Model failed to load! Error: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "XTTS_CHECKPOINT = \"/home/ahmet/xtts_gpt_model/xtts_model.pth\"\n",
    "try:\n",
    "    model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
    "    print(f\"🔥 Available Keys in XTTS Checkpoint: {model_state.keys()}\")\n",
    "\n",
    "    if \"model\" in model_state:\n",
    "        print(\"✅ Found `model` key instead of `net`. Trying to load...\")\n",
    "        model.load_state_dict(model_state[\"model\"], strict=False)\n",
    "        print(\"✅ XTTS Model loaded successfully!\")\n",
    "    else:\n",
    "        raise ValueError(\"❌ `model` key is missing in XTTS checkpoint.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ XTTS Model failed to load! Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "wav_folder = \"audio_dataset/wavs/\"\n",
    "\n",
    "for filename in os.listdir(wav_folder):\n",
    "    file_path = os.path.join(wav_folder, filename)\n",
    "    \n",
    "    # ✅ If file has no extension, add .wav\n",
    "    if \".\" not in filename:\n",
    "        new_file_path = file_path + \".wav\"\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f\"✅ Renamed: {filename} → {filename}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_time_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
