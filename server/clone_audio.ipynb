{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TTS.api import TTS\n",
    "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\", gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Hardcoded paths (update these based on actual locations)\n",
    "wav_file_path = os.path.join(os.getcwd(), \"temp_video/cf5f9419ae994200bf4af7472bc51abd.wav\")\n",
    "json_transcript_path = os.path.join(os.getcwd(), \"temp_video/subtitles/subtitles-original.json\")\n",
    "output_folder = os.path.join(os.getcwd(), \"segmented_wavs\")  # Make sure it exists\n",
    "dataset = os.path.join(os.getcwd(), \"dataset\")  # Make sure it exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "# ‚úÖ Paths\n",
    "wav_file_path = os.path.join(os.getcwd(), \"temp_video/cf5f9419ae994200bf4af7472bc51abd.wav\")\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Load full audio\n",
    "audio, sr = librosa.load(wav_file_path, sr=22050)  # üî• Ensure XTTS-compatible sample rate\n",
    "\n",
    "# ‚úÖ Split parameters\n",
    "chunk_duration = 9.6  # Split length in seconds\n",
    "samples_per_chunk = int(chunk_duration * sr)  # Convert seconds to samples\n",
    "\n",
    "# ‚úÖ Process and Save Chunks\n",
    "num_chunks = len(audio) // samples_per_chunk\n",
    "for i in range(num_chunks + 1):  # +1 to ensure we get the last segment if any\n",
    "    start = i * samples_per_chunk\n",
    "    end = min((i + 1) * samples_per_chunk, len(audio))\n",
    "    chunk_audio = audio[start:end]\n",
    "\n",
    "    if len(chunk_audio) > 1000:  # üî• Skip empty or tiny segments\n",
    "        chunk_filename = f\"segment_{i:03d}.wav\"\n",
    "        chunk_path = os.path.join(output_folder, chunk_filename)\n",
    "        sf.write(chunk_path, chunk_audio, sr)\n",
    "        print(f\"‚úÖ Saved {chunk_path}\")\n",
    "\n",
    "print(f\"üéâ Splitting complete! Segments saved in: {output_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transcribed: segment_000.wav ‚Üí Hallo und herzlich willkommen zur zweiten Folge von Einf√ºhrung in React mit dem Thema React Setup. Noch einmal kurz zu mir, mein Name ist David Losert.\n",
      "‚úÖ Transcribed: segment_001.wav ‚Üí Ich bin Software Engineer und seit √ºber zehn Jahren im Web unterwegs und arbeite nun auch bereits seit vier Jahren mit React. Neben React mag ich die Arbeit mit Chubbys.\n",
      "‚úÖ Transcribed: segment_002.wav ‚Üí TypeScript, Node.js, Linux-Servern, Docker und AWS. Die heutige Folge dreht sich also nun komplett darum, eine\n",
      "‚úÖ Transcribed: segment_003.wav ‚Üí Entwicklungsumgebung aufzusetzen und dort eine erste React Hello World Applikation zu implementieren. Wenn wir uns kurz erinnern, in der letzten Folge habe ich die Geschichte\n",
      "‚úÖ Transcribed: segment_004.wav ‚Üí und Prinzipien von React kurz vorgestellt und einen ersten theoretischen Einblick in den Virtual Dom und in JSX gegeben. Das habe ich an dieser Stelle auch einbezogen.\n",
      "‚úÖ Transcribed: segment_005.wav ‚Üí einmal kurz visualisiert. Wir erinnern uns, der Virtual DOM ist eine Abstraktion, die React verwendet, um den DOM zu synchronisieren.\n",
      "‚úÖ Transcribed: segment_006.wav ‚Üí Der Virtual Dong erlaubt uns zum einen das deklarative Programmieren und zum anderen gibt es uns einige Performance Vorteile.\n",
      "‚úÖ Transcribed: segment_007.wav ‚Üí In dieser Folge wollen wir nun eben also eine Entwicklungsumgebung aufsetzen. Ich nutze dazu Visual Studio Code. Wir werden uns ein erstes Video zeigen.\n",
      "‚úÖ Transcribed: segment_008.wav ‚Üí Toolset anschauen mit npm, nbx und Babel, was uns bei der Entwicklung von React-Applikationen hilft. Und wir werden nat√ºrlich eine erste React-Applikation implementieren und nutzen\n",
      "‚úÖ Transcribed: segment_009.wav ‚Üí dazu das React Element, ein atomarer Bildung Block von React und JSX. In dieser Stelle werdet ihr vielleicht kurz aufmerksam machen.\n",
      "‚úÖ Transcribed: segment_010.wav ‚Üí Ich habe das letzte Mal viel von React Components gesprochen. React Components sind nicht zu verwechseln mit React Element. Ich stelle nun aber in dieser Folge zu...\n",
      "‚úÖ Transcribed: segment_011.wav ‚Üí erst React Element vor, weil es sozusagen die Grundlage ist, oder der atomare Baustein, der tats√§chlich atomare Baustein von React.\n",
      "‚úÖ Transcribed: segment_012.wav ‚Üí nimmt uns aus GSX auch so ein wenig die Magie. Denn wenn man GSX das erste Ball sieht, kann man sich schnell fragen, wie funktioniert das eigentlich hinter den Kulissen?\n",
      "‚úÖ Transcribed: segment_013.wav ‚Üí Und Viect Element ist letztendlich das, was hinter den Kulissen steckt. Das werden wir am Ende der Folge dann auch einfach sehen. Bevor wir loslegen...\n",
      "‚úÖ Transcribed: segment_014.wav ‚Üí m√∂chte ich euch ermutigen, alle Code-Beispiele und praktischen Hands-on-Teile, die wir in dieser Folge machen, nachzuprogrammieren. Der praktische Einsatz ist\n",
      "‚úÖ Transcribed: segment_015.wav ‚Üí einfach der beste, um eine neue Technologie zu lernen. Das k√∂nnt ihr entweder machen, indem ihr nebenher programmiert und die Folge immer wieder pausiert oder aber\n",
      "‚úÖ Transcribed: segment_016.wav ‚Üí Ihr schaut euch die Folge einmal komplett an und programmiert das Beispiel im Nachhinein alleine. Wir werden den gesamten Code auch auf GitHub verwendet.\n",
      "‚úÖ Transcribed: segment_017.wav ‚Üí zur Verf√ºgung stellen. Das kann dann ein wenig als Orientierung dienen. Dabei geht einfach auf GitHub und sucht dort nach tech-lounge-sylject und ihr solltet das entsprechende Repost machen.\n",
      "‚úÖ Transcribed: segment_018.wav ‚Üí Tutorial finden. Das ist aktuell hier noch leer, weil ich den Code nat√ºrlich erst nach dieser Folge hochladen werde.\n",
      "‚úÖ Transcribed: segment_019.wav ‚Üí Und damit w√ºrde ich sagen, legen wir auch einfach schon mal los. Um unsere Umgebung vorzubereiten, m√ºssen ein paar Schritte unternommen werden.\n",
      "‚úÖ Transcribed: segment_020.wav ‚Üí mit React erstmal noch nichts zu tun. Zum einen m√ºsst ihr euch Visual Studio Code installieren, oder m√ºsst ihr nicht. Wenn ihr einen anderen Editor bevorzugt, ist das auch\n",
      "‚úÖ Transcribed: segment_021.wav ‚Üí vollkommen okay. Ich arbeite nur hier mit Visual Studio Code, weil ich diese Idee doch recht gerne habe. Sie bittet mir einige Unwahrheiten.\n",
      "‚úÖ Transcribed: segment_022.wav ‚Üí z.B. Codevervollst√§ndigung, was wir nachher auch sehen werden. Neben einer Entwicklungsumgebung Visual Studio Code brauchen wir Node.js und NPM.\n",
      "‚úÖ Transcribed: segment_023.wav ‚Üí Da erkl√§re ich auch gleich ein paar Worte dazu. Und wir m√ºssen nat√ºrlich einen Projektordner anlegen und unsere Umgebung vorbereiten mit ein paar wenigen Commands in der Kommandozeile.\n",
      "‚úÖ Transcribed: segment_024.wav ‚Üí Ein paar Worte zu NPM, falls ihr das noch nicht geh√∂rt habt. NPM ist ein Paketmanager.\n",
      "‚úÖ Transcribed: segment_025.wav ‚Üí f√ºr JavaScript Tools, Bibliotheken und Frameworks und erlaubt uns √ºber eine einfache Command Line Interface, das installieren und verwalten eben dieser Tools Bibliothek.\n",
      "‚úÖ Transcribed: segment_026.wav ‚Üí und Frameworks. Das ist ein einfacher Befehl wie npm install babel, den wir heute auch noch ausf√ºhren werden, der uns Abh√§ngigkeiten macht.\n",
      "‚úÖ Transcribed: segment_027.wav ‚Üí unser Projekt installiert. Und bei npm dreht sich eigentlich alles um die Package.json. Package.json ist eine Datei, die wir in unserem Projektordner im Bootfolder meistens\n",
      "‚úÖ Transcribed: segment_028.wav ‚Üí erstellen werden und in dieser Package JSON List sind Abh√§ngigkeiten beschrieben, Skripte und auch Projektmetadaten.\n",
      "‚úÖ Transcribed: segment_029.wav ‚Üí Tool, das npm mitbringt, ist npx oder np execute. Und das erm√∂glicht uns die Ausf√ºhrung all dieser JavaScript Tools biblik.\n",
      "‚úÖ Transcribed: segment_030.wav ‚Üí und frameworks ohne eine Installation. Und das k√∂nnen wir zum Beispiel nutzen, um einen lokalen Websever zu starten, der unser Testprojekt ausliefert.\n",
      "‚úÖ Transcribed: segment_031.wav ‚Üí werden wir auch tun und wir werden dazu den Web-Server Servo verwenden. Das ist kein Schreibfehler. Dieses Paket hei√üt wirklich so.\n",
      "‚úÖ Transcribed: segment_032.wav ‚Üí startet mit diesem Befehl einen lokalen Webserver im aktuellen Verzeichnis. Wir haben hier noch das Argument \"-reload\".\n",
      "‚úÖ Transcribed: segment_033.wav ‚Üí Das erlaubt uns oder das erlaubt dem Server alle Dateien, die in unserem Projekt sich tummeln, zu beobachten und bei einer √Ñnderung unseres\n",
      "‚úÖ Transcribed: segment_034.wav ‚Üí Browser automatisch neu zu laden. Das ist w√§hrend der Entwicklung sehr bequem, weil es uns das Neuladen der Seite h√§ndisch erspart, indem wir entweder F5 dr√ºcken oder hier √ºber den\n",
      "‚úÖ Transcribed: segment_035.wav ‚Üí Reload-Button die Seite neu laden. Wenn wir diesen Befehl per Default ausf√ºhren, liefert er eine Index-HTML, welche wir auch gleich erstellen werden.\n",
      "‚úÖ Transcribed: segment_036.wav ‚Üí im aktuellen Ordner unter der Adresse htdp localhost 8080 aus. Und nun um euch auch mal zu zeigen,\n",
      "‚úÖ Transcribed: segment_037.wav ‚Üí NPM Paket auf der Registry ausschaut. Hier unter npm.js.com habt ihr eine Suche, in der ihr alle Pakete, die es so gibt, suchen k√∂nnt.\n",
      "‚úÖ Transcribed: segment_038.wav ‚Üí F√ºr jedes Paket gibt es dann auch eine Seite mit einer Beschreibung und Installationsanweisungen. Alles, was man zu diesem Paket wissen muss.\n",
      "‚úÖ Transcribed: segment_039.wav ‚Üí Wollen wir das Ganze einfach mal ausf√ºhren? Dazu gehen wir also in unserer Command Line.\n",
      "‚úÖ Transcribed: segment_040.wav ‚Üí und legen uns erstmal einen Ordner an. Den nenne ich hier einfach mal Einf√ºhrung Reakt.\n",
      "‚úÖ Transcribed: segment_041.wav ‚Üí Jetzt sieht man, dass ich den zuvor schon angelegt habe, deswegen bringt er mir hier einen Error. Bei euch wird das dann funktionieren. Wir k√∂nnen einfach in diesem Ordner reinnamigieren und wir werden den Befehl √§ndern.\n",
      "‚úÖ Transcribed: segment_042.wav ‚Üí NPM Init ausf√ºhren. NPM Init erzeugt uns eben eine Package JSON, eine Initialen. Und das spart uns so ein wenig, das von Hand zu tun, indem es uns\n",
      "‚úÖ Transcribed: segment_043.wav ‚Üí √ºber die Kommandozeile ein paar Fragen stellt. Als allererstes will es den Package-Namen von uns wissen, den es per Default aus dem aktuellen Ordner einfach herauszieht. Einfach umgelegt ist in diesem Fall ok.\n",
      "‚úÖ Transcribed: segment_044.wav ‚Üí Die Version ist f√ºr uns jetzt auch erstmal ok. Wir erinnern uns kurz an die letzte Folge. Semper, Symantec Version, kommt bei NPM.\n",
      "‚úÖ Transcribed: segment_045.wav ‚Üí stark zum Einsatz. Eine Description, da k√∂nnen wir uns einfach irgendeinen Freitext √ºberlegen.\n",
      "‚úÖ Transcribed: segment_046.wav ‚Üí Das ist eine einfache Reaktion zum Beispiel. Der Entry Point werden wir nachher sehen. Das ist bei uns source.app.js. Test Command. Wir haben.\n",
      "‚úÖ Transcribed: segment_047.wav ‚Üí keine automatisierten Tests, deswegen lassen wir das leer. Wir haben auch noch kein Git Repository eingerichtet. Wir wollen auch erstmal keine Keywords vergeben. Den Author k√∂nnen wir uns selber reinschreiben.\n",
      "‚úÖ Transcribed: segment_048.wav ‚Üí Und eine Lizenz ist bei privaten Testprojekten auch eher nicht ganz so wichtig. Ich nehme hier in der Regel MIT. K√∂nnt ihr aber im Prinzip nicht.\n",
      "‚úÖ Transcribed: segment_049.wav ‚Üí auch auf ISC lassen. NPM fragt uns das nochmal, ob alle unsere Eingaben korrekt waren und wird uns eben diese JSON-Datei als Package JSON im aktuellen Moment zeigen.\n",
      "‚úÖ Transcribed: segment_050.wav ‚Üí ein Zeichniss anlegen. In unserem Fall ist das jetzt okay. Deswegen, yes. Als n√§chstes √∂ffnen wir nun diesen Ordner in unserer Entwicklungsumgebung.\n",
      "‚úÖ Transcribed: segment_051.wav ‚Üí In meinem Fall ist das eben Bisholz Studio Code. Das k√∂nnen wir ganz einfach machen, indem wir hier auf File Open gehen, zum entsprechenden Ordner navigieren und dann auf √ñffnen.\n",
      "‚úÖ Transcribed: segment_052.wav ‚Üí dr√ºcken. Und jetzt sehen wir, dass uns eben eine Package JSON generiert wurde, in der all die Feller, die wir vorher verwendet haben, verwendet haben.\n",
      "‚úÖ Transcribed: segment_053.wav ‚Üí Frage beantwortet haben, entsprechend eingetragen sind. Als n√§chsten Schritt wollen wir uns nun also noch eine Index-HTML anlegen.\n",
      "‚úÖ Transcribed: segment_054.wav ‚Üí initiale HTML-Seite, die ausgeliefert werden soll. Das machen wir √ºber new file index.html.\n",
      "‚úÖ Transcribed: segment_055.wav ‚Üí einfach eine standardm√§√üige HTML-Datei, angefangen mit dem DocType. Und hier haben wir jetzt schon ein tolles Feature von Visual Studio Code gesehen, die Autovervollst√§ndigung.\n",
      "‚úÖ Transcribed: segment_056.wav ‚Üí Ich werde das nochmal, indem ich also nur die ersten paar Zeichen des Codes eingebe, den ich hier erzeugen will, bietet mir Visual Studio Code schon eine\n",
      "‚úÖ Transcribed: segment_057.wav ‚Üí Vorauswahl an. Wenn ich diese best√§tige, entweder √ºber Enter oder indem ich draufklicke, f√ºllt es mir das entsprechend aus.\n",
      "‚úÖ Transcribed: segment_058.wav ‚Üí HTML. Wir brauchen einen Het oder einen Het-Tag, in welchem wir dann einen Titel vergeben k√∂nnen. Das nennen wir mal Einf√ºhrung in React.\n",
      "‚úÖ Transcribed: segment_059.wav ‚Üí Und wir brauchen einen Buddy. Und da wollen wir jetzt einfach, wie das so √ºblich ist, bei einem Code Beispiel mit HelloWorld anfangen.\n",
      "‚úÖ Transcribed: segment_060.wav ‚Üí Wenn ich nur noch Speichern dr√ºcke, nicht wundern, das ist auch ein Feature von Visual Studio Code, das ist mir automatisch mein Dokument nach gewissen\n",
      "‚úÖ Transcribed: segment_061.wav ‚Üí Kriterien formatiert. Dementsprechend kann es sein, dass hier manchmal ein Zeilenumbruch hinzugef√ºhrt wird. Das ist einfach sehr bequem, dass es immer alles einheitlich formatiert wird.\n",
      "‚úÖ Transcribed: segment_062.wav ‚Üí So, nun haben wir eine Index-HTML erstellt. Diese wollen wir jetzt nat√ºrlich noch √ºber einen Webserver ausliefern, um unsere Entwicklungsumgebung zu vervollst√§ndigen.\n",
      "‚úÖ Transcribed: segment_063.wav ‚Üí Und wie vorher besprochen, nehmen wir dazu npx, geben ein npx, servo, minus minus reload, enter.\n",
      "‚úÖ Transcribed: segment_064.wav ‚Üí Das dauert kurz und gibt uns dann hier auch entsprechend zur√ºck, dass nun unter adp localhost 8080 die aktuelle Web-Partei\n",
      "‚úÖ Transcribed: segment_065.wav ‚Üí Website oder der aktuelle Ordner ausgeliefert wird und standardm√§√üig eben diese Index HTML ausgeliefert wird. Und das sehen wir auch, indem wir zum Browser navigieren.\n",
      "‚úÖ Transcribed: segment_066.wav ‚Üí und entsprechend diese URL eingeben. Und siehe da, unsere Hello World Index HTML wird ausgeliefert. Eine kleine Unsch√∂nheit.\n",
      "‚úÖ Transcribed: segment_067.wav ‚Üí sehen wir hier noch Einf√ºhrung in React. Das √ú hat er irgendwie noch nicht erkannt. Dazu m√ºssen wir in dem HTML im Het-Teil noch\n",
      "‚úÖ Transcribed: segment_068.wav ‚Üí das Charset auf UTF-8 setzen. Geht auch ganz einfach in die Variante UTF-8\n",
      "‚úÖ Transcribed: segment_069.wav ‚Üí wir dr√ºcken speichern und wenn wir nun zur√ºck auf die website navigieren sehen wir dass das Reload Flag von Server schon seinen Job getan hat.\n",
      "‚úÖ Transcribed: segment_070.wav ‚Üí Die Seite wurde automatisch neu geladen und das √ú wird hier nun korrekt dargestellt. Und damit haben wir im Wesentlichen schon eine laufende Entwicklung.\n",
      "‚úÖ Transcribed: segment_071.wav ‚Üí Umgebung, in der wir nun React entwickeln k√∂nnen. Dazu nun wieder ein bisschen Theorie. Es gibt mehrere Methoden.\n",
      "‚úÖ Transcribed: segment_072.wav ‚Üí wir React nun in unser Projekt installieren k√∂nnen oder wie wir auch React aufsetzen k√∂nnen. Die einfachste Methode sind Online Playgrounds.\n",
      "‚úÖ Transcribed: segment_073.wav ‚Üí im Prinzip Entwicklungsumgebung direkt im Browser. React bietet selber auf seiner Seite Dokumentationen, einen Online-Project-Content an, zum Beispiel CodePen.\n",
      "‚úÖ Transcribed: segment_074.wav ‚Üí Das k√∂nnen wir uns auch mal ganz kurz anschauen. Wenn wir hier auf Get Started dr√ºcken, Try React, sehen wir hier die Online-Breakdowns. Wenn wir da zum Beispiel auf CodePen navigieren,\n",
      "‚úÖ Transcribed: segment_075.wav ‚Üí werden wir gleich weitergeleitet und haben nun hier ebenfalls eine Hello World Applikation von React.\n",
      "‚úÖ Transcribed: segment_076.wav ‚Üí Die wir entsprechend auch bearbeiten k√∂nnen, geben wir hier noch ein U ein, √ºber Command Enter wird die Seite nun aktualisiert.\n",
      "‚úÖ Transcribed: segment_077.wav ‚Üí wie gesagt super, um React auszuprobieren, um vielleicht auch mal, wenn irgendwas bei euch lokal nicht funktioniert, nachzustellen. Aber wenn wir langfristig\n",
      "‚úÖ Transcribed: segment_078.wav ‚Üí Projekte entwickeln, wollen wir nat√ºrlich irgendwie den Code, den wir produzieren, auch richtig abspeichern und das ist in diesen Online-Playgrounds eher schwierig m√∂glich.\n",
      "‚úÖ Transcribed: segment_079.wav ‚Üí Dementsprechend wollen wir den Code irgendwie lokal bei uns zur Verf√ºgung haben. Eine zweite Minute w√§re nat√ºrlich, den Quellcode von React runterzuladen.\n",
      "‚úÖ Transcribed: segment_080.wav ‚Üí k√∂nnten wir machen, ist heutzutage aber eher altmutig. Ist auch sehr aufwendig, wenn wir zum Beispiel eine neue Version von Vect installieren wollen.\n",
      "‚úÖ Transcribed: segment_081.wav ‚Üí m√ºssten wir wieder auf die React-Webseite den Code herunterladen. Also das ist ein Vorgehen, welches eher heutzutage nicht mehr zu empfehlen ist. Da gibt es h√∂here Methoden.\n",
      "‚úÖ Transcribed: segment_082.wav ‚Üí Eine ist die Nutzung einer sogenannten CDN URL. CDN steht f√ºr Content Delivery Network. Das ist letztendlich einfach eine Website, auf der React uns\n",
      "‚úÖ Transcribed: segment_083.wav ‚Üí den Quellcode schon einmal hochgeladen hat und den wir √ºber die URL ganz einfach in unserer index.html einbinden k√∂nnen. Und das ist ideal f√ºr schnelle Tests oder auch f√ºr Lernprozesse.\n",
      "‚úÖ Transcribed: segment_084.wav ‚Üí so wie dieses Jahr hier ein Lernprojekt ist. Und dementsprechend werden wir auch heute auf diese Variante der Installation von Reaktor greifen.\n",
      "‚úÖ Transcribed: segment_085.wav ‚Üí Die vierte Variante, die es auch noch gibt, ist nat√ºrlich die Installation mit NPM. Denn React gibt es auch auf NPM.\n",
      "‚úÖ Transcribed: segment_086.wav ‚Üí Stelle. √úgen nicht. Project wurde auch auf NPM hochgeladen.\n",
      "‚úÖ Transcribed: segment_087.wav ‚Üí dass die Installation √ºber NPM bzw. danach die Einbindung in eure Website nicht ganz so leicht von da angeht. Da braucht es dann doch\n",
      "‚úÖ Transcribed: segment_088.wav ‚Üí das ein oder andere Tool, was die Sache initial sehr komplex macht. Das rentiert sich bei gr√∂√üeren\n",
      "‚úÖ Transcribed: segment_089.wav ‚Üí komplexer Aufwand wird dann amortisiert √ºber viel Arbeit, die uns diese Variante sp√§ter erspart. Die Installation von React mit NPM\n",
      "‚úÖ Transcribed: segment_090.wav ‚Üí werden wir in einer sp√§teren Folge dann auch noch vornehmen, sp√§testens im zweiten Teil dieser Videoserie, wo sich ja alles um das Tooling drehen wird. F√ºr heute wie gesagt\n",
      "‚úÖ Transcribed: segment_091.wav ‚Üí nutzen wir aber den CDL URL Link. Das Ganze sieht dann so aus, dass wir in unserer index.html diese zwei Script Tags einstellen.\n",
      "‚úÖ Transcribed: segment_092.wav ‚Üí f√ºhren und damit ist React dann auf unserer Seite schon verf√ºgbar. Diese zwei Script Tags findet ihr ebenfalls auf\n",
      "‚úÖ Transcribed: segment_093.wav ‚Üí reactjs.org auf der offiziellen Seite unter CDN links. Ihr werdet hier sehen, dass es zwei unterschiedliche Varianten gibt, die Projekt einzubinden.\n",
      "‚úÖ Transcribed: segment_094.wav ‚Üí einmal im Development Modus und einmal im Production Modus. Der Unterschied ist ganz einfach, dass der Development Modus einige bessere Fehler meldet.\n",
      "‚úÖ Transcribed: segment_095.wav ‚Üí zur Verf√ºgung stellt und einige Tools uns zur Verf√ºgung stellt, die das Entwickeln von React leichter machen, die aber gleichzeitig diese JavaScript-Dateien sehr aufpl√§nen und sehr gro√ü machen.\n",
      "‚úÖ Transcribed: segment_096.wav ‚Üí Im produktiven Einsatz von React wollen wir nat√ºrlich so wenig wie m√∂glich Code auf der Seite, weil der ja auch heruntergeladen werden muss. Dementsprechend hier diese Unterscheidung.\n",
      "‚úÖ Transcribed: segment_097.wav ‚Üí jetzt aber noch weit weg sind von einem produktiven Einsatz, beschr√§nken wir uns heute auf die Development-Sourcen, die ich hier einfach schon mal kopiere.\n",
      "‚úÖ Transcribed: segment_098.wav ‚Üí Wenn wir dann nachher die Reakt-Sourcen in unserem Projekt eingebunden haben, wollen wir Reakt nat√ºrlich auch verwenden.\n",
      "‚úÖ Transcribed: segment_099.wav ‚Üí Der ist den ersten Code oder das erste, den ersten Block mit React, den wir verwenden werden, ist eben, wie vorher angek√ºndigt schon, React Element. React Element\n",
      "‚úÖ Transcribed: segment_100.wav ‚Üí oder ein Reakt-Element erstellen wir √ºber diesen Funktionsauffruch ReaktCreateElement und der besteht aus drei Parametern. Der erste Parameter gibt das HTML.\n",
      "‚úÖ Transcribed: segment_101.wav ‚Üí html-Tag an, das wir generieren wollen. In diesem Fall zum Beispiel ein p-Tag. Der zweite Parameter gibt an, welche Attribute wir diesem html-Tag mitgeben wollen.\n",
      "‚úÖ Transcribed: segment_102.wav ‚Üí und der dritte Parameter gibt an, welchen Inhalt wir in dieses HTML-Tech reinschreiben wollen.\n",
      "‚úÖ Transcribed: segment_103.wav ‚Üí Dieser Funktionsaufruf macht letztendlich nichts anderes als ein simples JavaScript-Objekt zu erzeugen.\n",
      "‚úÖ Transcribed: segment_104.wav ‚Üí JavaScript-Objekt wird aber von React verstanden und kann dann von React wiederum in den Virtual DOM von React implementiert werden oder eingebunden werden.\n",
      "‚úÖ Transcribed: segment_105.wav ‚Üí Sobald es im Virtual Dom eingebunden ist, wir erinnern uns, React synchronisiert den Virtual Dom damit im richtigen Dom, wird eben dieses Element zu einem realen Dom.\n",
      "‚úÖ Transcribed: segment_106.wav ‚Üí Dom-Objekt und dementsprechend im Browser dargestellt. Man kann also sagen, dass ein Reakt-Element ein reales Dom-Objekt repr√§sentiert.\n",
      "‚úÖ Transcribed: segment_107.wav ‚Üí Wenn wir uns das Ganze auf der Visualisierung noch einmal anschauen, haben wir nun also neben dem Virtual Dumpf Reakt in unseren JavaScript-Dateien diesen Funktionsaufruf.\n",
      "‚úÖ Transcribed: segment_108.wav ‚Üí React-Create-Element, welcher dann eben hier im Virtual Dom als einzelnen Element eingebunden wird, dann mit dem Dom synchronisiert und nachher in unserem Browser dargestellt.\n",
      "‚úÖ Transcribed: segment_109.wav ‚Üí Dieser Visualisierung habe ich ebenfalls npm hinzugef√ºgt, welches eben √ºber die Package Chasing, die wir gerade schon gesehen haben, initiiert wird.\n",
      "‚úÖ Transcribed: segment_110.wav ‚Üí und installiert wird. Und wir haben NPX kennengelernt, was uns den Webseller startet. Nun wollen wir das Ganze noch einmal.\n",
      "‚úÖ Transcribed: segment_111.wav ‚Üí eben umsetzen. Wir kopieren nochmal kurz die Sources, die brauchen wir jetzt als allererstes. Gehen zur√ºck in unser Visual Studio.\n",
      "‚úÖ Transcribed: segment_112.wav ‚Üí und f√ºgen nun React ganz einfach am Ende des BodyTags ein. An sich war es das schon.\n",
      "‚úÖ Transcribed: segment_113.wav ‚Üí Jetzt haben wir Reakt auf unserer Seite. Wir tun aber nat√ºrlich noch nichts damit. Um noch etwas damit zu tun, brauchen wir noch etwas eigenes JavaScript, das wir einbinden.\n",
      "‚úÖ Transcribed: segment_114.wav ‚Üí Dementsprechend f√ºgen wir nun erstmal hier noch ein zweites oder ein drittes Script Tag hinzu. Das bei uns auf.\n",
      "‚úÖ Transcribed: segment_115.wav ‚Üí sourceapp.js zeigen wird. Diese Totei legen wir dann noch an. Erstmal ein Ordner source.\n",
      "‚úÖ Transcribed: segment_116.wav ‚Üí Das ist einfach ein g√§ngiges Vorgehen. SRC steht eben f√ºr Source, in dem alle Source-Dateien eines Projektes abgelegt werden. Dort legen wir dann die Datei an.\n",
      "‚úÖ Transcribed: segment_117.wav ‚Üí beobachten. Bevor wir nun JavaScript schreiben, bereiten wir in der Index.html noch eine weitere Sache vor. Wir erinnern uns\n",
      "‚úÖ Transcribed: segment_118.wav ‚Üí vielleicht das letzte Mal an die React DOM Render Funktion, der wir zum einen mitgeben, eine React Komponente, die wir gerendert haben wollen, zum anderen aber auch angeben m√ºssen, wohin in unserer\n",
      "‚úÖ Transcribed: segment_119.wav ‚Üí HTML wir diese Komponente gel√ºndert haben wollen. Und dieses wohin erstellen wir jetzt hier. Wir machen uns ein einfaches div und geben dem eine id.\n",
      "‚úÖ Transcribed: segment_120.wav ‚Üí anhand der wir es dann sp√§ter identifizieren k√∂nnen und das nehmen wir dann einfach React.\n",
      "‚úÖ Transcribed: segment_121.wav ‚Üí Wenn wir nochmal kurz auf die Seite schauen, es hat sich nichts ver√§ndert. Alles beim alten.\n",
      "‚úÖ Transcribed: segment_122.wav ‚Üí noch kein Vierktelement generiert haben, das wir hier einbinden. Das tun wir jetzt. Zuerst generieren wir uns also ein Vierktelement.\n",
      "‚úÖ Transcribed: segment_123.wav ‚Üí Das speichern wir in eine Variable, in die wir jetzt einfach mal Element nennen, nutzen nun eben Reakt.Create.\n",
      "‚úÖ Transcribed: segment_124.wav ‚Üí Element. React ist eine globale Variable, die uns jetzt eben zur Verf√ºgung steht, weil wir hier diese √ºber dieses Script eingebunden haben.\n",
      "‚úÖ Transcribed: segment_125.wav ‚Üí macht letztendlich nichts anderes als die Reaktionshosen unter dieser Variablen zur Verf√ºgung zu stellen und so auch die CreateElement Methode. Als ersten Parameter eben das\n",
      "‚úÖ Transcribed: segment_126.wav ‚Üí Tag, in dem Fall bleiben wir immer beim P tag. Als zweiter Parameter die Attribute. Das lassen wir erstmal leer.\n",
      "‚úÖ Transcribed: segment_127.wav ‚Üí Dritten Parameter den Content. Hier schreiben wir jetzt einfach rein, das ist mein erstes React.\n",
      "‚úÖ Transcribed: segment_128.wav ‚Üí Element. Richtiger. Wie nur angesprochen, React-Trade Element generiert erst einmal nur ein ...\n",
      "‚úÖ Transcribed: segment_129.wav ‚Üí Simples JavaScript-Objekt. Mit diesem Objekt m√ºssen wir jetzt noch etwas tun. Wir m√ºssen ein React zeigen, wohin es uns dieses JavaScript-Objekt oder dieses React-Element...\n",
      "‚úÖ Transcribed: segment_130.wav ‚Üí nachher im Browser rendern soll. Und dazu nutzen wir react-dom. Das ist ebenfalls eine globale Variable, die uns zur Verf√ºgung steht.\n",
      "‚úÖ Transcribed: segment_131.wav ‚Üí wir hier dieses zweite Skript eingebunden haben f√ºr React Dom. Das liefert uns eben die Wenderfunktion.\n",
      "‚úÖ Transcribed: segment_132.wav ‚Üí Wir m√ºssen erst unser Element reingeben. High Element. Und nun sagen m√ºssen, wohin wir das gel√§ndert haben wollen. Und hierzu haben wir uns ja vorher das Diff angelegt. Und eine Edifier-Partei.\n",
      "‚úÖ Transcribed: segment_133.wav ‚Üí und das k√∂nnen wir hier nun mit einem Selektor herausfinden oder herausfiltern, indem wir sagen document getElementBy\n",
      "‚úÖ Transcribed: segment_134.wav ‚Üí Und hier die ID, die wir vorher vergeben haben, in dem Fall React App, kopieren und einf√ºgen.\n",
      "‚úÖ Transcribed: segment_135.wav ‚Üí Wenn wir das ganze speichern und auf die Website gehen, dann sehen wir, juhu, wir haben unsere erste Reakt Komponente.\n",
      "‚úÖ Transcribed: segment_136.wav ‚Üí unser erstes React Element auf der Website gerendert. Wir haben zum allerersten Mal React eingesetzt, um unseren Dom zu manipulieren. Das ist nat√ºrlich jetzt noch etwas...\n",
      "‚úÖ Transcribed: segment_137.wav ‚Üí statisch, aber ist doch immerhin schon mal ein toller erster Schritt. Jetzt werdet ihr euch fragen, ich habe das letzte mal erz√§hlt.\n",
      "‚úÖ Transcribed: segment_138.wav ‚Üí von Deklarativen programmieren und von JSX, dass es uns erlaubt, HTML in JavaScript-Dateien zu schreiben. Ihr w√ºrdet nun erwarten, dass wir...\n",
      "‚úÖ Transcribed: segment_139.wav ‚Üí eigentlich das ganze nicht hier mit dem Reactory Element Aufruf machen, sondern eher sowas schreiben k√∂nnen wie so also ein HTML Element\n",
      "‚úÖ Transcribed: segment_140.wav ‚Üí mit der ID direkt in der JavaScript-Datei.\n",
      "‚úÖ Transcribed: segment_141.wav ‚Üí Wenn wir das machen, werden wir einen Fehler bekommen. Wenn wir zur√ºck navigieren in den Browser, sehen wir zum einen\n",
      "‚úÖ Transcribed: segment_142.wav ‚Üí dass unser Element nicht mehr gerendert wurde. Um herauszufinden, was hier gerade schiefgelaufen ist, k√∂nnen wir die Entwicklertools von Chrome oder ...\n",
      "‚úÖ Transcribed: segment_143.wav ‚Üí von jedem anderen Browser, jeder Browser bringt Entwicklertools mit, uns zur Hand nehmen. Das k√∂nnen wir entweder √ºber die Taste F12 machen oder indem wir rechtsklick untersuchen machen.\n",
      "‚úÖ Transcribed: segment_144.wav ‚Üí In diesen Untersuchen sehen wir dann unseren aktuellen DOM oder unser HTML, das wir auch hier durch navigieren k√∂nnen. Unser Title, unser Meta-Characters.\n",
      "‚úÖ Transcribed: segment_145.wav ‚Üí eben der Buddy. Wenn ich es auch schaffe, da mal hinzukicken. So, perfekt. Und wir haben auch die Konsolen.\n",
      "‚úÖ Transcribed: segment_146.wav ‚Üí die uns Fehler ausspuckt. Und in diesem Fall sehen wir jetzt, wir haben einen S√ºndungsfehler, ein unerwartetes Zeichen, Unexpected Token, eine ge√∂ffnete Klammer.\n",
      "‚úÖ Transcribed: segment_147.wav ‚Üí Und das ist genau dieses Zeichen. Blick daran, dass ein Browser heutzutage zumindest noch nicht JSX versteht.\n",
      "‚úÖ Transcribed: segment_148.wav ‚Üí natives JavaScript und das hier ist nun mal kein natives JavaScript. Dementsprechend kann der Browser diesen Code-Schnippel einfach auch nicht verstehen.\n",
      "‚úÖ Transcribed: segment_149.wav ‚Üí Und wir m√ºssen erst etwas tun, damit er das kann. Wir m√ºssen n√§mlich diesen Code Snippet zur√ºck umwandeln in ganz normales JavaScript. Und an dieser Stelle schon ein kleiner Hint.\n",
      "‚úÖ Transcribed: segment_150.wav ‚Üí React Create Element ist ganz normales JavaScript. Das haben wir gerade gesehen, dass das funktioniert. Um das nun zu erm√∂glichen und um diese √úbersetzung vorzunehmen,\n",
      "‚úÖ Transcribed: segment_151.wav ‚Üí Das m√ºssen wir nat√ºrlich nicht h√§ndisch machen, das w√§re sehr aufwendig, sondern da gibt es Tools. Und eines dieser Tools, wir gehen zur√ºck zur Theorie,\n",
      "‚úÖ Transcribed: segment_152.wav ‚Üí Babel. Babel ist eben so eine Art Helferin oder eine √úbersetzerin, die uns Features und Funktionen sowie GSX zur Verf√ºgung stellt.\n",
      "‚úÖ Transcribed: segment_153.wav ‚Üí und zur√ºck √ºbersetzt in natives JavaScript, dass der Browser versteht. Babel besteht im Wesentlichen aus drei Kernkomponenten, die wir auch heute einsetzen werden.\n",
      "‚úÖ Transcribed: segment_154.wav ‚Üí Das ist einmal der Core. Das ist die gesamte √úbersetzungslogik. Das Command Line Interface, oder kurz CLI, das l√§sst uns mit Babel kommunizieren und sprechen.\n",
      "‚úÖ Transcribed: segment_155.wav ‚Üí und Babel mitteilen, was wir eigentlich √ºbersetzt haben wollen. Und es gibt Presets. Presets k√∂nnt ihr euch vorstellen als W√∂rterb√ºcher, die wir Babel mitgeben, um Babel zu ermitteln.\n",
      "‚úÖ Transcribed: segment_156.wav ‚Üí beispielsweise GSX, den normalen JavaScript zu √ºbersetzen. Und wir setzen heute Preset React ein. Preset React ist eben genau dieses W√∂rterbuch, das den Einsatz\n",
      "‚úÖ Transcribed: segment_157.wav ‚Üí JSX in unseren Dateien erlaubt und Babel wird dieses JSX dann umwandeln in ganz normales JavaScript.\n",
      "‚úÖ Transcribed: segment_158.wav ‚Üí aktuell standen wir hier, wir haben mit JavaScript React und Create Element ausgef√ºhrt, das hat funktioniert, haben jetzt aber JSX mit reingebracht.\n",
      "‚úÖ Transcribed: segment_159.wav ‚Üí mehr funktioniert. JSX m√ºssen wir also nun erst durch Babel schleifen, um das in normales JavaScript umzuwandeln, das dann wiederum von React bzw. von unseren Browsern aufwacht.\n",
      "‚úÖ Transcribed: segment_160.wav ‚Üí verstanden werden kann. Und an dieser Stelle wird es vielleicht schon langsam bewusst, Babel macht nichts anderes als aus diesen JSXs\n",
      "‚úÖ Transcribed: segment_161.wav ‚Üí oder aus den HTML-Teilen in JavaScript, die wir in JSX schreiben, react.createElement-Funktionsaufrufe zu machen.\n",
      "‚úÖ Transcribed: segment_162.wav ‚Üí Das ist die ganze Magie, die dahinter steckt und das zeige ich euch auch gleich hands-on. Aber zuerst, wie installieren wir Babel?\n",
      "‚úÖ Transcribed: segment_163.wav ‚Üí Dazu nutzen wir nun eben NPM und wir werden den Babel Core, die Babel CLI und das Preset React in unserem Projekt installieren.\n",
      "‚úÖ Transcribed: segment_164.wav ‚Üí machen wir jetzt. Zur√ºck also in die Command Line. Wir beenden mal kurz unseren Web Server an dieser Stelle und f√ºhren nun das Command aus.\n",
      "‚úÖ Transcribed: segment_165.wav ‚Üí install add-barbel-core, add-barbel-cli und add-barbel-reset\n",
      "‚úÖ Transcribed: segment_166.wav ‚Üí Wir f√ºgen hier noch einen kleinen weiteren Parameter hinzu, n√§mlich \"-save-def\". Warum wir das tun, erkl√§re ich dann gleich.\n",
      "‚úÖ Transcribed: segment_167.wav ‚Üí Das dauert ein wenig, weil er die ganzen Pakete nat√ºrlich erst runterladen muss.\n",
      "‚úÖ Transcribed: segment_168.wav ‚Üí Jetzt seht ihr hier ein Error, das passiert auf Mac. Nicht wundern, wenn ihr diesen Error seht und auch keine Sorge, das spielt f√ºr uns erstmal keine Rolle.\n",
      "‚úÖ Transcribed: segment_169.wav ‚Üí Die Features von Babel werden wegen dieses Errors nicht funktionieren, das sind aber keine Features, die wir jetzt gerade ben√∂tigen. Von dem her k√∂nnen wir diesen Error einfach getrost ignorieren.\n",
      "‚úÖ Transcribed: segment_170.wav ‚Üí ist abgeschlossen. Gehen wir zur√ºck in Visual Studio Code. Schauen wir mal kurz, hier hat sich etwas ver√§ndert. Zum einen wurden in unserer Package Chasen\n",
      "‚úÖ Transcribed: segment_171.wav ‚Üí ein neues Feld hinzugef√ºgt, n√§mlich diese Dev-Dependencies. Das ist das, was ich mit dem Befehl \"-save-dev veranlasst habe und das schreibt letztendlich\n",
      "‚úÖ Transcribed: segment_172.wav ‚Üí einfach alle Abh√§ngigkeiten unseres Projekts oder alle Entwicklungsabh√§ngigkeiten hier in diese Package chasen. Ich will jetzt nicht zu sehr ins Detail gehen, weil das\n",
      "‚úÖ Transcribed: segment_173.wav ‚Üí Das ist ja ein Kurs √ºber React und nicht √ºber NPM, aber einfach, dass ihr versteht, warum ich diesen Befehl angegeben habe. Es ist auch eine Package-Log-Json generiert worden. Das ist sozusagen ein Log-Json.\n",
      "‚úÖ Transcribed: segment_174.wav ‚Üí aller derer Pakete, die wir installiert haben mit genauen Funktionen. Und zu guter Letzt wurde hier Node Modules angelegt. Das ist der Ordner, in dem die Pakete, die wir installiert haben,\n",
      "‚úÖ Transcribed: segment_175.wav ‚Üí haben tats√§chlich heruntergeladen wurden. Wenn wir das mal kurz aufmachen, sehen wir hier nat√ºrlich deutlich mehr als wir installiert haben. Das liegt einfach da.\n",
      "‚úÖ Transcribed: segment_176.wav ‚Üí dass Babel selbst ja auch Abh√§ngigkeiten hat, die es wiederum √ºber eine Package-JSON definiert und alle diese Abh√§ngigkeiten installiert Babel oder NPM f√ºr uns dann gleich mit.\n",
      "‚úÖ Transcribed: segment_177.wav ‚Üí dass wir das nicht h√§ndisch machen m√ºssen. Aber wir sehen auch, AdBabel CLI wurde installiert, AdBabel Core wurde installiert und weiter unten.\n",
      "‚úÖ Transcribed: segment_178.wav ‚Üí etwa mit Preset Reakt ist dann auch in unserem Projekt vorhanden. Nun m√ºssen wir\n",
      "‚úÖ Transcribed: segment_179.wav ‚Üí Babel noch entsprechend ein wenig konfigurieren bzw. auch ausf√ºhren. Denn aktuell w√ºsste Babel ja noch nicht, was es tun soll und Babel wei√ü aktuell auch noch nicht.\n",
      "‚úÖ Transcribed: segment_180.wav ‚Üí welche W√∂rterb√ºcher es einsetzen soll. Fangen wir mit dem W√∂rterbuch an. Wir k√∂nnen Babel ganz einfach konfigurieren, indem wir nochmal eine neue Datei anlegen, die sich Punkt Babel\n",
      "‚úÖ Transcribed: segment_181.wav ‚Üí RC nennt. In dieser Datei k√∂nnen wir valides Chasern schreiben. Also das ist eigentlich nur eine Chasen-Datei, auch wenn sie nicht auf Chasen-M sind.\n",
      "‚úÖ Transcribed: segment_182.wav ‚Üí Wir k√∂nnen dort ein Objekt anlegen und das Feld Presets bef√ºllen und dort nun in einem Array alle die Presets reinschreiben, die ...\n",
      "‚úÖ Transcribed: segment_183.wav ‚Üí Babel f√ºr uns verwenden, also sprich alle diese W√∂rterb√ºcher, die wir √ºbersetzen wollen. In unserem Fall ist das nur eines, n√§mlich Babel preset react.\n",
      "‚úÖ Transcribed: segment_184.wav ‚Üí Das ist der erste Schritt. Im zweiten Schritt m√ºssen wir nun aus dieser App.js eine App\n",
      "‚úÖ Transcribed: segment_185.wav ‚Üí JSX-Datei machen. Denn wir haben hier ja tats√§chlich nicht mehr verlieh das JavaScript stehen, deswegen ist das auch keine JavaScript-Datei mehr, sondern es ist jetzt eine JSX-Datei.\n",
      "‚úÖ Transcribed: segment_186.wav ‚Üí wir hier JSX eingef√ºgt haben. Und nun m√ºssen wir Babel diese Datei f√ºr uns √ºbersetzen lassen. Das k√∂nnen wir nun eben mit Babel CLI √ºber die Command-Karte\n",
      "‚úÖ Transcribed: segment_187.wav ‚Üí Es gibt nun zwei M√∂glichkeiten.\n",
      "‚úÖ Transcribed: segment_188.wav ‚Üí einsetzen, um Babel direkt auszuf√ºhren. Wir haben aber die Babel CLI auch bei uns installiert. Das Ganze nehme ich in Node Modules.\n",
      "‚úÖ Transcribed: segment_189.wav ‚Üí bin oder binary folder gibt es den Befehl babel. Dieser Befehl erwartet drei Parameter. Zum einen\n",
      "‚úÖ Transcribed: segment_190.wav ‚Üí das Surs-Verzeichnis oder das Verzeichnis, das Babel f√ºr uns √ºbersetzen soll. In unserem Fall ist das tats√§chlich Surs. Es erwartet den Parameter minus minus out hier.\n",
      "‚úÖ Transcribed: segment_191.wav ‚Üí wo Babel die kombinierten oder die √ºbersetzten Dateien hinschreiben soll. Und das machen wir bei uns jetzt einfach mal in ein Verzeichnis, das wir lib nennen.\n",
      "‚úÖ Transcribed: segment_192.wav ‚Üí Wenn wir mit diesen Befehlen nun ausf√ºhren, quittiert uns Babel das mit einer erfolgreichen Meldung, dass es uns eine Datei erfolgreich kompiliert hat.\n",
      "‚úÖ Transcribed: segment_193.wav ‚Üí oder √ºbersetzt hat. Wenn wir zur√ºck ins Projektverzeichnis springen, sehen wir auch, dass der Lipfolder angelegt wurde und hier entsprechend analog zu unserer App JSX.\n",
      "‚úÖ Transcribed: segment_194.wav ‚Üí eine App.js-Datei angelegt wurde. Und die k√∂nnen wir uns auch anschauen. Und siehe da. Aus unserem JSX ist nichts anderes.\n",
      "‚úÖ Transcribed: segment_195.wav ‚Üí geworden als ein React-Create-Element-Aufruf, der relativ oder nicht nur relativ, sondern ziemlich gleich aussieht, wie das, was wir davor h√§ndisch eingegeben haben.\n",
      "‚úÖ Transcribed: segment_196.wav ‚Üí Das ist tats√§chlich die gesamte Magie hinter JSX und Babel. Das macht nichts anderes, als die Teile, die HTML und unserem JSX sind, in ein React.CreateElement zu √ºbersetzen.\n",
      "‚úÖ Transcribed: segment_197.wav ‚Üí Nun m√ºssen wir, damit das auch funktioniert, noch eine kleine √Ñnderung in unserer Index-HTML vornehmen, weil wir jetzt\n",
      "‚úÖ Transcribed: segment_198.wav ‚Üí nicht mehr die App.js aus unseren Source ausliefern oder die App.js X, sondern wir m√ºssen ja die √ºbersetzte Variante unserer App ausliefern. Dementsprechend erinnern wir...\n",
      "‚úÖ Transcribed: segment_199.wav ‚Üí das hier einfach auf LIP. Und wenn wir nun nochmal einen LAP-Server starten.\n",
      "‚úÖ Transcribed: segment_200.wav ‚Üí und zur√ºck auf unsere Website navigieren. Sehen wir, wir haben ein erstes, ups, nein, wir haben noch das Alchemy Element.\n",
      "‚úÖ Transcribed: segment_201.wav ‚Üí wei√ü auch warum. Weil wir hier noch das alte Element auch eingebunden haben. Wir wollen jetzt aber eigentlich hier das die erste ChessX Komponenta einbinden. Dementsprechend\n",
      "‚úÖ Transcribed: segment_202.wav ‚Üí m√ºssen wir das hier ersetzen. Wir wollen nun My.js.x-Element rendern. Wir m√ºssen das ganze jetzt nat√ºrlich von Babel nochmal neu √ºberwinden.\n",
      "‚úÖ Transcribed: segment_203.wav ‚Üí setzen lassen. Wenn wir jetzt den Webserver starten sollten wir tats√§chlich sehen, dass wir unsere erste CharsX Component\n",
      "‚úÖ Transcribed: segment_204.wav ‚Üí erfolgreich im Browser gewendet haben. Das ist super!\n",
      "‚úÖ Transcribed: segment_205.wav ‚Üí Zum Abschluss kommen m√∂chte ich euch noch einen kleinen Tipp mitgeben, denn diesen Befehl, den wir hier gerade gesehen haben Not Modules Bin Babels Source\n",
      "‚úÖ Transcribed: segment_206.wav ‚Üí ist schwer zu merken und ist auch nicht sonderlich sch√∂n einzutippen. Da k√∂nnen wir uns mit npm und npm scripts ein wenig Abfehlung verschaffen.\n",
      "‚úÖ Transcribed: segment_207.wav ‚Üí Erlaubt uns es, n√§mlich Skripte unter einem sogenannten Alias oder einem anderen Namen vorzudefinieren. Wenn wir jetzt also hier sowas wie compile schreiben.\n",
      "‚úÖ Transcribed: segment_208.wav ‚Üí Dort den Befehl, WABEL, SUS, minus minus OUT, DIR, LIP, gefolgt von einem Komma reinschreiben.\n",
      "‚úÖ Transcribed: segment_209.wav ‚Üí k√∂nnen wir diesen sehr aufwendigen command line ausf√ºhren. ganz kurz wir k√∂nnen uns hier das noten module spin sparen, weil\n",
      "‚úÖ Transcribed: segment_210.wav ‚Üí die Scripts von npm per Default dieses Binary-Verzeichnis einbinden. Das hei√üt, hier k√∂nnen wir auf diesen ganzen vorherigen Pfad verzichten und einfach Babel direkt ausf√ºhren.\n",
      "‚úÖ Transcribed: segment_211.wav ‚Üí Und nun k√∂nnen wir einfach √ºber den Befehl in unserem aktuellen Verzeichnis npm run jedes Script, das wir definiert haben, ausf√ºhren.\n",
      "‚úÖ Transcribed: segment_212.wav ‚Üí und wir sehen, wurde Babel wieder ausgef√ºhrt, um uns unsere JSX-Dateien zu √ºbersetzen.\n",
      "‚úÖ Transcribed: segment_213.wav ‚Üí Letztendlich kann hier in scripts jedes valide CLI-Command eingef√ºgt werden. Wir k√∂nnen so die f√ºr unser Projekt relevanten\n",
      "‚úÖ Transcribed: segment_214.wav ‚Üí die CLE-Commands sehr, sehr einfach und sehr, sehr √ºbersichtlich in der Package-Chase zu pflegen. Das ist ein kleiner Trick, der in vielen Projekten auch sehr massiv eingesetzt wird.\n",
      "‚úÖ Transcribed: segment_215.wav ‚Üí Das k√∂nnen wir zum Beispiel als letztes noch erweitern, indem wir hier NPX Server eintragen.\n",
      "‚úÖ Transcribed: segment_216.wav ‚Üí ein tippen m√ºssen, sondern nun um unseren Webserver zu starten und die Website auszuliefern, npm run start eingeben k√∂nnen. Und nun ist unser Webserver wieder.\n",
      "‚úÖ Transcribed: segment_217.wav ‚Üí verf√ºgbar. Und das war es auch schon zur heutigen Folge.\n",
      "‚úÖ Transcribed: segment_218.wav ‚Üí Noch mal ein kurzes Review. Was haben wir gemacht? Wir haben das lokale Setup mit Visual Studio Code und NPM vorbereitet. Ich habe einen ersten Einblick in die Tools NPM, NPX und ...\n",
      "‚úÖ Transcribed: segment_219.wav ‚Üí und Babel gegeben und wir haben diese auch schon live eingesetzt. Wir haben React eingebunden mit der Variante CDN oder Content Delivery Network, URL.\n",
      "‚úÖ Transcribed: segment_220.wav ‚Üí wir haben mit React Element und JSX eine erste Hello World Applikation von React implementiert. Da haben wir schon einen gro√üen Schritt gemacht. Das n√§chste Mal wird es noch spannender.\n",
      "‚úÖ Transcribed: segment_221.wav ‚Üí Da werden wir n√§mlich React Components vorstellen und einsetzen und mit den React Components die Renderfunktion, Props und dann noch ein paar Besonderheiten von JL.\n",
      "‚úÖ Transcribed: segment_222.wav ‚Üí an dieser Stelle bedanke ich mich recht herzlich, dass ihr dieses Video angesehen habt. Ich hoffe, ihr konntet etwas lernen.\n",
      "‚úÖ Transcribed: segment_223.wav ‚Üí Bei Fragen, Feedback oder sonstigen W√ºnschen und Anregungen meldet euch gerne bei uns √ºber diese Kan√§le. Hello at theNativeFab.io per E-Mail oder auch auf Twitter und Github.\n",
      "‚úÖ Transcribed: segment_224.wav ‚Üí und ich w√ºnsche euch jetzt noch einen sch√∂nen Tag und bis zur n√§chsten Folge. Ciao!\n",
      "üéâ Transcription complete! Metadata saved: /home/ahmet/my_projects/realtime_translator_V2_updated/server/segmented_wavs/metadata.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import whisper\n",
    "\n",
    "# ‚úÖ Load Whisper Model (Use 'medium' for better accuracy)\n",
    "model = whisper.load_model(\"medium\")\n",
    "\n",
    "# ‚úÖ Paths\n",
    "input_folder = os.path.join(os.getcwd(), \"segmented_wavs\")\n",
    "output_metadata = os.path.join(os.getcwd(), \"segmented_wavs/metadata.txt\")\n",
    "\n",
    "# ‚úÖ Transcribe and Save Metadata\n",
    "with open(output_metadata, \"w\", encoding=\"utf-8\") as f:\n",
    "    for file in sorted(os.listdir(input_folder)):\n",
    "        if file.endswith(\".wav\"):\n",
    "            file_path = os.path.join(input_folder, file)\n",
    "            result = model.transcribe(file_path, language=\"de\")  # Set your language\n",
    "            transcript = result[\"text\"].strip()\n",
    "            \n",
    "            if transcript:\n",
    "                f.write(f\"{file.replace('.wav', '')}|{transcript}\\n\")\n",
    "                print(f\"‚úÖ Transcribed: {file} ‚Üí {transcript}\")\n",
    "\n",
    "print(f\"üéâ Transcription complete! Metadata saved: {output_metadata}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All audio files are within the allowed duration.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "\n",
    "# ‚úÖ Set Paths\n",
    "audio_folder = os.path.join(os.getcwd(), \"segmented_wavs/wavs\")  # Update this path\n",
    "max_duration = 10.0  # Maximum allowed duration in seconds\n",
    "\n",
    "# ‚úÖ Scan and Check Durations\n",
    "long_files = []\n",
    "for file in sorted(os.listdir(audio_folder)):\n",
    "    if file.endswith(\".wav\"):\n",
    "        file_path = os.path.join(audio_folder, file)\n",
    "        audio, sr = librosa.load(file_path, sr=None)  # Load with original sample rate\n",
    "        duration = librosa.get_duration(y=audio, sr=sr)\n",
    "\n",
    "        if duration > max_duration:\n",
    "            long_files.append((file, round(duration, 2)))  # Store file name & duration\n",
    "\n",
    "# ‚úÖ Print Results\n",
    "if long_files:\n",
    "    print(f\"üö® {len(long_files)} audio files exceed {max_duration}s:\")\n",
    "    for file, duration in long_files:\n",
    "        print(f\"  ‚ùå {file}: {duration}s\")\n",
    "else:\n",
    "    print(\"‚úÖ All audio files are within the allowed duration.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "\n",
    "def extract_audio_segments(wav_file, json_file, output_folder):\n",
    "    \"\"\"\n",
    "    Extracts speech segments from the full WAV file using JSON timestamps.\n",
    "\n",
    "    Args:\n",
    "        wav_file (str): Path to the full WAV audio file.\n",
    "        json_file (str): Path to the JSON subtitles/transcripts file.\n",
    "        output_folder (str): Folder where extracted audio files and metadata.txt will be saved.\n",
    "    \"\"\"\n",
    "    # Ensure output directory exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # Load full audio file\n",
    "    audio, sr = librosa.load(wav_file, sr=22050)  # Resample to 22.05kHz for TTS\n",
    "\n",
    "    # Load JSON transcript\n",
    "    with open(json_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript_data = json.load(f)\n",
    "\n",
    "    metadata_entries = []\n",
    "\n",
    "    # Process each segment in the JSON file\n",
    "    for idx, segment in enumerate(transcript_data):\n",
    "        start_time = segment[\"start_time\"] / 1000.0  # Convert ms to sec\n",
    "        end_time = segment[\"end_time\"] / 1000.0 +0.5  # Convert ms to sec\n",
    "        text = segment[\"text\"].strip()\n",
    "\n",
    "        # Extract audio segment\n",
    "        start_sample = int(start_time * sr)\n",
    "        end_sample = int(end_time * sr)\n",
    "        audio_segment = audio[start_sample:end_sample]\n",
    "\n",
    "        # Save audio segment\n",
    "        segment_filename = f\"audio_{idx+1:03d}.wav\"\n",
    "        segment_path = os.path.join(output_folder, segment_filename)\n",
    "        sf.write(segment_path, audio_segment, sr)\n",
    "\n",
    "        # Add entry to metadata\n",
    "        metadata_entries.append(f\"{segment_filename}|{text}|{text.lower()}\")\n",
    "\n",
    "    # Save metadata.txt\n",
    "    metadata_path = os.path.join(output_folder, \"metadata.txt\")\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(metadata_entries))\n",
    "\n",
    "    print(f\"‚úÖ Audio segments saved to: {output_folder}\")\n",
    "    print(f\"‚úÖ Metadata file saved: {metadata_path}\")\n",
    "\n",
    "    return metadata_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Audio segments saved to: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones\n",
      "‚úÖ Metadata file saved: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones/metadata.txt\n"
     ]
    }
   ],
   "source": [
    "metadata_file = extract_audio_segments(wav_file_path, json_transcript_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment, silence\n",
    "import numpy as np\n",
    "\n",
    "def segment_audio_smart_v2(wav_file, output_folder, target_chunk_duration=10, max_chunk_duration=12, min_chunk_duration=5, silence_threshold=-40, silence_padding=0.2):\n",
    "    \"\"\"\n",
    "    Segments an audio file into chunks of approximately target_chunk_duration seconds,\n",
    "    prioritizing cuts at silent periods to avoid mid-word cuts.\n",
    "\n",
    "    Args:\n",
    "        wav_file (str): Path to the input WAV file.\n",
    "        output_folder (str): Directory for segmented audio files.\n",
    "        target_chunk_duration (int): Target duration for each chunk in seconds.\n",
    "        max_chunk_duration (int): Maximum allowed duration for a chunk in seconds.\n",
    "        min_chunk_duration (int): Minimum allowed duration for a chunk in seconds.\n",
    "        silence_threshold (int): Silence threshold in dBFS for pydub silence detection.\n",
    "        silence_padding (float):  Seconds of silence to include after a detected silence point.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of paths to the generated audio segment files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    segment_paths = []\n",
    "    segment_count = 1\n",
    "\n",
    "    audio = AudioSegment.from_wav(wav_file)\n",
    "    audio_duration_sec = len(audio) / 1000  # Duration in seconds\n",
    "    start_time_ms = 0\n",
    "\n",
    "    while start_time_ms < len(audio):\n",
    "        target_end_time_ms = start_time_ms + target_chunk_duration * 1000\n",
    "        max_end_time_ms = min(start_time_ms + max_chunk_duration * 1000, len(audio))\n",
    "        min_end_time_ms = min(start_time_ms + min_chunk_duration * 1000, len(audio))\n",
    "\n",
    "        if max_end_time_ms >= len(audio): # Last segment\n",
    "            end_time_ms = len(audio)\n",
    "        else:\n",
    "            # Look for silence near the target end time\n",
    "            search_start_ms = max(start_time_ms + min_chunk_duration * 1000, target_end_time_ms - 2000) # Search silence after min duration and around target\n",
    "            search_end_ms = max_end_time_ms\n",
    "\n",
    "            silent_ranges = silence.detect_silence(\n",
    "                audio[search_start_ms:search_end_ms],\n",
    "                min_silence_len=200,  # Shorter silence detection for finer cuts\n",
    "                silence_thresh=silence_threshold\n",
    "            )\n",
    "\n",
    "            if silent_ranges:\n",
    "                # Take the first silence found\n",
    "                silence_end_relative_ms = silent_ranges[0][0] # Start of silence relative to search_start_ms\n",
    "                end_time_ms = search_start_ms + silence_end_relative_ms + int(silence_padding * 1000) # Cut at silence + padding\n",
    "                end_time_ms = min(end_time_ms, max_end_time_ms) # Ensure not exceeding max duration\n",
    "                end_time_ms = max(end_time_ms, min_end_time_ms) # Ensure not less than min duration\n",
    "            else:\n",
    "                end_time_ms = max_end_time_ms # If no silence found, cut at max duration\n",
    "\n",
    "\n",
    "        segment_audio = audio[start_time_ms:end_time_ms]\n",
    "        segment_filename = f\"segment_{segment_count:03d}.wav\"\n",
    "        segment_path = os.path.join(output_folder, segment_filename)\n",
    "        segment_audio.export(segment_path, format=\"wav\")\n",
    "        segment_paths.append(segment_path)\n",
    "\n",
    "        start_time_ms = end_time_ms\n",
    "        segment_count += 1\n",
    "\n",
    "    print(f\"‚úÖ Smart segmentation v2 done with {len(segment_paths)} files.\")\n",
    "    return segment_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smart segmentation v2 done with 256 files.\n",
      "Segmented files: ['/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_001.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_002.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_003.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_004.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_005.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_006.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_007.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_008.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_009.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_010.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_011.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_012.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_013.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_014.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_015.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_016.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_017.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_018.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_019.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_020.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_021.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_022.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_023.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_024.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_025.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_026.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_027.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_028.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_029.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_030.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_031.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_032.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_033.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_034.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_035.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_036.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_037.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_038.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_039.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_040.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_041.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_042.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_043.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_044.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_045.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_046.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_047.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_048.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_049.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_050.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_051.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_052.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_053.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_054.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_055.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_056.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_057.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_058.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_059.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_060.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_061.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_062.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_063.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_064.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_065.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_066.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_067.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_068.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_069.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_070.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_071.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_072.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_073.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_074.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_075.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_076.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_077.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_078.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_079.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_080.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_081.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_082.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_083.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_084.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_085.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_086.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_087.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_088.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_089.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_090.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_091.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_092.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_093.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_094.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_095.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_096.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_097.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_098.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_099.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_100.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_101.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_102.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_103.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_104.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_105.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_106.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_107.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_108.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_109.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_110.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_111.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_112.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_113.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_114.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_115.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_116.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_117.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_118.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_119.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_120.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_121.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_122.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_123.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_124.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_125.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_126.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_127.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_128.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_129.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_130.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_131.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_132.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_133.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_134.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_135.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_136.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_137.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_138.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_139.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_140.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_141.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_142.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_143.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_144.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_145.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_146.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_147.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_148.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_149.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_150.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_151.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_152.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_153.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_154.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_155.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_156.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_157.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_158.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_159.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_160.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_161.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_162.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_163.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_164.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_165.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_166.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_167.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_168.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_169.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_170.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_171.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_172.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_173.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_174.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_175.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_176.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_177.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_178.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_179.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_180.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_181.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_182.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_183.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_184.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_185.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_186.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_187.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_188.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_189.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_190.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_191.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_192.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_193.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_194.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_195.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_196.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_197.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_198.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_199.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_200.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_201.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_202.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_203.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_204.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_205.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_206.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_207.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_208.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_209.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_210.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_211.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_212.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_213.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_214.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_215.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_216.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_217.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_218.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_219.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_220.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_221.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_222.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_223.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_224.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_225.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_226.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_227.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_228.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_229.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_230.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_231.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_232.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_233.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_234.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_235.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_236.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_237.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_238.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_239.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_240.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_241.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_242.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_243.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_244.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_245.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_246.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_247.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_248.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_249.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_250.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_251.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_252.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_253.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_254.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_255.wav', '/home/ahmet/my_projects/realtime_translator_V2_updated/server/clones_2/segment_256.wav']\n"
     ]
    }
   ],
   "source": [
    "segmented_files = segment_audio_smart_v2(\n",
    "            wav_file_path,\n",
    "            output_folder_2,\n",
    "            target_chunk_duration=10,\n",
    "            max_chunk_duration=12,\n",
    "            min_chunk_duration=5,\n",
    "            silence_threshold=-40,\n",
    "            silence_padding=0.2 # Add a bit of silence after cut\n",
    "        )\n",
    "print(\"Segmented files:\", segmented_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Updated metadata saved as: /home/ahmet/my_projects/realtime_translator_V2_updated/server/clones/metadata_cleaned.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def clean_metadata(metadata_file):\n",
    "    \"\"\"\n",
    "    Removes '.wav' extension from filenames in metadata.txt.\n",
    "    \n",
    "    Args:\n",
    "        metadata_file (str): Path to metadata.txt\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to updated metadata file\n",
    "    \"\"\"\n",
    "    new_metadata_file = metadata_file.replace(\".txt\", \"_cleaned.txt\")\n",
    "    \n",
    "    with open(metadata_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove .wav extension from filenames\n",
    "    cleaned_lines = [line.replace(\".wav|\", \"|\", 1) for line in lines]\n",
    "\n",
    "    # Save new metadata file\n",
    "    with open(new_metadata_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.writelines(cleaned_lines)\n",
    "\n",
    "    print(f\"‚úÖ Updated metadata saved as: {new_metadata_file}\")\n",
    "    return new_metadata_file\n",
    "\n",
    "# Run the function\n",
    "metadata_path = output_folder+\"/metadata.txt\"  # Update with actual path\n",
    "cleaned_metadata_path = clean_metadata(metadata_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " > Downloading DVAE files!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.07k/1.07k [00:01<00:00, 708iB/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'TTS.tts.datasets' has no attribute 'custom'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 52\u001b[0m\n\u001b[1;32m     43\u001b[0m config_dataset \u001b[38;5;241m=\u001b[39m BaseDatasetConfig(\n\u001b[1;32m     44\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \n\u001b[1;32m     45\u001b[0m     dataset_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustom_xtts_dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m     language\u001b[38;5;241m=\u001b[39mLANGUAGE,\n\u001b[1;32m     49\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# ‚úÖ Load Training Samples\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m train_samples, eval_samples \u001b[38;5;241m=\u001b[39m \u001b[43mload_tts_samples\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_split_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.05\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use 5% for evaluation\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m training samples and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(eval_samples)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m eval samples\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     60\u001b[0m \u001b[38;5;66;03m# ‚úÖ Define Model Arguments (GPT-Based Fine-Tuning)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/datasets/__init__.py:118\u001b[0m, in \u001b[0;36mload_tts_samples\u001b[0;34m(datasets, eval_split, formatter, eval_split_max_size, eval_split_size)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;66;03m# setup the right data processor\u001b[39;00m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m formatter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_get_formatter_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;66;03m# load train set\u001b[39;00m\n\u001b[1;32m    120\u001b[0m meta_data_train \u001b[38;5;241m=\u001b[39m formatter(root_path, meta_file_train, ignored_speakers\u001b[38;5;241m=\u001b[39mignored_speakers)\n",
      "File \u001b[0;32m~/anaconda3/envs/tts_env/lib/python3.10/site-packages/TTS/tts/datasets/__init__.py:166\u001b[0m, in \u001b[0;36m_get_formatter_by_name\u001b[0;34m(name)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the respective preprocessing function.\"\"\"\u001b[39;00m\n\u001b[1;32m    165\u001b[0m thismodule \u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mthismodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'TTS.tts.datasets' has no attribute 'custom'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import os\n",
    "from trainer import Trainer, TrainerArgs\n",
    "from TTS.tts.datasets import load_tts_samples\n",
    "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
    "from TTS.tts.layers.xtts.trainer.gpt_trainer import GPTArgs, GPTTrainer, GPTTrainerConfig, XttsAudioConfig\n",
    "from TTS.utils.manage import ModelManager\n",
    "\n",
    "# ‚úÖ Define Paths\n",
    "dataset = os.path.join(os.getcwd(), \"dataset\")\n",
    "DATASET_PATH = os.path.join(os.getcwd(),\"audio_dataset\")  # Your dataset folder\n",
    "METADATA_FILE =os.path.join(os.getcwd(),\"audio_dataset/metadata.txt\")\n",
    "OUTPUT_PATH = \"/home/ahmet/tts_finetuned_models/xtts_v2_gpt\"  # Fine-tuned models output\n",
    "CHECKPOINTS_OUT_PATH = os.path.join(OUTPUT_PATH, \"checkpoints\")  \n",
    "os.makedirs(CHECKPOINTS_OUT_PATH, exist_ok=True)\n",
    "\n",
    "# ‚úÖ Load XTTS-V2 Pretrained Model Checkpoint\n",
    "XTTS_CHECKPOINT = \"/home/ahmet/yourtts_model/tts_models--multilingual--multi-dataset--your_tts/model_file.pth\"\n",
    "\n",
    "# ‚úÖ Define Speaker Reference (One high-quality voice sample)\n",
    "SPEAKER_REFERENCE = [\n",
    "    os.path.join(DATASET_PATH, \"wavs\", \"segment_001.wav\")\n",
    "]\n",
    "\n",
    "# ‚úÖ Training Parameters\n",
    "BATCH_SIZE = 3  # Keep small to avoid memory issues\n",
    "GRAD_ACCUM_STEPS = 84  # Must be at least 252 when multiplied with batch size\n",
    "LEARNING_RATE = 5e-6  # Lower learning rate for stable fine-tuning\n",
    "LANGUAGE = \"de\"  # Adjust based on dataset\n",
    "\n",
    "# ‚úÖ DVAE & XTTS Model Files (GPT-Based Fine-Tuning)\n",
    "DVAE_CHECKPOINT = os.path.join(CHECKPOINTS_OUT_PATH, \"dvae.pth\")\n",
    "MEL_NORM_FILE = os.path.join(CHECKPOINTS_OUT_PATH, \"mel_stats.pth\")\n",
    "\n",
    "# ‚úÖ Download DVAE & XTTS Model Files If Not Available\n",
    "if not os.path.isfile(DVAE_CHECKPOINT) or not os.path.isfile(MEL_NORM_FILE):\n",
    "    print(\" > Downloading DVAE files!\")\n",
    "    ModelManager._download_model_files([\n",
    "        \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-V2/main/mel_stats.pth\",\n",
    "        \"https://coqui.gateway.scarf.sh/hf-coqui/XTTS-V2/main/dvae.pth\"\n",
    "    ], CHECKPOINTS_OUT_PATH, progress_bar=True)\n",
    "\n",
    "# ‚úÖ Define Dataset Configuration\n",
    "config_dataset = BaseDatasetConfig(\n",
    "    formatter=\"custom\",  \n",
    "    dataset_name=\"custom_xtts_dataset\",\n",
    "    path=DATASET_PATH,\n",
    "    meta_file_train=os.path.join(DATASET_PATH, METADATA_FILE),\n",
    "    language=LANGUAGE,\n",
    ")\n",
    "\n",
    "# ‚úÖ Load Training Samples\n",
    "train_samples, eval_samples = load_tts_samples(\n",
    "    config_dataset,\n",
    "    eval_split=True,\n",
    "    eval_split_size=0.05,  # Use 5% for evaluation\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(train_samples)} training samples and {len(eval_samples)} eval samples\")\n",
    "\n",
    "# ‚úÖ Define Model Arguments (GPT-Based Fine-Tuning)\n",
    "model_args = GPTArgs(\n",
    "    max_conditioning_length=132300,  # 6 seconds\n",
    "    min_conditioning_length=66150,  # 3 seconds\n",
    "    debug_loading_failures=False,\n",
    "    max_wav_length=255995,  # ~11.6 seconds\n",
    "    max_text_length=200,\n",
    "    mel_norm_file=MEL_NORM_FILE,\n",
    "    dvae_checkpoint=DVAE_CHECKPOINT,\n",
    "    xtts_checkpoint=XTTS_CHECKPOINT,  # XTTS-V2 checkpoint path\n",
    "    tokenizer_file=None,  # XTTS already has a tokenizer\n",
    "    gpt_num_audio_tokens=1026,\n",
    "    gpt_start_audio_token=1024,\n",
    "    gpt_stop_audio_token=1025,\n",
    "    gpt_use_masking_gt_prompt_approach=True,\n",
    "    gpt_use_perceiver_resampler=True,\n",
    ")\n",
    "\n",
    "# ‚úÖ Define Audio Configuration\n",
    "audio_config = XttsAudioConfig(\n",
    "    sample_rate=22050,  \n",
    "    dvae_sample_rate=22050,  \n",
    "    output_sample_rate=24000\n",
    ")\n",
    "\n",
    "# ‚úÖ Define Training Config (GPT-Based)\n",
    "config = GPTTrainerConfig(\n",
    "    output_path=OUTPUT_PATH,\n",
    "    model_args=model_args,\n",
    "    run_name=\"GPT_XTTS_v2.0_FT\",\n",
    "    project_name=\"XTTS_trainer\",\n",
    "    dashboard_logger=\"tensorboard\",\n",
    "    logger_uri=None,\n",
    "    audio=audio_config,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    batch_group_size=48,  \n",
    "    eval_batch_size=BATCH_SIZE,\n",
    "    num_loader_workers=8,\n",
    "    eval_split_max_size=256,\n",
    "    print_step=50,\n",
    "    plot_step=100,\n",
    "    log_model_step=1000,\n",
    "    save_step=5000,  # Save model every 5000 steps\n",
    "    save_n_checkpoints=1,  \n",
    "    save_checkpoints=True,\n",
    "    optimizer=\"AdamW\",\n",
    "    optimizer_wd_only_on_weights=True,\n",
    "    optimizer_params={\"betas\": [0.9, 0.96], \"eps\": 1e-8, \"weight_decay\": 1e-2},\n",
    "    lr=LEARNING_RATE,  # Learning rate\n",
    "    lr_scheduler=\"MultiStepLR\",\n",
    "    lr_scheduler_params={\"milestones\": [50000 * 18, 150000 * 18, 300000 * 18], \"gamma\": 0.5, \"last_epoch\": -1},\n",
    "    test_sentences=[\n",
    "        {\n",
    "            \"text\": \"This is an AI-powered voice cloning model.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "        {\n",
    "            \"text\": \"Artificial intelligence is changing the world.\",\n",
    "            \"speaker_wav\": SPEAKER_REFERENCE,\n",
    "            \"language\": LANGUAGE,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# ‚úÖ Initialize Model\n",
    "model = GPTTrainer.init_from_config(config)\n",
    "\n",
    "# ‚úÖ Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    TrainerArgs(\n",
    "        restore_path=None,  # XTTS checkpoint is restored via `xtts_checkpoint`, so no need to restore here\n",
    "        skip_train_epoch=False,\n",
    "        start_with_eval=True,\n",
    "        grad_accum_steps=GRAD_ACCUM_STEPS,\n",
    "    ),\n",
    "    config,\n",
    "    output_path=OUTPUT_PATH,\n",
    "    model=model,\n",
    "    train_samples=train_samples,\n",
    "    eval_samples=eval_samples,\n",
    ")\n",
    "\n",
    "# ‚úÖ Start Training üöÄ\n",
    "trainer.fit()\n",
    "\n",
    "print(f\"üéâ XTTS-V2 GPT-based fine-tuning completed! Model saved to: {OUTPUT_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7489/1117580102.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
      "/home/ahmet/anaconda3/envs/tts_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "üî• Model Keys: dict_keys(['config', 'model', 'scaler', 'optimizer', 'step', 'date', 'model_loss'])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "XTTS_CHECKPOINT = \"/home/ahmet/xtts_gpt_model/xtts_model.pth\"\n",
    "\n",
    "try:\n",
    "    model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
    "    print(\"‚úÖ Model loaded successfully!\")\n",
    "    print(\"üî• Model Keys:\", model_state.keys())  # üî• Print all keys in the checkpoint\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Model failed to load! Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_195868/1349123997.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Available Keys in XTTS Checkpoint: dict_keys(['config', 'model', 'scaler', 'optimizer', 'step', 'date', 'model_loss'])\n",
      "‚úÖ Found `model` key instead of `net`. Trying to load...\n",
      "‚ùå XTTS Model failed to load! Error: name 'model' is not defined\n"
     ]
    }
   ],
   "source": [
    "XTTS_CHECKPOINT = \"/home/ahmet/xtts_gpt_model/xtts_model.pth\"\n",
    "try:\n",
    "    model_state = torch.load(XTTS_CHECKPOINT, map_location=\"cpu\")\n",
    "    print(f\"üî• Available Keys in XTTS Checkpoint: {model_state.keys()}\")\n",
    "\n",
    "    if \"model\" in model_state:\n",
    "        print(\"‚úÖ Found `model` key instead of `net`. Trying to load...\")\n",
    "        model.load_state_dict(model_state[\"model\"], strict=False)\n",
    "        print(\"‚úÖ XTTS Model loaded successfully!\")\n",
    "    else:\n",
    "        raise ValueError(\"‚ùå `model` key is missing in XTTS checkpoint.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå XTTS Model failed to load! Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "wav_folder = \"audio_dataset/wavs/\"\n",
    "\n",
    "for filename in os.listdir(wav_folder):\n",
    "    file_path = os.path.join(wav_folder, filename)\n",
    "    \n",
    "    # ‚úÖ If file has no extension, add .wav\n",
    "    if \".\" not in filename:\n",
    "        new_file_path = file_path + \".wav\"\n",
    "        os.rename(file_path, new_file_path)\n",
    "        print(f\"‚úÖ Renamed: {filename} ‚Üí {filename}.wav\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_time_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
