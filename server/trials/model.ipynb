{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def transcribe_audio(audio_file, model_size=\"medium\", use_gpu=True):\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using Whisper, considering performance and accuracy.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file.\n",
    "        model_size (str): Size of the Whisper model (\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\").\n",
    "        use_gpu (bool): Whether to use the GPU if available.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the transcription text and timing information.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Determine Device (GPU or CPU)\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(\"Using CUDA (GPU)\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        if use_gpu:\n",
    "             print(\"CUDA not available, using CPU instead.\")\n",
    "        else:\n",
    "            print(\"Using CPU.\")\n",
    "\n",
    "    # 2. Load the Model\n",
    "    try:\n",
    "        model = whisper.load_model(model_size, device=device)\n",
    "        print(f\"Model '{model_size}' loaded successfully on {device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return {\"text\": \"\", \"error\": str(e), \"load_time\": time.time() - start_time}\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    # 3. Transcribe the Audio\n",
    "    try:\n",
    "        transcribe_start_time = time.time()\n",
    "        result = model.transcribe(audio_file)\n",
    "        transcribe_time = time.time() - transcribe_start_time\n",
    "        print(f\"Transcription completed in {transcribe_time:.2f} seconds.\")\n",
    "        return {\n",
    "            \"text\": result[\"text\"],\n",
    "            \"load_time\": load_time,\n",
    "            \"transcribe_time\": transcribe_time,\n",
    "            \"total_time\": time.time() - start_time,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"load_time\": load_time,\n",
    "            \"transcribe_time\": 0,\n",
    "            \"total_time\": time.time() - start_time,\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"test.wav\"  # Replace with your audio file\n",
    "    # Example Usage:\n",
    "    # 1. Medium model with GPU\n",
    "    result_medium_gpu = transcribe_audio(audio_file, model_size=\"medium\", use_gpu=True)\n",
    "    print(\"\\n--- Medium Model with GPU ---\")\n",
    "    print(f\"Transcription: {result_medium_gpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_medium_gpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_medium_gpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_medium_gpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_medium_gpu:\n",
    "        print(f\"Error: {result_medium_gpu['error']}\")\n",
    "\n",
    "    # 2. Medium model with CPU (for comparison)\n",
    "    result_medium_cpu = transcribe_audio(audio_file, model_size=\"medium\", use_gpu=False)\n",
    "    print(\"\\n--- Medium Model with CPU ---\")\n",
    "    print(f\"Transcription: {result_medium_cpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_medium_cpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_medium_cpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_medium_cpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_medium_cpu:\n",
    "        print(f\"Error: {result_medium_cpu['error']}\")\n",
    "\n",
    "    # 3. Small model with GPU (if medium has issues)\n",
    "    result_small_gpu = transcribe_audio(audio_file, model_size=\"small\", use_gpu=True)\n",
    "    print(\"\\n--- Small Model with GPU ---\")\n",
    "    print(f\"Transcription: {result_small_gpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_small_gpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_small_gpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_small_gpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_small_gpu:\n",
    "        print(f\"Error: {result_small_gpu['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Clear GPU memory\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "print(\"Cleared GPU memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory.\n",
      "Failed to initialize NVML: Driver/library version mismatch\n",
      "NVML library version: 535.183\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory.\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Define local cache path\n",
    "local_model_path = \"/home/ahmet/.cache/huggingface/hub/models--openai--whisper-large-v2\"\n",
    "\n",
    "# Load processor and model from the local path\n",
    "print(\"Loading processor and model from local cache...\")\n",
    "processor = WhisperProcessor.from_pretrained(local_model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "print(\"Model loaded successfully on GPU.\")\n",
    "\n",
    "# Test the model with an example input\n",
    "def test_model(audio_path):\n",
    "    \"\"\"\n",
    "    Test the Whisper model with a given audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "    \"\"\"\n",
    "    print(f\"Testing model on: {audio_path}\")\n",
    "    # Replace this with real preprocessing (e.g., spectrogram generation)\n",
    "    inputs = processor(audio_path, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
    "    outputs = model.generate(inputs)\n",
    "    transcription = processor.decode(outputs[0])\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "# Example usage\n",
    "# Replace with an actual audio file path\n",
    "test_audio_path = \"./test.wav\"\n",
    "# Uncomment the line below to test\n",
    "# test_model(test_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading M2M-100 model and tokenizer...\n",
      "M2M-100 model and tokenizer downloaded successfully!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import sentencepiece\n",
    "# Define the model name\n",
    "model_name = \"facebook/m2m100_418M\"  # Smaller version (418M parameters) for testing\n",
    "#model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "#tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "# Download and load the tokenizer and model\n",
    "print(\"Downloading M2M-100 model and tokenizer...\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "print(\"M2M-100 model and tokenizer downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for facebook/m2m100_1.2B...\n",
      "Loading model facebook/m2m100_1.2B with dtype=torch.float16 on device=cuda...\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"facebook/m2m100_1.2B\"  # Larger model with 1.2B parameters\n",
    "\n",
    "# Set device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32  # Use mixed precision on GPU\n",
    "\n",
    "# Load the tokenizer\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model with optimization\n",
    "print(f\"Loading model {model_name} with dtype={dtype} on device={device}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply dtype optimization\n",
    "if dtype == torch.float16:\n",
    "    model = model.half()  # Convert model weights to float16\n",
    "model = model.to(device)  # Move model to the selected device\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCorked = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isCorked != False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_time_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
