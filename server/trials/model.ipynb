{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA (GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 1.42G/1.42G [09:38<00:00, 2.64MiB/s]\n",
      "/home/ahmet/anaconda3/envs/langenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'medium' loaded successfully on cuda.\n",
      "Transcription completed in 11.54 seconds.\n",
      "\n",
      "--- Medium Model with GPU ---\n",
      "Transcription:  We'll show you how to download YouTube videos. If you find this guide useful, consider subscribing and liking the video. And let's get right into this. There are a couple of different ways you can go and download YouTube videos. And I'm going to go and explain them in this video. So firstly, I want to show you how you can go and download your own YouTube videos. To do that, you first need to go to your YouTube studio. So go to the top right and click on your profile icon and then tap on YouTube Studio. Once you're in the studio, then go over to the left like this and go and tap on content. And then what you want to do is choose the video which you want to go and download. In this case, it's this top one here. Then hover over it with your mouse, and you're going to go and see these three dots here called options. Click on the options button and then you can go and tap on download. And shortly, the download will begin. If I then go look at the top right of my browser, as you can see here, is the video is then downloading on my computer. However, you may want to go and download other people's videos. So now let me show you that. So firstly, go and find a video which you want to go and download. I'm going to go and download this video here. So go and click on the one you want to. And then all you need to do is scroll down. And below the video, you're going to notice a download button. And all you need to do is go and click on download like this. And then as you can see, it says download this video with premium. So to download other people's YouTube videos, you are going to need something called YouTube Premium. It comes with a ton of other benefits and it's basically a YouTube membership program. So what I'm going to do is go and subscribe because I've got a free trial as well. So, yeah, you will need to go and subscribe to then download the video. I just went and subscribed. And now what I'm going to do is go and click on the download button just like so. And the download is now beginning. So in the bottom left, you can also see it's downloading and you can go and see the progress bar. I'm going to go and click on view just like this. And this will take us to our downloads. And as you can see, it's almost finished. And then once it's finished downloading, you can then just go and watch it. I can watch it now without having an Internet connection, which is super useful. And you can also go and look at the download settings as well. If you found this useful, please leave a like.\n",
      "Load Time: 584.53 seconds\n",
      "Transcribe Time: 11.54 seconds\n",
      "Total Time: 596.07 seconds\n",
      "Using CPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/langenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'medium' loaded successfully on cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/langenv/lib/python3.12/site-packages/whisper/transcribe.py:124: UserWarning: Performing inference on CPU when CUDA is available\n",
      "  warnings.warn(\"Performing inference on CPU when CUDA is available\")\n",
      "/home/ahmet/anaconda3/envs/langenv/lib/python3.12/site-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
      "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription completed in 60.41 seconds.\n",
      "\n",
      "--- Medium Model with CPU ---\n",
      "Transcription:  We'll show you how to download YouTube videos. If you find this guide useful, consider subscribing and liking the video. And let's get right into this. There are a couple of different ways you can go and download YouTube videos. And I'm going to go and explain them in this video. So firstly, I want to show you how you can go and download your own YouTube videos. To do that, you first need to go to your YouTube studio. So go to the top right and click on your profile icon and then tap on YouTube Studio. Once you're in the studio, then go over to the left like this and go and tap on content. And then what you want to do is choose the video which you want to go and download. In this case, it's this top one here. Then hover over it with your mouse, and you're going to go and see these three dots here called options. Click on the options button and then you can go and tap on download. And shortly the download will begin. If I then go look at the top right of my browser, as you can see here, is the video is then downloading on my computer. However, you may want to go and download other people's videos. So now let me show you that. So firstly, go and find a video which you want to go and download. I'm going to go and download this video here. So go and click on the one you want to. And then all you need to do is scroll down. And below the video, you're going to notice a download button. And all you need to do is go and click on download like this. And then as you can see, it says download this video with premium. So to download other people's YouTube videos, you are going to need something called YouTube Premium. It comes with a ton of other benefits and it's basically a YouTube membership program. So what I'm going to do is go and subscribe because I've got a free trial as well. So, yeah, you will need to go and subscribe to then download the video. I just went and subscribed. And now what I'm going to do is go and click on the download button just like so. And the download is now beginning. So in the bottom left, you can also see it's downloading and you can go and see the progress bar. I'm going to go and click on view just like this. And this will take us to our downloads. And as you can see, it's almost finished. And then once it's finished downloading, you can then just go and watch it. I can watch it now without having an Internet connection, which is super useful. And you can also go and look at the download settings as well. If you found this useful, please leave a like.\n",
      "Load Time: 4.90 seconds\n",
      "Transcribe Time: 60.41 seconds\n",
      "Total Time: 65.31 seconds\n",
      "Using CUDA (GPU)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [02:19<00:00, 3.47MiB/s]\n",
      "/home/ahmet/anaconda3/envs/langenv/lib/python3.12/site-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'small' loaded successfully on cuda.\n",
      "Transcription completed in 5.58 seconds.\n",
      "\n",
      "--- Small Model with GPU ---\n",
      "Transcription:  We'll show you how to download YouTube videos. If you find this guide useful, consider subscribing and liking the video and let's get right into this. There are a couple of different ways you can go and download YouTube videos and I'm going to go and explain them in this video. So firstly, I want to show you how you can go and download your own YouTube videos. To do that, you first need to go to your YouTube studio. So go to the top right and click on your profile icon and then tap on YouTube Studio. Once you're in the studio, then go over to the left like this and go and tap on content. And then what you want to do is choose the video which you want to go and download. In this case, it's this top one here. Then hover over it with your mouse and you're going to go and see these three dots here called options. Click on the options button and then you can go and tap on download and shortly the download will begin. If I then go and look at the top right of my browser, as you can see here is the video is then downloading on my computer. However, you may want to go and download other people's videos. So now let me show you that. So firstly, go and find a video which you want to go and download. I'm going to go and download this video here. So go and click on the one you want to. And then all you need to do is scroll down and below the video, you're going to notice a download button. And all you need to do is go and click on download like this. And then as you can see, it says download this video with premium. So to download other people's YouTube videos, you are going to need something called YouTube premium. It comes with a ton of other benefits and it's basically a YouTube membership program. So what I'm going to do is go and subscribe because I've got a free trial as well. So yeah, you will need to go and subscribe to then download the video. I just went and subscribed. And now what I'm going to do is go and click on the download button just like so. And the download is now beginning. So in the bottom left, you can also see it's downloading. And you can go and see the progress bar. I'm going to go and click on view just like this. And this will take us to our downloads. And as you can see, it's almost finished. And then once it's finished downloading, you can then just go and watch it. I can watch it now without having an internet connection, which is super useful. And you can also go and look at the download settings as well. If you found this useful, please leave a like.\n",
      "Load Time: 141.29 seconds\n",
      "Transcribe Time: 5.58 seconds\n",
      "Total Time: 146.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "import torch\n",
    "import time\n",
    "\n",
    "def transcribe_audio(audio_file, model_size=\"medium\", use_gpu=True):\n",
    "    \"\"\"\n",
    "    Transcribes an audio file using Whisper, considering performance and accuracy.\n",
    "\n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file.\n",
    "        model_size (str): Size of the Whisper model (\"tiny\", \"base\", \"small\", \"medium\", \"large-v2\").\n",
    "        use_gpu (bool): Whether to use the GPU if available.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the transcription text and timing information.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 1. Determine Device (GPU or CPU)\n",
    "    if use_gpu and torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        print(\"Using CUDA (GPU)\")\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        if use_gpu:\n",
    "             print(\"CUDA not available, using CPU instead.\")\n",
    "        else:\n",
    "            print(\"Using CPU.\")\n",
    "\n",
    "    # 2. Load the Model\n",
    "    try:\n",
    "        model = whisper.load_model(model_size, device=device)\n",
    "        print(f\"Model '{model_size}' loaded successfully on {device}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return {\"text\": \"\", \"error\": str(e), \"load_time\": time.time() - start_time}\n",
    "\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    # 3. Transcribe the Audio\n",
    "    try:\n",
    "        transcribe_start_time = time.time()\n",
    "        result = model.transcribe(audio_file)\n",
    "        transcribe_time = time.time() - transcribe_start_time\n",
    "        print(f\"Transcription completed in {transcribe_time:.2f} seconds.\")\n",
    "        return {\n",
    "            \"text\": result[\"text\"],\n",
    "            \"load_time\": load_time,\n",
    "            \"transcribe_time\": transcribe_time,\n",
    "            \"total_time\": time.time() - start_time,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error during transcription: {e}\")\n",
    "        return {\n",
    "            \"text\": \"\",\n",
    "            \"error\": str(e),\n",
    "            \"load_time\": load_time,\n",
    "            \"transcribe_time\": 0,\n",
    "            \"total_time\": time.time() - start_time,\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"test.wav\"  # Replace with your audio file\n",
    "    # Example Usage:\n",
    "    # 1. Medium model with GPU\n",
    "    result_medium_gpu = transcribe_audio(audio_file, model_size=\"medium\", use_gpu=True)\n",
    "    print(\"\\n--- Medium Model with GPU ---\")\n",
    "    print(f\"Transcription: {result_medium_gpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_medium_gpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_medium_gpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_medium_gpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_medium_gpu:\n",
    "        print(f\"Error: {result_medium_gpu['error']}\")\n",
    "\n",
    "    # 2. Medium model with CPU (for comparison)\n",
    "    result_medium_cpu = transcribe_audio(audio_file, model_size=\"medium\", use_gpu=False)\n",
    "    print(\"\\n--- Medium Model with CPU ---\")\n",
    "    print(f\"Transcription: {result_medium_cpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_medium_cpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_medium_cpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_medium_cpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_medium_cpu:\n",
    "        print(f\"Error: {result_medium_cpu['error']}\")\n",
    "\n",
    "    # 3. Small model with GPU (if medium has issues)\n",
    "    result_small_gpu = transcribe_audio(audio_file, model_size=\"small\", use_gpu=True)\n",
    "    print(\"\\n--- Small Model with GPU ---\")\n",
    "    print(f\"Transcription: {result_small_gpu.get('text', 'Error')}\")\n",
    "    print(f\"Load Time: {result_small_gpu.get('load_time', 0):.2f} seconds\")\n",
    "    print(f\"Transcribe Time: {result_small_gpu.get('transcribe_time', 0):.2f} seconds\")\n",
    "    print(f\"Total Time: {result_small_gpu.get('total_time', 0):.2f} seconds\")\n",
    "    if \"error\" in result_small_gpu:\n",
    "        print(f\"Error: {result_small_gpu['error']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "\n",
    "# Clear GPU memory\n",
    "cuda.select_device(0)\n",
    "cuda.close()\n",
    "print(\"Cleared GPU memory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared GPU memory.\n",
      "Failed to initialize NVML: Driver/library version mismatch\n",
      "NVML library version: 535.183\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"Cleared GPU memory.\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "\n",
    "# Define local cache path\n",
    "local_model_path = \"/home/ahmet/.cache/huggingface/hub/models--openai--whisper-large-v2\"\n",
    "\n",
    "# Load processor and model from the local path\n",
    "print(\"Loading processor and model from local cache...\")\n",
    "processor = WhisperProcessor.from_pretrained(local_model_path)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(local_model_path).to(\"cuda\")\n",
    "\n",
    "print(\"Model loaded successfully on GPU.\")\n",
    "\n",
    "# Test the model with an example input\n",
    "def test_model(audio_path):\n",
    "    \"\"\"\n",
    "    Test the Whisper model with a given audio file.\n",
    "\n",
    "    Args:\n",
    "        audio_path (str): Path to the audio file.\n",
    "    \"\"\"\n",
    "    print(f\"Testing model on: {audio_path}\")\n",
    "    # Replace this with real preprocessing (e.g., spectrogram generation)\n",
    "    inputs = processor(audio_path, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
    "    outputs = model.generate(inputs)\n",
    "    transcription = processor.decode(outputs[0])\n",
    "    print(\"Transcription:\", transcription)\n",
    "\n",
    "# Example usage\n",
    "# Replace with an actual audio file path\n",
    "test_audio_path = \"./test.wav\"\n",
    "# Uncomment the line below to test\n",
    "# test_model(test_audio_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading M2M-100 model and tokenizer...\n",
      "M2M-100 model and tokenizer downloaded successfully!\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n",
    "import sentencepiece\n",
    "# Define the model name\n",
    "model_name = \"facebook/m2m100_418M\"  # Smaller version (418M parameters) for testing\n",
    "#model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "#tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_1.2B\")\n",
    "# Download and load the tokenizer and model\n",
    "print(\"Downloading M2M-100 model and tokenizer...\")\n",
    "tokenizer = M2M100Tokenizer.from_pretrained(model_name)\n",
    "model = M2M100ForConditionalGeneration.from_pretrained(model_name)\n",
    "print(\"M2M-100 model and tokenizer downloaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahmet/anaconda3/envs/real_time_t/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer for facebook/m2m100_1.2B...\n",
      "Loading model facebook/m2m100_1.2B with dtype=torch.float16 on device=cuda...\n",
      "Model and tokenizer loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import torch\n",
    "\n",
    "# Define the model name\n",
    "model_name = \"facebook/m2m100_1.2B\"  # Larger model with 1.2B parameters\n",
    "\n",
    "# Set device and dtype\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float16 if device == \"cuda\" else torch.float32  # Use mixed precision on GPU\n",
    "\n",
    "# Load the tokenizer\n",
    "print(f\"Loading tokenizer for {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Load the model with optimization\n",
    "print(f\"Loading model {model_name} with dtype={dtype} on device={device}...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Apply dtype optimization\n",
    "if dtype == torch.float16:\n",
    "    model = model.half()  # Convert model weights to float16\n",
    "model = model.to(device)  # Move model to the selected device\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "isCorked = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isCorked != False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "real_time_t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
